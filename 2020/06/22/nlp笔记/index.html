<!DOCTYPE html><html lang="zh-CN//语言"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="1EB8XoOl0C"><meta name="google-site-verification" content="K7thEgdLm0UfRWJ5MGdF7sCcjClSzAlxFLPv2Oz5CGM"><title> ⭐️nlp笔记 · Note</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="⭐️nlp笔记 - Dufy"><meta name="keywords"><meta name="author" content="Dufy"><link rel="short icon" href="/images/favicon.ico"><link rel="stylesheet" href="/css/bubuzou.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Note"><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script></head><body><header><div class="header row"> <a href="/" class="logo-link"><img src="/images/logo.png"></a><ul id="nav_list" class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" data-hover="博文" class="nav-list-link">博文</a></li><li class="nav-list-item"><a href="/archives/" target="_self" data-hover="归档" class="nav-list-link">归档</a></li></ul><div class="search"><a id="search_btn" href="#search"></a></div><div id="nav_btn" class="nav-btn"><span></span><span></span><span></span></div></div></header><div class="row scroll-con"><section class="container"><!-- for archive page--><div id="postAr" class="post"><article class="post-block"><h1 class="post-title">⭐️nlp笔记</h1><div class="post-info">2020-06-22</div><div class="post-content"><p>[TOC]</p>
<p>这是摘要……</p>
<a id="more"></a>

<h2 id="大框架"><a href="#大框架" class="headerlink" title="大框架"></a>大框架</h2><p>可以百度脑图查看，<a href="https://naotu.baidu.com/file/f644044a8fb37fdba2d3d0bb4eb350e1?token=fd9855a9fc353aca" target="_blank" rel="noopener">点击链接</a></p>
<p>出处：<a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.%20NLP/README.md" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.%20NLP/README.md</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg1dm92gghj30wh0u0e83.jpg" alt="image-20200622211438309"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg1dnz5umfj31f90u04qq.jpg" alt="image-20200622211617842"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg1dow7xvdj31s80sgnpd.jpg" alt="image-20200622211710960"></p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkdg2xebeuj30rw10oe81.jpg" alt="image-20201104210734713"></p>
<p>链接：<a href="https://supcache.wyqrks.com/data/mantchs.com/img/mindmap.jpg" target="_blank" rel="noopener">https://supcache.wyqrks.com/data/mantchs.com/img/mindmap.jpg</a></p>
<h2 id="什么是nlp-？"><a href="#什么是nlp-？" class="headerlink" title="什么是nlp ？"></a>什么是nlp ？</h2><p><strong>自然语言处理</strong>（Natural Language Processing，NLP）是一门集语言学，数学及计算机科学于一体的科学。它的核心目标就是把人的自然语言转换为计算机可以阅读的指令，简单来说就是<strong>让机器读懂人的语言</strong>。</p>
<p>NLP是人工智能领域一个非常重要的分支，其它重要分支包括计算机视觉，语音及机器学习和深度学习等。那么，NLP与机器学习，深度学习有什么关系呢？我们可以用下面的图来表示。可以看出，深度学习是机器学习的其中一个分支，而自然语言处理与机器学习之间是并行的，机器学习为自然语言处理提供了解决问题的许多模型和方法。所以，二者之间具有密不可分的关系。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gpijds0t37j30yy0i4qed.jpg" alt="image-20210413233114134"></p>
<ul>
<li>另外的一种理解</li>
</ul>
<p><a href="https://easyai.tech/ai-definition/nlp/" target="_blank" rel="noopener">https://easyai.tech/ai-definition/nlp/</a></p>
<p>不同物种都有自己的沟通方式</p>
<p>自然语言处理（NLP）就是在机器语言和人类语言之间沟通的桥梁，以实现人机交流的目的。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gpikfgbwlij30ti0fegxc.jpg" alt="image-20210414000725230"></p>
<h2 id="NLP基础知识"><a href="#NLP基础知识" class="headerlink" title="NLP基础知识"></a>NLP基础知识</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p><a href="https://zhuanlan.zhihu.com/p/334460660" target="_blank" rel="noopener">如何通俗易懂地让女朋友明白什么是语言模型？</a></p>
<h3 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h3><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>我们要描述文章内容，首先要将文章内容表示出来。</p>
<p>一个比较直观的想法是采用 <strong>序号编码</strong>。</p>
<p>这对于具有大小关系的数据是有意义的，如表示‘低、中、高’三档次，可以用‘1、2、3’表示。转换后依然保留原先的大小关系。</p>
<table>
<thead>
<tr>
<th>低</th>
<th>1</th>
</tr>
</thead>
<tbody><tr>
<td>中</td>
<td>2</td>
</tr>
<tr>
<td>高</td>
<td>3</td>
</tr>
</tbody></table>
<p>但是，更多情况下，对于不具有大小关系的数据，该如何表示呢？</p>
<p>可以考虑向量方法，如<strong>‘one-hot encoding’</strong></p>
<p>比如，词表有{‘水果’, ‘橘子’,’葡萄’,’汽车’, ‘大象’, ‘动物’}，共6个词</p>
<p>那么，这些词就可以采用如下方式表示：</p>
<table>
<thead>
<tr>
<th>水果</th>
<th>（1，0，0，0，0，0）</th>
</tr>
</thead>
<tbody><tr>
<td>中</td>
<td>（0，1，0，0，0，0）</td>
</tr>
<tr>
<td>高</td>
<td>（0，0，0，0，1，0）</td>
</tr>
</tbody></table>
<p>这样的问题是：</p>
<ol>
<li>词表中词很多时候，向量维度会过大</li>
<li>词与词之间 的关系表示不出来，对于任意两个向量a,b，计算cos(a,b)==0</li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9ja8ejmnj30fq056abx.jpg" alt="image-20210102183711769"></p>
<p>又引入了接下里的‘<strong>word2vec</strong>’模型</p>
<p><strong>word2vec</strong>由谷歌于2013年提出，是一种浅层的神经网络模型，分为两种方法：</p>
<ul>
<li>CBOW—完形填空</li>
<li>Skip-gram—造句，跳字模型</li>
</ul>
<h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>衡量词在当前文档中的重要程度计算如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkdjxj2c14j30wg03k0x3.jpg" alt="image-20201104232113679"></p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkdjvybfm1j30se06cgsu.jpg" alt="image-20201104231940549"></p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkdjx3x503j30uw06in78.jpg" alt="image-20201104232047326"></p>
<p>比如，</p>
<blockquote>
<p>还是以《中国的蜜蜂养殖》为例，假定该文长度为1000个词，”中国”、”蜜蜂”、”养殖”各出现20次，则这三个词的”词频”（TF）都为0.02。然后，搜索Google发现，包含”的”字的网页共有250亿张，假定这就是中文网页总数。包含”中国”的网页共有62.3亿张，包含”蜜蜂”的网页为0.484亿张，包含”养殖”的网页为0.973亿张。则它们的逆文档频率（IDF）和TF-IDF如下：<img src="http://www.ruanyifeng.com/blogimg/asset/201303/bg2013031508.png" alt="img"></p>
<p>tf-idf(‘蜜蜂’)=0.02*lg_10(250/0.484)=0.02 x 2.713=0.0543</p>
<p>从上表可见，”蜜蜂”的TF-IDF值最高，”养殖”其次，”中国”最低。（如果还计算”的”字的TF-IDF，那将是一个极其接近0的值。）所以，如果只选择一个词，”蜜蜂”就是这篇文章的关键词。</p>
</blockquote>
<ul>
<li><p>应用场景【1】<a name="toc1"></a></p>
<p>自动提取关键词、信息检索、相似文章、自动摘要等</p>
</li>
</ul>
<p>1) 自动提取关键词</p>
<p>2）信息检索</p>
<blockquote>
<p>除了自动提取关键词，TF-IDF算法还可以用于许多别的地方。比如，信息检索时，对于每个文档，都可以分别计算一组搜索词（”中国”、”蜜蜂”、”养殖”）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。</p>
</blockquote>
<p>3) 找出相似文章(结合余弦相似度)</p>
<blockquote>
<ol>
<li><p>使用TF-IDF算法，找出两篇文章的关键词；</p>
</li>
<li><p>每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；</p>
</li>
<li><p>生成两篇文章各自的词频向量；</p>
</li>
<li><p>计算两个向量的余弦相似度，值越大就表示越相似。</p>
</li>
</ol>
</blockquote>
<p>词频向量（类似于词袋模型）</p>
<p>e.g.    计算下面两个句子的相似性：</p>
<p>句子A：我喜欢看电视，不喜欢看电影。</p>
<p>句子B：我不喜欢看电视，也不喜欢看电影。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmqltlm2tuj30za0u0noh.jpg" alt="image-20210117125938972"></p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmqluzit33j30iv05p3yb.jpg" alt="img"></p>
<p>4）自动摘要</p>
<p>只考虑关键词首先出现的句子:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Summarizer(originalText, maxSummarySize):</span><br><span class="line"></span><br><span class="line">　　　　// 计算原始文本的词频，生成一个数组，比如[(10,'the'), (3,'language'), (8,'code')...]</span><br><span class="line">　　　　wordFrequences = getWordCounts(originalText)</span><br><span class="line"></span><br><span class="line">　　　　// 过滤掉停用词，数组变成[(3, 'language'), (8, 'code')...]</span><br><span class="line">　　　　contentWordFrequences = filtStopWords(wordFrequences)</span><br><span class="line"></span><br><span class="line">　　　　// 按照词频进行排序，数组变成['code', 'language'...]</span><br><span class="line">　　　　contentWordsSortbyFreq = sortByFreqThenDropFreq(contentWordFrequences)</span><br><span class="line"></span><br><span class="line">　　　　// 将文章分成句子</span><br><span class="line">　　　　sentences = getSentences(originalText)</span><br><span class="line"></span><br><span class="line">　　　　// 选择关键词首先出现的句子</span><br><span class="line">　　　　setSummarySentences = &#123;&#125;</span><br><span class="line">　　　　foreach word in contentWordsSortbyFreq:</span><br><span class="line">　　　　　　firstMatchingSentence = search(sentences, word)</span><br><span class="line">　　　　　　setSummarySentences.add(firstMatchingSentence)</span><br><span class="line">　　　　　　if setSummarySentences.size() = maxSummarySize:</span><br><span class="line">　　　　　　　　break</span><br><span class="line"></span><br><span class="line">　　　　// 将选中的句子按照出现顺序，组成摘要</span><br><span class="line">　　　　summary = ""</span><br><span class="line">　　　　foreach sentence in sentences:</span><br><span class="line">　　　　　　if sentence in setSummarySentences:</span><br><span class="line">　　　　　　　　summary = summary + " " + sentence</span><br><span class="line"></span><br><span class="line">　　　　return summary</span><br></pre></td></tr></table></figure>

<h2 id="N-gram-模型"><a href="#N-gram-模型" class="headerlink" title="N-gram 模型"></a>N-gram 模型</h2><p>是基于统计的语言模型</p>
<p>N 代表 考虑的前面词的相关的个数</p>
<p>当 m=1, 一个一元模型（unigram model)即为 ：</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkdm46wtt1j30hn01pglk.jpg" alt="img"></p>
<p>当 m=2, 一个二元模型（bigram model)即为 ：</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkdm47wcatj30h701qjrc.jpg" alt="img"></p>
<p>当 m=3, 一个三元模型（trigram model)即为</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkdm47irxhj30hn01pglk.jpg" alt="img"></p>
<p>而N-Gram模型也就是这样，当m=1，叫1-gram或者unigram ；m=2，叫2-gram或者bigram ；当 m=3叫3-gram或者trigram ；当m=N时，就表示的是N-gram啦。</p>
<p>应用场景：【2】<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkdm3ow0nij315a0eoww6.jpg" alt="image-20201105003619753"></p>
<h2 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h2><blockquote>
<p>Effectively, Word2Vec is based on distributional hypothesis where the context for each word is in its nearby words. Hence, by looking at its neighbouring words, we can attempt to predict the target word.</p>
<p>——–<a href="https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281" target="_blank" rel="noopener">An implementation guide to Word2Vec using NumPy and Google Sheets: Learn the inner workings of Word2Vec</a></p>
<p>中译：<a href="https://www.leiphone.com/news/201812/2o1E1Xh53PAfoXgD.html" target="_blank" rel="noopener">手把手教你NumPy来实现Word2vec</a></p>
</blockquote>
<ul>
<li>Cbow 与skip-gram区别</li>
</ul>
<p>Skip-gram: works well with small amount of the training data, represents well even rare words or phrases<br>CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>
<h3 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h3><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/27234078" target="_blank" rel="noopener">理解 Word2Vec 之 Skip-Gram 模型</a></p>
<p>里面介绍了负采样 的细节</p>
</blockquote>
<p>一般使用跳字模型的中心词向量作为词的表征向量。</p>
<ul>
<li>skip-gram 中概率乘积的理解【3】</li>
</ul>
<p>条件概率公式如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgkdbzn13j30lx05hjse.jpg" alt="formula"></p>
<p>根据其神经网络结构图，可知两个向量其实分别对应于两个 不同矩阵W中的行和列</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgkdcndbvj30fx0a7jrv.jpg" alt="skip-gram"></p>
<blockquote>
<p>Skip-gram 损失函数</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgkk1736nj314i0c8tmd.jpg" alt="image-20201107135928187"></p>
<h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><p>fastText 是 2016年 FAIR(Facebook AIResearch) 推出的一款文本分类与向量化工具。</p>
<p>fastText 提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入 word2vec 中的 <strong>skip-gram</strong>。</p>
<p>比如，对于单词‘where’，当n = 3时，我们得到所有 ⻓度为3的子词:“&lt;wh”“whe”“her”“ere”“re&gt;”以及特殊子词“<where>”。将中心词向量表示成单词的子词向量之和。</where></p>
<blockquote>
<p>构词学（morphology）作为语言学的一个重要分支，研究的正是词的内部结构和形成方式。</p>
</blockquote>
<p>在 word2vec 中，我们并没有直接利用构词学中的信息。无论是在 <strong>skip-gram</strong> 还是 <strong>CBOW</strong> 中，我们将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。</p>
<ul>
<li><a href="https://heleifz.github.io/14732610572844.html" target="_blank" rel="noopener">fastText 源码分析</a></li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgv92i6kfj31ji0u0te3.jpg" alt="image-20200308224944226"></p>
<ul>
<li>FastText v0.1.0 中文文档</li>
</ul>
<p><a href="https://www.bookstack.cn/books/fasttext-doc-zh" target="_blank" rel="noopener">https://www.bookstack.cn/books/fasttext-doc-zh</a></p>
<h2 id="textRNN-amp-textCNN"><a href="#textRNN-amp-textCNN" class="headerlink" title="textRNN &amp; textCNN"></a>textRNN &amp; textCNN</h2><p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkglh8qur3j30os0kwn68.jpg" alt></p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkglhog3l4j30n20j40un.jpg" alt></p>
<h2 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h2><ul>
<li><p><a href="https://www.jiqizhixin.com/articles/2019-02-28-11" target="_blank" rel="noopener">出身清华姚班，斯坦福博士毕业，她的毕业论文成了「爆款」</a></p>
</li>
<li><p><a href="https://blog.csdn.net/GitChat/article/details/78087377" target="_blank" rel="noopener">自然语言处理在电商的技术实践</a></p>
</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giu1gfhogqj31b20u0x6p.jpg" alt="image-20200917225555486"></p>
<ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/58931044" target="_blank" rel="noopener">解读NLP深度学习的各类模型</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giu1wclgdej31dr0u0u0x.jpg" alt="image-20200917231113728"></p>
</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="noopener">TF-IDF与余弦相似性的应用（一）：自动提取关键词</a><a href="#toc1">🔼</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34219483" target="_blank" rel="noopener">了解N-Gram模型</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/53425736" target="_blank" rel="noopener">word2vec详解（CBOW，skip-gram，负采样，分层Softmax）</a></li>
<li></li>
</ol>
</div></article></div><div class="right-container"><div class="widget"><div id="arAnchorBar"></div></div></div></section></div><div class="right-menu"></div><div class="modal search-modal"><div class="input-field"><input type="text" id="search_input"><label for="search-input">搜索</label></div><div id="search_result" class="search-result"></div></div><div class="blog-overlay"></div><footer class="row"><div class="footer-con"><div class="paginator"><a href="/2020/07/25/简历思考/" title="⭐️简历思考" class="prev">PREV</a><a href="/2020/06/15/bert专题/" title="⭐️bert专题" class="next">NEXT</a></div><div class="copyright"><p>© 2019 - 2021 <a target="_blank">Dufy</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <br> and <a href="https://github.com/Bulandent/hexo-theme-bubuzou" target="_blank">hexo-theme-bubuzou</a></p><p> <span style="padding-right: 6px;">闽ICP备16007301号-2</span></p></div><div class="totop"><i></i></div></div></footer><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="/scripts/jquery-1.8.2.min.js"></script><script src="/scripts/ar-anchor.js"></script><script src="/scripts/main.js"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>