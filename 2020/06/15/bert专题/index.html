<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>⭐️bert专题 | Note</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="[TOC]">
<meta name="keywords" content="nlp,bert">
<meta property="og:type" content="article">
<meta property="og:title" content="⭐️bert专题">
<meta property="og:url" content="http://yoursite.com/2020/06/15/bert专题/index.html">
<meta property="og:site_name" content="Note">
<meta property="og:description" content="[TOC]">
<meta property="og:locale" content="zh-CN//语言">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggiv2d0ikgj31ap0u0jw7.jpg">
<meta property="og:image" content="https://i.loli.net/2021/03/07/VJC7hpc9QT2mu3s.jpg">
<meta property="og:image" content="https://i.loli.net/2021/03/07/zVo8r7HbxByp1F9.png">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEly1gnjpevk5kuj31240k2e7k.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjxtmy8fsj31qm0u01ky.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh741wtwkbj30w60ekt9j.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl4o410dzj317m0d8wu1.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl4u2gzp0j31100ki7rx.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl51cogktj31080goqpy.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl5drs71jj31js0ay7o2.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjwaw2dvsj30gu0gcwls.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl5qvxs8kj31hg0iox6p.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghinb57v39j316t0u0npd.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh73xk6h72j30x40pe77t.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh73zadk1wj31400ndn04.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh73vedyqjg30hs0fq4qp.gif">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggqvl6g6jfj31jh0u04qq.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0eauu7dj31ay0u0hdt.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggwqexq3nij31fc0t0e81.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggwqay6dzmj316v0u0x5a.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggw8z1p21oj30x10u0b29.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0lq5nsjj31q80u0e81.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0kjixpej31f80q8qp7.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz1a8a67yj316w0u07wh.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggon41ke13j31fs0to7wh.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1giswgduusej31ef0u0kjl.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0aff3u0j31ar0u0b29.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0081Kckwgy1glgw2ryo91j31po0qekjm.jpg">
<meta property="og:updated_time" content="2021-03-07T14:22:00.237Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="⭐️bert专题">
<meta name="twitter:description" content="[TOC]">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggiv2d0ikgj31ap0u0jw7.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Note" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Note</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-bert专题" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/15/bert专题/" class="article-date">
  <time datetime="2020-06-14T16:00:00.000Z" itemprop="datePublished">2020-06-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      ⭐️bert专题
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggiv2d0ikgj31ap0u0jw7.jpg" alt="image-20200705000546978"></p>
<a id="more"></a>



<h2 id="bert-模型结构"><a href="#bert-模型结构" class="headerlink" title="bert 模型结构"></a>bert 模型结构</h2><p>BERT 模型的结构主要由三部分构成：</p>
<ul>
<li><p>输入层</p>
</li>
<li><p>编码层</p>
</li>
<li><p>任务相关层 </p>
<p><img src="https://i.loli.net/2021/03/07/VJC7hpc9QT2mu3s.jpg" alt="img"> </p>
</li>
</ul>
<h3 id="输入表示"><a href="#输入表示" class="headerlink" title="输入表示"></a>输入表示</h3><p>输入表示为每个词对应的词向量，segment向量，位置向量相加而成。</p>
<p><img src="https://i.loli.net/2021/03/07/zVo8r7HbxByp1F9.png" alt="img"> </p>
<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>预训练整体过程：</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnjpevk5kuj31240k2e7k.jpg" alt="image-20210211170514652"></p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>参考：<a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码）</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjxtmy8fsj31qm0u01ky.jpg" alt="image-20200708223353867"></p>
<ul>
<li>包含种类</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh741wtwkbj30w60ekt9j.jpg" alt="img"></p>
<ul>
<li>attention 本质</li>
</ul>
<p>以seq2seq为例，不采用attention 的结构如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl4o410dzj317m0d8wu1.jpg" alt="image-20200709231621649"></p>
<p>采用attention 后，结构如下：</p>
<p>也就是，由原先的固定的中间语义表示C，变为随着输出Y来动态变化的Ci，从而实现对于输入信息的选择性关注。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl4u2gzp0j31100ki7rx.jpg" alt="image-20200709232205460"></p>
<p>脱离 encoder-decoder 架构，attention 要实现的一个操作是：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl51cogktj31080goqpy.jpg" alt="image-20200709232905641"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl5drs71jj31js0ay7o2.jpg" alt="image-20200709234102024"></p>
<p>Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。</p>
<p>优点：</p>
<ol>
<li>远距离相互依赖特征的捕获</li>
<li>并行化</li>
</ol>
<ul>
<li>mask作用</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjwaw2dvsj30gu0gcwls.jpg" alt="image-20200708214115337"></p>
<p>参见：<a href="https://zhuanlan.zhihu.com/p/139595546" target="_blank" rel="noopener">NLP 中的Mask全解</a></p>
<blockquote>
<p>对于NLP中mask的作用，先上结论：</p>
<blockquote>
<p>1、padding mask：处理非定长序列，区分padding和非padding部分，如在RNN等模型和Attention机制中的应用等<br>2、sequence mask：防止标签泄露，如：Transformer decoder中的mask矩阵，BERT中的[Mask]位，XLNet中的mask矩阵等<br>PS：padding mask 和 sequence mask非官方命名</p>
</blockquote>
</blockquote>
<ul>
<li>发展</li>
</ul>
<blockquote>
<p>本质上，注意⼒机制能够为表征中较有价值的部分分配较多的计算资源。这个有趣的想法⾃提出后得到了快速发展，特别是启发了依靠注意⼒机制来编码输⼊序列并解码出输出序列的<strong>变换器（Transformer）模型</strong>的设计。变换器抛弃了卷积神经⽹络和循环神经⽹络的架构。它在计算效率上⽐基于循环神经⽹络的编码器—解码器模型通常更具明显优势。含注意⼒机制的变换器的编码结构在后来的<strong>BERT预训练模型</strong>中得以应⽤并令后者⼤放异彩：微调后的模型在多达11项⾃然语⾔处理任务中取得了当时最先进的结果。不久后，同样是基于变换器设计的<strong>GPT-2模型</strong>于新收集的语料数据集预训练后，在7个未参与训练的语⾔模型数据集上均取得了当时最先进的结果。除了⾃然语⾔处理领域，注意⼒机制还被⼴泛⽤于图像分类、⾃动图像描述、唇语解读以及语⾳识别。</p>
</blockquote>
<p>GitHub:<a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl5qvxs8kj31hg0iox6p.jpg" alt="image-20200709235337403"></p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><blockquote>
<p>代码实现</p>
</blockquote>
<p><a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">https://github.com/harvardnlp/annotated-transformer</a></p>
<p>文章：</p>
<ol>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>
<li><a href="https://www.cnblogs.com/guoyaohua/p/transformer.html【译文】" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/transformer.html【译文】</a></li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghinb57v39j316t0u0npd.jpg" alt="image-20200807230450833"></p>
<blockquote>
<p><strong>强烈推荐：<a href="https://zhuanlan.zhihu.com/p/98650532" target="_blank" rel="noopener">Transformer原理：自底向上解析</a></strong></p>
</blockquote>
<ul>
<li>transformer结构：</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh73xk6h72j30x40pe77t.jpg" alt="img"></p>
<ul>
<li>解读</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh73zadk1wj31400ndn04.jpg" alt="img"></p>
<blockquote>
<p>上图整体上依然是个seq2seq结构，左边表示Encoder，右边表示Decoder，两边的N均表示将对应的block重复N次。</p>
<ol>
<li>首先将带有位置信息的Embedding形式的输入sequence <code>a</code>经过Multi-Head Attention处理成sequence <code>b</code>；</li>
<li>Add：将Multi-Head Attention的input <code>a</code>和output <code>b</code>相加得到<code>b&#39;</code>；</li>
<li>Norm：将<code>b&#39;</code>进行Layer Norm规范化处理；</li>
<li>为了增强模型表示能力，将上一步产生的结果递交给两层全连接的FFN，第一层ReLU激活，第二层线性激活，然后继续Add&amp;Norm处理；</li>
<li>将1-4步重复N次后进入Decoder，首先依然进行Multi-Head Attention处理，与上边不同的是加入Masked方法使得模型只能attend到已经产生的sequence，然后继续Add&amp;Norm处理；</li>
<li>将Encoder的结果进行Multi-Head Attention处理；</li>
<li>将上一步的结果进行FNN&amp;Add&amp;Norm操作；</li>
<li>将5-7步重复N次后得到最终output。</li>
</ol>
</blockquote>
<ul>
<li>实际应用时的流程</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh73vedyqjg30hs0fq4qp.gif" alt="img"></p>
<h2 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h2><p><a href="https://mp.weixin.qq.com/s/S0dR--M88apEf5RV5dLmWw" target="_blank" rel="noopener">「行知」NLP新星：BERT的优雅解读</a></p>
<p><a href="https://www.cnblogs.com/sandwichnlp/p/11947627.html" target="_blank" rel="noopener">预训练语言模型整理（ELMo/GPT/BERT…）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/111513291" target="_blank" rel="noopener">你保存的BERT模型为什么那么大？</a></p>
<h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="美团BERT的探索和实践"><a href="#美团BERT的探索和实践" class="headerlink" title="美团BERT的探索和实践"></a><a href="https://tech.meituan.com/2019/11/14/nlp-bert-practice.html" target="_blank" rel="noopener">美团BERT的探索和实践</a></h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggqvl6g6jfj31jh0u04qq.jpg" alt="image-20200714223506424"></p>
<h3 id="NLP-15-分钟搭建中文文本分类模型"><a href="#NLP-15-分钟搭建中文文本分类模型" class="headerlink" title="NLP - 15 分钟搭建中文文本分类模型"></a>NLP - 15 分钟搭建中文文本分类模型</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0eauu7dj31ay0u0hdt.jpg" alt="image-20200721232624433"></p>
<h3 id="Albert-–文本分类"><a href="#Albert-–文本分类" class="headerlink" title="Albert –文本分类"></a>Albert –文本分类</h3><ul>
<li>NLP（二十八）多标签文本分类</li>
</ul>
<p><a href="https://blog.csdn.net/jclian91/article/details/105386190" target="_blank" rel="noopener">https://blog.csdn.net/jclian91/article/details/105386190</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggwqexq3nij31fc0t0e81.jpg" alt="image-20200720000956746"></p>
<h3 id="Albert-–NER"><a href="#Albert-–NER" class="headerlink" title="Albert –NER"></a>Albert –NER</h3><p><a href="https://blog.csdn.net/jclian91/article/details/104806598" target="_blank" rel="noopener">NLP（二十四）利用ALBERT实现命名实体识别</a></p>
<p>GitHub：<a href="https://github.com/percent4/ALBERT_NER_KERAS" target="_blank" rel="noopener">https://github.com/percent4/ALBERT_NER_KERAS</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggwqay6dzmj316v0u0x5a.jpg" alt="image-20200720000606291"></p>
<ul>
<li>NLP（三十）利用ALBERT和机器学习来做<strong>文本分类</strong></li>
</ul>
<p><a href="https://mp.weixin.qq.com/s/rt0yGcsYHQVRsXUYVmGoyQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/rt0yGcsYHQVRsXUYVmGoyQ</a></p>
<p>结合了常规机器学习方法，进行NER 效果的展示。当然，也可以结合深度学习：<a href="http://mp.weixin.qq.com/s?__biz=MzU2NTYyMDk5MQ==&mid=2247484526&idx=1&sn=89ded8db053fbd66b1f24d964a0f31a6&chksm=fcb9bdfecbce34e888f83c8d2109807e46a514c5818c9fabdb30e96ab5bd0d69a5e86c201165&scene=21#wechat_redirect" target="_blank" rel="noopener">NLP（二十二）利用ALBERT实现文本二分类</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggw8z1p21oj30x10u0b29.jpg" alt="image-20200719140624964"></p>
<h3 id="NLP-基于kashgari和BERT实现中文命名实体识别（NER）"><a href="#NLP-基于kashgari和BERT实现中文命名实体识别（NER）" class="headerlink" title="NLP 基于kashgari和BERT实现中文命名实体识别（NER）"></a>NLP 基于kashgari和BERT实现中文命名实体识别（NER）</h3><p><a href="https://www.shuzhiduo.com/A/x9J2EabKz6/" target="_blank" rel="noopener">https://www.shuzhiduo.com/A/x9J2EabKz6/</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0lq5nsjj31q80u0e81.jpg" alt="image-20200721233332717"></p>
<p>同时，参考： NLP - 基于 BERT 的中文命名实体识别（NER)<a href="https://eliyar.biz/nlp_chinese_bert_ner/" target="_blank" rel="noopener">https://eliyar.biz/nlp_chinese_bert_ner/</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0kjixpej31f80q8qp7.jpg" alt="image-20200721233222760"></p>
<p>自己的github:</p>
<p><a href="http://localhost:8888/notebooks/machine_learning/BERT/kashgari-BERT-ner.ipynb" target="_blank" rel="noopener">http://localhost:8888/notebooks/machine_learning/BERT/kashgari-BERT-ner.ipynb</a></p>
<h3 id="Bert-bilstm-CRF实体识别-bertBiLSTMCRF"><a href="#Bert-bilstm-CRF实体识别-bertBiLSTMCRF" class="headerlink" title="Bert+bilstm+CRF实体识别,bertBiLSTMCRF"></a><a href="https://www.pythonf.cn/read/57841" target="_blank" rel="noopener">Bert+bilstm+CRF实体识别,bertBiLSTMCRF</a></h3><p>基于Keras实现</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz1a8a67yj316w0u07wh.jpg" alt="image-20200721235659840"></p>
<h3 id="BERT中文命名实体识别TensorFlow实现"><a href="#BERT中文命名实体识别TensorFlow实现" class="headerlink" title="BERT中文命名实体识别TensorFlow实现"></a><a href="https://blog.csdn.net/macanv/article/details/85684284" target="_blank" rel="noopener">BERT中文命名实体识别TensorFlow实现</a></h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggon41ke13j31fs0to7wh.jpg" alt="image-20200713001108626"></p>
<h3 id="Bert-flask"><a href="#Bert-flask" class="headerlink" title="Bert flask"></a>Bert flask</h3><p><a href="https://github.com/luoyangbiao/bert_flask" target="_blank" rel="noopener">https://github.com/luoyangbiao/bert_flask</a></p>
<p>使用bert训练MRPC数据集，写成API接口模式以及简易的html界面</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giswgduusej31ef0u0kjl.jpg" alt="image-20200916231711260"></p>
<h3 id="BERT相关论文、文章和代码资源汇总"><a href="#BERT相关论文、文章和代码资源汇总" class="headerlink" title="BERT相关论文、文章和代码资源汇总"></a><a href="https://www.52nlp.cn/bert-paper-论文-文章-代码资源汇总" target="_blank" rel="noopener">BERT相关论文、文章和代码资源汇总</a></h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0aff3u0j31ar0u0b29.jpg" alt="image-20200714000535300"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li></li>
<li><p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glgw2ryo91j31po0qekjm.jpg" alt="image-20201208235818627"></p>
</li>
<li><p><a href="https://www.6aiq.com/article/1587091834767" target="_blank" rel="noopener"> PTMs：史上最全面总结 NLP 预训练模型</a></p>
</li>
<li></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/15/bert专题/" data-id="ckor8mduz000awnonygd62lhx" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/bert/">bert</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/">nlp</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/06/22/nlp笔记/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          ⭐️nlp笔记
        
      </div>
    </a>
  
  
    <a href="/2020/06/14/NER专题/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">⭐️NER专题</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/CV/">CV</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/网页/">网页</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/">Git</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bert/">bert</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/">nlp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyhanlp/">pyhanlp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/力扣/">力扣</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大数据/">大数据</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/并发/">并发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/底层/">底层</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/搜索-推荐/">搜索/推荐</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爬虫/">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/电影/">电影</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/编辑/">编辑</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/网站/">网站</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/网课/">网课</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/规范/">规范</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/视觉/">视觉</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/阅读/">阅读</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Python/" style="font-size: 12px;">Python</a> <a href="/tags/bert/" style="font-size: 10px;">bert</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/nlp/" style="font-size: 18px;">nlp</a> <a href="/tags/pyhanlp/" style="font-size: 10px;">pyhanlp</a> <a href="/tags/力扣/" style="font-size: 12px;">力扣</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a> <a href="/tags/并发/" style="font-size: 10px;">并发</a> <a href="/tags/底层/" style="font-size: 18px;">底层</a> <a href="/tags/搜索-推荐/" style="font-size: 10px;">搜索/推荐</a> <a href="/tags/机器学习/" style="font-size: 16px;">机器学习</a> <a href="/tags/深度学习/" style="font-size: 16px;">深度学习</a> <a href="/tags/爬虫/" style="font-size: 10px;">爬虫</a> <a href="/tags/电影/" style="font-size: 10px;">电影</a> <a href="/tags/算法/" style="font-size: 20px;">算法</a> <a href="/tags/编辑/" style="font-size: 10px;">编辑</a> <a href="/tags/网站/" style="font-size: 10px;">网站</a> <a href="/tags/网课/" style="font-size: 14px;">网课</a> <a href="/tags/规范/" style="font-size: 10px;">规范</a> <a href="/tags/视觉/" style="font-size: 14px;">视觉</a> <a href="/tags/阅读/" style="font-size: 14px;">阅读</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2119/10/">October 2119</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2100/01/">January 2100</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2000/01/">January 2000</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2119/10/28/待研究问题/">⭐️待研究问题</a>
          </li>
        
          <li>
            <a href="/2100/01/29/网站推荐/">⭐️网站推荐</a>
          </li>
        
          <li>
            <a href="/2021/11/08/copy/">⭐️copy</a>
          </li>
        
          <li>
            <a href="/2021/04/04/数据结构与算法之美（加餐）/">⭐️数据结构与算法之美（加餐）</a>
          </li>
        
          <li>
            <a href="/2021/04/02/数据结构与算法之美（基础篇）/">⭐️数据结构与算法之美（基础篇）</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Dufy<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>