<!DOCTYPE html><html lang="zh-CN//语言"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="1EB8XoOl0C"><meta name="google-site-verification" content="K7thEgdLm0UfRWJ5MGdF7sCcjClSzAlxFLPv2Oz5CGM"><title> ⭐️bert专题 · Note</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="⭐️bert专题 - Dufy"><meta name="keywords"><meta name="author" content="Dufy"><link rel="short icon" href="/images/favicon.ico"><link rel="stylesheet" href="/css/bubuzou.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Note"><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script></head><body><header><div class="header row"> <a href="/" class="logo-link"><img src="/images/logo.png"></a><ul id="nav_list" class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" data-hover="博文" class="nav-list-link">博文</a></li><li class="nav-list-item"><a href="/archives/" target="_self" data-hover="归档" class="nav-list-link">归档</a></li></ul><div class="search"><a id="search_btn" href="#search"></a></div><div id="nav_btn" class="nav-btn"><span></span><span></span><span></span></div></div></header><div class="row scroll-con"><section class="container"><!-- for archive page--><div id="postAr" class="post"><article class="post-block"><h1 class="post-title">⭐️bert专题</h1><div class="post-info">2020-06-15</div><div class="post-content"><p>[TOC]</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggiv2d0ikgj31ap0u0jw7.jpg" alt="image-20200705000546978"></p>
<a id="more"></a>



<h2 id="bert-模型结构"><a href="#bert-模型结构" class="headerlink" title="bert 模型结构"></a>bert 模型结构</h2><p>BERT 模型的结构主要由三部分构成：</p>
<ul>
<li><p>输入层</p>
</li>
<li><p>编码层</p>
</li>
<li><p>任务相关层 </p>
<p><img src="https://i.loli.net/2021/03/07/VJC7hpc9QT2mu3s.jpg" alt="img"> </p>
</li>
</ul>
<h3 id="输入表示"><a href="#输入表示" class="headerlink" title="输入表示"></a>输入表示</h3><p>输入表示为每个词对应的词向量，segment向量，位置向量相加而成。</p>
<p><img src="https://i.loli.net/2021/03/07/zVo8r7HbxByp1F9.png" alt="img"> </p>
<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>预训练整体过程：</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnjpevk5kuj31240k2e7k.jpg" alt="image-20210211170514652"></p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>参考：<a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码）</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjxtmy8fsj31qm0u01ky.jpg" alt="image-20200708223353867"></p>
<ul>
<li>包含种类</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh741wtwkbj30w60ekt9j.jpg" alt="img"></p>
<ul>
<li>attention 本质</li>
</ul>
<p>以seq2seq为例，不采用attention 的结构如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl4o410dzj317m0d8wu1.jpg" alt="image-20200709231621649"></p>
<p>采用attention 后，结构如下：</p>
<p>也就是，由原先的固定的中间语义表示C，变为随着输出Y来动态变化的Ci，从而实现对于输入信息的选择性关注。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl4u2gzp0j31100ki7rx.jpg" alt="image-20200709232205460"></p>
<p>脱离 encoder-decoder 架构，attention 要实现的一个操作是：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl51cogktj31080goqpy.jpg" alt="image-20200709232905641"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl5drs71jj31js0ay7o2.jpg" alt="image-20200709234102024"></p>
<p>Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。</p>
<p>优点：</p>
<ol>
<li>远距离相互依赖特征的捕获</li>
<li>并行化</li>
</ol>
<ul>
<li>mask作用</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjwaw2dvsj30gu0gcwls.jpg" alt="image-20200708214115337"></p>
<p>参见：<a href="https://zhuanlan.zhihu.com/p/139595546" target="_blank" rel="noopener">NLP 中的Mask全解</a></p>
<blockquote>
<p>对于NLP中mask的作用，先上结论：</p>
<blockquote>
<p>1、padding mask：处理非定长序列，区分padding和非padding部分，如在RNN等模型和Attention机制中的应用等<br>2、sequence mask：防止标签泄露，如：Transformer decoder中的mask矩阵，BERT中的[Mask]位，XLNet中的mask矩阵等<br>PS：padding mask 和 sequence mask非官方命名</p>
</blockquote>
</blockquote>
<ul>
<li>发展</li>
</ul>
<blockquote>
<p>本质上，注意⼒机制能够为表征中较有价值的部分分配较多的计算资源。这个有趣的想法⾃提出后得到了快速发展，特别是启发了依靠注意⼒机制来编码输⼊序列并解码出输出序列的<strong>变换器（Transformer）模型</strong>的设计。变换器抛弃了卷积神经⽹络和循环神经⽹络的架构。它在计算效率上⽐基于循环神经⽹络的编码器—解码器模型通常更具明显优势。含注意⼒机制的变换器的编码结构在后来的<strong>BERT预训练模型</strong>中得以应⽤并令后者⼤放异彩：微调后的模型在多达11项⾃然语⾔处理任务中取得了当时最先进的结果。不久后，同样是基于变换器设计的<strong>GPT-2模型</strong>于新收集的语料数据集预训练后，在7个未参与训练的语⾔模型数据集上均取得了当时最先进的结果。除了⾃然语⾔处理领域，注意⼒机制还被⼴泛⽤于图像分类、⾃动图像描述、唇语解读以及语⾳识别。</p>
</blockquote>
<p>GitHub:<a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggl5qvxs8kj31hg0iox6p.jpg" alt="image-20200709235337403"></p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><blockquote>
<p>代码实现</p>
</blockquote>
<p><a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">https://github.com/harvardnlp/annotated-transformer</a></p>
<p>文章：</p>
<ol>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>
<li><a href="https://www.cnblogs.com/guoyaohua/p/transformer.html【译文】" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/transformer.html【译文】</a></li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghinb57v39j316t0u0npd.jpg" alt="image-20200807230450833"></p>
<blockquote>
<p><strong>强烈推荐：<a href="https://zhuanlan.zhihu.com/p/98650532" target="_blank" rel="noopener">Transformer原理：自底向上解析</a></strong></p>
</blockquote>
<ul>
<li>transformer结构：</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh73xk6h72j30x40pe77t.jpg" alt="img"></p>
<ul>
<li>解读</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh73zadk1wj31400ndn04.jpg" alt="img"></p>
<blockquote>
<p>上图整体上依然是个seq2seq结构，左边表示Encoder，右边表示Decoder，两边的N均表示将对应的block重复N次。</p>
<ol>
<li>首先将带有位置信息的Embedding形式的输入sequence <code>a</code>经过Multi-Head Attention处理成sequence <code>b</code>；</li>
<li>Add：将Multi-Head Attention的input <code>a</code>和output <code>b</code>相加得到<code>b&#39;</code>；</li>
<li>Norm：将<code>b&#39;</code>进行Layer Norm规范化处理；</li>
<li>为了增强模型表示能力，将上一步产生的结果递交给两层全连接的FFN，第一层ReLU激活，第二层线性激活，然后继续Add&amp;Norm处理；</li>
<li>将1-4步重复N次后进入Decoder，首先依然进行Multi-Head Attention处理，与上边不同的是加入Masked方法使得模型只能attend到已经产生的sequence，然后继续Add&amp;Norm处理；</li>
<li>将Encoder的结果进行Multi-Head Attention处理；</li>
<li>将上一步的结果进行FNN&amp;Add&amp;Norm操作；</li>
<li>将5-7步重复N次后得到最终output。</li>
</ol>
</blockquote>
<ul>
<li>实际应用时的流程</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh73vedyqjg30hs0fq4qp.gif" alt="img"></p>
<h2 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h2><p><a href="https://mp.weixin.qq.com/s/S0dR--M88apEf5RV5dLmWw" target="_blank" rel="noopener">「行知」NLP新星：BERT的优雅解读</a></p>
<p><a href="https://www.cnblogs.com/sandwichnlp/p/11947627.html" target="_blank" rel="noopener">预训练语言模型整理（ELMo/GPT/BERT…）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/111513291" target="_blank" rel="noopener">你保存的BERT模型为什么那么大？</a></p>
<h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="美团BERT的探索和实践"><a href="#美团BERT的探索和实践" class="headerlink" title="美团BERT的探索和实践"></a><a href="https://tech.meituan.com/2019/11/14/nlp-bert-practice.html" target="_blank" rel="noopener">美团BERT的探索和实践</a></h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggqvl6g6jfj31jh0u04qq.jpg" alt="image-20200714223506424"></p>
<h3 id="NLP-15-分钟搭建中文文本分类模型"><a href="#NLP-15-分钟搭建中文文本分类模型" class="headerlink" title="NLP - 15 分钟搭建中文文本分类模型"></a>NLP - 15 分钟搭建中文文本分类模型</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0eauu7dj31ay0u0hdt.jpg" alt="image-20200721232624433"></p>
<h3 id="Albert-–文本分类"><a href="#Albert-–文本分类" class="headerlink" title="Albert –文本分类"></a>Albert –文本分类</h3><ul>
<li>NLP（二十八）多标签文本分类</li>
</ul>
<p><a href="https://blog.csdn.net/jclian91/article/details/105386190" target="_blank" rel="noopener">https://blog.csdn.net/jclian91/article/details/105386190</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggwqexq3nij31fc0t0e81.jpg" alt="image-20200720000956746"></p>
<h3 id="Albert-–NER"><a href="#Albert-–NER" class="headerlink" title="Albert –NER"></a>Albert –NER</h3><p><a href="https://blog.csdn.net/jclian91/article/details/104806598" target="_blank" rel="noopener">NLP（二十四）利用ALBERT实现命名实体识别</a></p>
<p>GitHub：<a href="https://github.com/percent4/ALBERT_NER_KERAS" target="_blank" rel="noopener">https://github.com/percent4/ALBERT_NER_KERAS</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggwqay6dzmj316v0u0x5a.jpg" alt="image-20200720000606291"></p>
<ul>
<li>NLP（三十）利用ALBERT和机器学习来做<strong>文本分类</strong></li>
</ul>
<p><a href="https://mp.weixin.qq.com/s/rt0yGcsYHQVRsXUYVmGoyQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/rt0yGcsYHQVRsXUYVmGoyQ</a></p>
<p>结合了常规机器学习方法，进行NER 效果的展示。当然，也可以结合深度学习：<a href="http://mp.weixin.qq.com/s?__biz=MzU2NTYyMDk5MQ==&mid=2247484526&idx=1&sn=89ded8db053fbd66b1f24d964a0f31a6&chksm=fcb9bdfecbce34e888f83c8d2109807e46a514c5818c9fabdb30e96ab5bd0d69a5e86c201165&scene=21#wechat_redirect" target="_blank" rel="noopener">NLP（二十二）利用ALBERT实现文本二分类</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggw8z1p21oj30x10u0b29.jpg" alt="image-20200719140624964"></p>
<h3 id="NLP-基于kashgari和BERT实现中文命名实体识别（NER）"><a href="#NLP-基于kashgari和BERT实现中文命名实体识别（NER）" class="headerlink" title="NLP 基于kashgari和BERT实现中文命名实体识别（NER）"></a>NLP 基于kashgari和BERT实现中文命名实体识别（NER）</h3><p><a href="https://www.shuzhiduo.com/A/x9J2EabKz6/" target="_blank" rel="noopener">https://www.shuzhiduo.com/A/x9J2EabKz6/</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0lq5nsjj31q80u0e81.jpg" alt="image-20200721233332717"></p>
<p>同时，参考： NLP - 基于 BERT 的中文命名实体识别（NER)<a href="https://eliyar.biz/nlp_chinese_bert_ner/" target="_blank" rel="noopener">https://eliyar.biz/nlp_chinese_bert_ner/</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0kjixpej31f80q8qp7.jpg" alt="image-20200721233222760"></p>
<p>自己的github:</p>
<p><a href="http://localhost:8888/notebooks/machine_learning/BERT/kashgari-BERT-ner.ipynb" target="_blank" rel="noopener">http://localhost:8888/notebooks/machine_learning/BERT/kashgari-BERT-ner.ipynb</a></p>
<h3 id="Bert-bilstm-CRF实体识别-bertBiLSTMCRF"><a href="#Bert-bilstm-CRF实体识别-bertBiLSTMCRF" class="headerlink" title="Bert+bilstm+CRF实体识别,bertBiLSTMCRF"></a><a href="https://www.pythonf.cn/read/57841" target="_blank" rel="noopener">Bert+bilstm+CRF实体识别,bertBiLSTMCRF</a></h3><p>基于Keras实现</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz1a8a67yj316w0u07wh.jpg" alt="image-20200721235659840"></p>
<h3 id="BERT中文命名实体识别TensorFlow实现"><a href="#BERT中文命名实体识别TensorFlow实现" class="headerlink" title="BERT中文命名实体识别TensorFlow实现"></a><a href="https://blog.csdn.net/macanv/article/details/85684284" target="_blank" rel="noopener">BERT中文命名实体识别TensorFlow实现</a></h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggon41ke13j31fs0to7wh.jpg" alt="image-20200713001108626"></p>
<h3 id="Bert-flask"><a href="#Bert-flask" class="headerlink" title="Bert flask"></a>Bert flask</h3><p><a href="https://github.com/luoyangbiao/bert_flask" target="_blank" rel="noopener">https://github.com/luoyangbiao/bert_flask</a></p>
<p>使用bert训练MRPC数据集，写成API接口模式以及简易的html界面</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giswgduusej31ef0u0kjl.jpg" alt="image-20200916231711260"></p>
<h3 id="BERT相关论文、文章和代码资源汇总"><a href="#BERT相关论文、文章和代码资源汇总" class="headerlink" title="BERT相关论文、文章和代码资源汇总"></a><a href="https://www.52nlp.cn/bert-paper-论文-文章-代码资源汇总" target="_blank" rel="noopener">BERT相关论文、文章和代码资源汇总</a></h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggz0aff3u0j31ar0u0b29.jpg" alt="image-20200714000535300"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li></li>
<li><p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glgw2ryo91j31po0qekjm.jpg" alt="image-20201208235818627"></p>
</li>
<li><p><a href="https://www.6aiq.com/article/1587091834767" target="_blank" rel="noopener"> PTMs：史上最全面总结 NLP 预训练模型</a></p>
</li>
<li></li>
</ol>
</div></article></div><div class="right-container"><div class="widget"><div id="arAnchorBar"></div></div></div></section></div><div class="right-menu"></div><div class="modal search-modal"><div class="input-field"><input type="text" id="search_input"><label for="search-input">搜索</label></div><div id="search_result" class="search-result"></div></div><div class="blog-overlay"></div><footer class="row"><div class="footer-con"><div class="paginator"><a href="/2020/06/22/nlp笔记/" title="⭐️nlp笔记" class="prev">上一篇</a><a href="/2020/06/14/NER专题/" title="⭐️NER专题" class="next">下一篇</a></div><div class="copyright"><p>© 2019 - 2021 <a target="_blank">Dufy</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <br> and <a href="https://github.com/Bulandent/hexo-theme-bubuzou" target="_blank">hexo-theme-bubuzou</a></p><p> <span style="padding-right: 6px;">闽ICP备16007301号-2</span></p></div><div class="totop"><i></i></div></div></footer><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="/scripts/jquery-1.8.2.min.js"></script><script src="/scripts/ar-anchor.js"></script><script src="/scripts/main.js"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>