<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="1EB8XoOl0C"><meta name="google-site-verification" content="K7thEgdLm0UfRWJ5MGdF7sCcjClSzAlxFLPv2Oz5CGM"><title> ⭐️bert源码理解 · Hexo</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="⭐️bert源码理解 - John Doe"><meta name="keywords"><meta name="author" content="John Doe"><link rel="short icon" href="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/favicon.ico"><link rel="stylesheet" href="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/bubuzou.css"><link rel="search" type="application/opensearchdescription+xml" href="http://example.com/atom.xml" title="Hexo"><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body><header><div class="header row"> <a href="/" class="logo-link"><img src="https://simg.nicepng.com/png/small/31-311776_royal…-free-collection-of-free-chipmunk-download-on.png"></a><ul id="nav_list" class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" data-hover="博文" class="nav-list-link">博文</a></li><li class="nav-list-item"><a href="/archives/" target="_self" data-hover="归档" class="nav-list-link">归档</a></li><li class="nav-list-item"><a href="/about/" target="_self" data-hover="关于" class="nav-list-link">关于</a></li></ul><div class="search"><a id="search_btn" href="#search"></a></div><div id="nav_btn" class="nav-btn"><span></span><span></span><span></span></div></div></header><div class="row scroll-con"><section class="container"><!-- for archive page--><div id="postAr" class="post"><article class="post-block"><h1 class="post-title">⭐️bert源码理解</h1><div class="post-info">2021-06-06</div><div class="post-content"><span id="more"></span>



<p>原始无监督语料：sample.txt</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">koa  	s?peer Electr-onics upc8172tb 汽车 3v_1pf</span><br><span class="line"></span><br><span class="line">cel  eval  board  upc8172tb</span><br><span class="line"></span><br><span class="line">avx  corporation  478  08052c223kaz2atr</span><br><span class="line"></span><br><span class="line">24  024  101  dip  cable</span><br><span class="line"></span><br><span class="line">cap  tant, poly cha</span><br><span class="line">c326c561fag5ta7301  kemet  cap</span><br><span class="line"> ipp  tvs  二极管. ?</span><br><span class="line">665  ohms  0.1%  0.4w  2/5w</span><br><span class="line">薄膜  a140666ct  nd  te</span><br><span class="line">===</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><p>max_seq_length： 每条训练语料的所含有的最大 token数</p>
<p>short_seq_prob： 缩短句子的概率（针对上面 的max_seq_length）</p>
<h2 id="create-pretraining-data-py"><a href="#create-pretraining-data-py" class="headerlink" title="create_pretraining_data.py"></a>create_pretraining_data.py</h2><p>生成预训练的标注数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">_</span>):</span></span><br><span class="line">  tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line">  tokenizer = tokenization.FullTokenizer(</span><br><span class="line">      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)-------------（<span class="number">1</span>）</span><br><span class="line"></span><br><span class="line">  input_files = []</span><br><span class="line">  <span class="keyword">for</span> input_pattern <span class="keyword">in</span> FLAGS.input_file.split(<span class="string">&quot;,&quot;</span>):</span><br><span class="line">    input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line"></span><br><span class="line">  tf.logging.info(<span class="string">&quot;*** Reading from input files ***&quot;</span>)</span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    tf.logging.info(<span class="string">&quot;  %s&quot;</span>, input_file)</span><br><span class="line"></span><br><span class="line">  rng = random.Random(FLAGS.random_seed)</span><br><span class="line">  instances = create_training_instances(                             -------------（<span class="number">2</span>）</span><br><span class="line">      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,</span><br><span class="line">      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,</span><br><span class="line">      rng)</span><br><span class="line"></span><br><span class="line">  output_files = FLAGS.output_file.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">  tf.logging.info(<span class="string">&quot;*** Writing to output files ***&quot;</span>)</span><br><span class="line">  <span class="keyword">for</span> output_file <span class="keyword">in</span> output_files:</span><br><span class="line">    tf.logging.info(<span class="string">&quot;  %s&quot;</span>, output_file)</span><br><span class="line"></span><br><span class="line">  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,</span><br><span class="line">                                  FLAGS.max_predictions_per_seq, output_files)</span><br></pre></td></tr></table></figure>



<h3 id="1-创建分词器-tokenizer"><a href="#1-创建分词器-tokenizer" class="headerlink" title="(1)创建分词器 tokenizer"></a>(1)创建分词器 <code>tokenizer</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = tokenization.FullTokenizer(</span><br><span class="line">      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)</span><br></pre></td></tr></table></figure>

<p>并传入 词汇表和大小写标志进行初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Runs end-to-end tokenziation.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_file, do_lower_case=<span class="literal">True</span></span>):</span></span><br><span class="line">    self.vocab = load_vocab(vocab_file)  <span class="comment"># 加载读取词表，字典形式</span></span><br><span class="line">    self.inv_vocab = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.vocab.items()&#125;</span><br><span class="line">    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)</span><br><span class="line">    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;针对每篇document 先分好词（结合词典）</span></span><br><span class="line"><span class="string">    可以看出，传入的`text` 会经过两道程序：</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text):  <span class="comment"># 先对句子清洗、拆分	               				   --------1.1</span></span><br><span class="line">      <span class="keyword">for</span> sub_token <span class="keyword">in</span> self.wordpiece_tokenizer.tokenize(token):  <span class="comment"># 然后依据词表对每个分词进行进一步表示  -------1.2</span></span><br><span class="line">        split_tokens.append(sub_token)</span><br></pre></td></tr></table></figure>

<p>self.vocab：<code>(&#39;[PAD]&#39;, 0), (&#39;[unused1]&#39;, 1), (&#39;[unused2]&#39;, 2), (&#39;[unused3]&#39;, 3), (&#39;[UNK]&#39;, 4), (&#39;[CLS]&#39;, 5....</code></p>
<p>self.inv_vocab:    <code> &#123;0: &#39;[PAD]&#39;, 1: &#39;[unused1]&#39;, 2: &#39;[unused2]&#39;, 3: &#39;[unused3]&#39;, 4: &#39;[UNK]&#39;, 5: &#39;[CLS]&#39;, 6: &#39;[SEP]&#39;, 7: &#39;[MASK]...</code></p>
<ul>
<li>1.1 basic_tokenizer.tokenize(text):</li>
</ul>
<p>进行清洗和拆分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Runs basic tokenization (punctuation splitting, lower casing, etc.).&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, do_lower_case=<span class="literal">True</span></span>):</span></span><br><span class="line">    self.do_lower_case = do_lower_case</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenizes a piece of text.</span></span><br><span class="line"><span class="string">    举例：</span></span><br><span class="line"><span class="string">    text：【koa  	s�peer Electr-onics 汽车 3v_1pf】</span></span><br><span class="line"><span class="string">    输出：[&#x27;koa&#x27;, &#x27;speer&#x27;, &#x27;Electr&#x27;, &#x27;-&#x27;, &#x27;onics&#x27;, &#x27;汽&#x27;, &#x27;车&#x27;, &#x27;3v&#x27;, &#x27;_&#x27;, &#x27;1pf&#x27;]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    tf.logging.info(<span class="string">f&#x27;after [convert_to_unicode] :<span class="subst">&#123;text&#125;</span>&#x27;</span>)</span><br><span class="line">    text = self._clean_text(text)  <span class="comment">#检查单个字符是否合理, 以及空格的处理</span></span><br><span class="line">    tf.logging.info(<span class="string">f&#x27;after [_clean_text] :<span class="subst">&#123;text&#125;</span>&#x27;</span>)  <span class="comment"># 【koa   speer Electr-onics 汽车 3v_1pf】</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 中文处理</span></span><br><span class="line">    text = self._tokenize_chinese_chars(text)  <span class="comment"># 对中文进行单字的分隔</span></span><br><span class="line">    tf.logging.info(<span class="string">f&#x27;after [_tokenize_chinese_chars] :<span class="subst">&#123;text&#125;</span>&#x27;</span>)  <span class="comment"># 【koa   speer Electr-onics  汽  车  3v_1pf】</span></span><br><span class="line"></span><br><span class="line">    orig_tokens = whitespace_tokenize(text)  <span class="comment"># split（），转为列表</span></span><br><span class="line">    tf.logging.info(<span class="string">f&#x27;after [whitespace_tokenize] :<span class="subst">&#123;orig_tokens&#125;</span>&#x27;</span>) <span class="comment">#【[&#x27;koa&#x27;, &#x27;speer&#x27;, &#x27;Electr-onics&#x27;, &#x27;汽&#x27;, &#x27;车&#x27;, &#x27;3v_1pf&#x27;]】</span></span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:  <span class="comment"># 逐个选取单个词，按照标点符号进行进一步的拆分</span></span><br><span class="line">      <span class="keyword">if</span> self.do_lower_case:</span><br><span class="line">        token = token.lower()</span><br><span class="line">        token = self._run_strip_accents(token)</span><br><span class="line">        tf.logging.info(<span class="string">f&#x27;token :<span class="subst">&#123;token&#125;</span>&#x27;</span>)</span><br><span class="line">      split_tokens.extend(self._run_split_on_punc(token))</span><br><span class="line">    tf.logging.info(<span class="string">f&#x27;split_tokens :<span class="subst">&#123;split_tokens&#125;</span>&#x27;</span>)  <span class="comment">#  【[&#x27;koa&#x27;, &#x27;speer&#x27;, &#x27;Electr&#x27;, &#x27;-&#x27;, &#x27;onics&#x27;, &#x27;汽&#x27;, &#x27;车&#x27;, &#x27;3v&#x27;, &#x27;_&#x27;, &#x27;1pf&#x27;]】</span></span><br><span class="line"></span><br><span class="line">    output_tokens = whitespace_tokenize(<span class="string">&quot; &quot;</span>.join(split_tokens))</span><br><span class="line">    tf.logging.info(<span class="string">f&#x27;after [basic_tokenizer.tokenize] :<span class="subst">&#123;output_tokens&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>

<ul>
<li>1.2 wordpiece_tokenizer.tokenize(token)</li>
</ul>
<p>对上一步每个token 进行进一步的拆分，方便后续的词典表示</p>
<p>结合词典进行前缀匹配，贪心算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Runs WordPiece tokenziation.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab, unk_token=<span class="string">&quot;[UNK]&quot;</span>, max_input_chars_per_word=<span class="number">200</span></span>):</span></span><br><span class="line">    self.vocab = vocab</span><br><span class="line">    self.unk_token = unk_token</span><br><span class="line">    self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenizes a piece of text into its word pieces.</span></span><br><span class="line"><span class="string">    This uses a greedy longest-match-first algorithm to perform tokenization</span></span><br><span class="line"><span class="string">    using the given vocabulary.</span></span><br><span class="line"><span class="string">    For example:</span></span><br><span class="line"><span class="string">      input = &quot;unaffable&quot;</span></span><br><span class="line"><span class="string">      output = [&quot;un&quot;, &quot;##aff&quot;, &quot;##able&quot;]</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      text: A single token or whitespace separated tokens. This should have</span></span><br><span class="line"><span class="string">        already been passed through `BasicTokenizer.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">f&#x27;[wordpiece_tokenizer.tokenize]传入 :<span class="subst">&#123;text&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line"></span><br><span class="line">    output_tokens = []</span><br><span class="line">    tf.logging.info(<span class="string">f&#x27;[whitespace_tokenize(text) :<span class="subst">&#123;whitespace_tokenize(text)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">      chars = <span class="built_in">list</span>(token)</span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">len</span>(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">      is_bad = <span class="literal">False</span></span><br><span class="line">      start = <span class="number">0</span></span><br><span class="line">      sub_tokens = []</span><br><span class="line">      <span class="keyword">while</span> start &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">        end = <span class="built_in">len</span>(chars)</span><br><span class="line">        cur_substr = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> start &lt; end:</span><br><span class="line">          substr = <span class="string">&quot;&quot;</span>.join(chars[start:end])</span><br><span class="line">          <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">            substr = <span class="string">&quot;##&quot;</span> + substr</span><br><span class="line">          <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab:</span><br><span class="line">            cur_substr = substr</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">          end -= <span class="number">1</span>  <span class="comment"># 从这里可以看出，优先匹配前缀</span></span><br><span class="line">        tf.logging.info(<span class="string">f&#x27;cur_substr :<span class="subst">&#123;cur_substr&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># 如果遇到子字符串为None，则查找失败</span></span><br><span class="line">          is_bad = <span class="literal">True</span></span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">        sub_tokens.append(cur_substr)</span><br><span class="line">        start = end  <span class="comment"># 去除前缀后继续查找</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> is_bad:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        output_tokens.extend(sub_tokens)</span><br><span class="line">    tf.logging.info(<span class="string">f&#x27;[wordpiece_tokenizer.tokenize]返回 :<span class="subst">&#123;output_tokens&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>

<p>比如，传入 ‘electronics’，</p>
<p>词表中有：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">....</span><br><span class="line">cap</span><br><span class="line">cel</span><br><span class="line">ele</span><br><span class="line">##ctronics</span><br><span class="line">elec</span><br><span class="line">##tronics</span><br><span class="line">cer</span><br><span class="line">cha</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>那么，结果返回为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">INFO - [wordpiece_tokenizer.tokenize]传入 :electronics</span><br><span class="line">2021-06-06 19:25:55,843 - tensorflow - INFO - cur_substr :elec</span><br><span class="line">2021-06-06 19:25:55,843 - tensorflow - INFO - cur_substr :##tronics</span><br><span class="line">2021-06-06 19:25:55,843 - tensorflow - INFO - [wordpiece_tokenizer.tokenize]返回 :[&#x27;elec&#x27;, &#x27;##tronics&#x27;]</span><br></pre></td></tr></table></figure>

<p>但是，如果词表又仅仅增加了 <code>electro</code></p>
<p>那么，结果就将为 [UNK]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO - [wordpiece_tokenizer.tokenize]传入 :electronics</span><br><span class="line">2021-06-06 19:35:10,232 - tensorflow - INFO - cur_substr :electro</span><br><span class="line">2021-06-06 19:35:10,232 - tensorflow - INFO - [wordpiece_tokenizer.tokenize]返回 :[&#x27;[UNK]&#x27;]</span><br></pre></td></tr></table></figure>



<h3 id="2-create-training-instances"><a href="#2-create-training-instances" class="headerlink" title="(2)create_training_instances()"></a>(2)create_training_instances()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_training_instances</span>(<span class="params">input_files, tokenizer, max_seq_length,</span></span></span><br><span class="line"><span class="params"><span class="function">                              dupe_factor, short_seq_prob, masked_lm_prob,</span></span></span><br><span class="line"><span class="params"><span class="function">                              max_predictions_per_seq, rng</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Create `TrainingInstance`s from raw text.&quot;&quot;&quot;</span></span><br><span class="line">  all_documents = [[]]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输入文件格式：</span></span><br><span class="line">  <span class="comment"># 每行一句话，空行表示document 分割，也即属于不同的文档</span></span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(input_file, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">      <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        line = tokenization.convert_to_unicode(reader.readline())</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line:  <span class="comment"># 表示读完</span></span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">        line = line.strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Empty lines are used as document delimiters</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line:  <span class="comment"># 空行，表示新的文档</span></span><br><span class="line">          all_documents.append([])</span><br><span class="line">        tokens = tokenizer.tokenize(line)   <span class="comment"># 参见（1）</span></span><br><span class="line">        <span class="keyword">if</span> tokens:</span><br><span class="line">          all_documents[-<span class="number">1</span>].append(tokens)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Remove empty documents</span></span><br><span class="line">  all_documents = [x <span class="keyword">for</span> x <span class="keyword">in</span> all_documents <span class="keyword">if</span> x]</span><br><span class="line">  rng.shuffle(all_documents)</span><br><span class="line"></span><br><span class="line">  vocab_words = <span class="built_in">list</span>(tokenizer.vocab.keys())</span><br><span class="line">  instances = []</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dupe_factor):</span><br><span class="line">    <span class="keyword">for</span> document_index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(all_documents)): <span class="comment"># 按照document_index 取值</span></span><br><span class="line">      instances.extend(</span><br><span class="line">          create_instances_from_document(				---------------<span class="number">2.1</span></span><br><span class="line">              all_documents, document_index, max_seq_length, short_seq_prob,</span><br><span class="line">              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))</span><br><span class="line"></span><br><span class="line">  rng.shuffle(instances)</span><br><span class="line">  <span class="keyword">return</span> instances</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>2.1 create_instances_from_document()</li>
</ul>
<p>生成训练集</p>
<p>依据 （1）产生的分词结果，进行遮盖替换，并标注正确标签</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_instances_from_document</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    all_documents, document_index, max_seq_length, short_seq_prob,</span></span></span><br><span class="line"><span class="params"><span class="function">    masked_lm_prob, max_predictions_per_seq, vocab_words, rng</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Creates `TrainingInstance`s for a single document.&quot;&quot;&quot;</span></span><br><span class="line">  document = all_documents[document_index]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Account for [CLS], [SEP], [SEP]</span></span><br><span class="line">  max_num_tokens = max_seq_length - <span class="number">3</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 在max_num_tokens基础上，缩短句子的概率</span></span><br><span class="line">  target_seq_length = max_num_tokens</span><br><span class="line">  <span class="keyword">if</span> rng.random() &lt; short_seq_prob:</span><br><span class="line">    target_seq_length = rng.randint(<span class="number">2</span>, max_num_tokens)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We DON&#x27;T just concatenate all of the tokens from a document into a long</span></span><br><span class="line">  <span class="comment"># sequence and choose an arbitrary split point because this would make the</span></span><br><span class="line">  <span class="comment"># next sentence prediction task too easy. Instead, we split the input into</span></span><br><span class="line">  <span class="comment"># segments &quot;A&quot; and &quot;B&quot; based on the actual &quot;sentences&quot; provided by the user</span></span><br><span class="line">  <span class="comment"># input.</span></span><br><span class="line">  instances = []</span><br><span class="line">  current_chunk = []</span><br><span class="line">  current_length = <span class="number">0</span></span><br><span class="line">  i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(document):   <span class="comment"># 从doucument 不断选择segment，受制于target_seq_length</span></span><br><span class="line">    segment = document[i]</span><br><span class="line">    current_chunk.append(segment)</span><br><span class="line">    current_length += <span class="built_in">len</span>(segment)</span><br><span class="line">    <span class="keyword">if</span> i == <span class="built_in">len</span>(document) - <span class="number">1</span> <span class="keyword">or</span> current_length &gt;= target_seq_length:</span><br><span class="line">      <span class="keyword">if</span> current_chunk:</span><br><span class="line">        <span class="comment"># `a_end` is how many segments from `current_chunk` go into the `A`</span></span><br><span class="line">        <span class="comment"># (first) sentence.</span></span><br><span class="line">        a_end = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(current_chunk) &gt;= <span class="number">2</span>:</span><br><span class="line">          a_end = rng.randint(<span class="number">1</span>, <span class="built_in">len</span>(current_chunk) - <span class="number">1</span>)  <span class="comment"># 决定选择多少个segment，对于多segment 的文档</span></span><br><span class="line"></span><br><span class="line">        tokens_a = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(a_end):</span><br><span class="line">          tokens_a.extend(current_chunk[j])</span><br><span class="line"></span><br><span class="line">        tokens_b = []</span><br><span class="line">        <span class="comment"># Random next</span></span><br><span class="line">        is_random_next = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(current_chunk) == <span class="number">1</span> <span class="keyword">or</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          is_random_next = <span class="literal">True</span></span><br><span class="line">          target_b_length = target_seq_length - <span class="built_in">len</span>(tokens_a)</span><br><span class="line"></span><br><span class="line">          <span class="comment"># This should rarely go for more than one iteration for large</span></span><br><span class="line">          <span class="comment"># corpora. However, just to be careful, we try to make sure that</span></span><br><span class="line">          <span class="comment"># the random document is not the same as the document</span></span><br><span class="line">          <span class="comment"># we&#x27;re processing.</span></span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            random_document_index = rng.randint(<span class="number">0</span>, <span class="built_in">len</span>(all_documents) - <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> random_document_index != document_index:</span><br><span class="line">              <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">          random_document = all_documents[random_document_index]</span><br><span class="line">          random_start = rng.randint(<span class="number">0</span>, <span class="built_in">len</span>(random_document) - <span class="number">1</span>)</span><br><span class="line">          <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(random_start, <span class="built_in">len</span>(random_document)):</span><br><span class="line">            tokens_b.extend(random_document[j])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(tokens_b) &gt;= target_b_length:</span><br><span class="line">              <span class="keyword">break</span></span><br><span class="line">          <span class="comment"># We didn&#x27;t actually use these segments so we &quot;put them back&quot; so</span></span><br><span class="line">          <span class="comment"># they don&#x27;t go to waste.</span></span><br><span class="line">          num_unused_segments = <span class="built_in">len</span>(current_chunk) - a_end</span><br><span class="line">          i -= num_unused_segments</span><br><span class="line">        <span class="comment"># Actual next</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          is_random_next = <span class="literal">False</span></span><br><span class="line">          <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(a_end, <span class="built_in">len</span>(current_chunk)):</span><br><span class="line">            tokens_b.extend(current_chunk[j])</span><br><span class="line">        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(tokens_a) &gt;= <span class="number">1</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(tokens_b) &gt;= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        tokens = []</span><br><span class="line">        segment_ids = []</span><br><span class="line">        tokens.append(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">        segment_ids.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens_a:</span><br><span class="line">          tokens.append(token)</span><br><span class="line">          segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        tokens.append(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line">        segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens_b:</span><br><span class="line">          tokens.append(token)</span><br><span class="line">          segment_ids.append(<span class="number">1</span>)</span><br><span class="line">        tokens.append(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line">        segment_ids.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        (tokens, masked_lm_positions,</span><br><span class="line">         masked_lm_labels) = create_masked_lm_predictions(</span><br><span class="line">             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)</span><br><span class="line">        instance = TrainingInstance(</span><br><span class="line">            tokens=tokens,</span><br><span class="line">            segment_ids=segment_ids,</span><br><span class="line">            is_random_next=is_random_next,</span><br><span class="line">            masked_lm_positions=masked_lm_positions,</span><br><span class="line">            masked_lm_labels=masked_lm_labels)</span><br><span class="line">        instances.append(instance)</span><br><span class="line">      current_chunk = []</span><br><span class="line">      current_length = <span class="number">0</span></span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> instances</span><br></pre></td></tr></table></figure>







<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis.html">BERT-预训练源码理解</a></li>
<li></li>
</ol>
</div></article></div><div class="right-container"><div class="widget"><div id="arAnchorBar"></div></div></div></section></div><div class="right-menu"></div><div class="modal search-modal"><div class="input-field"><input type="text" id="search_input"><label for="search-input">搜索</label></div><div id="search_result" class="search-result"></div></div><div class="blog-overlay"></div><footer class="row"><div class="footer-con"><div class="paginator"><a href="/2021/06/29/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/" title="⭐️机器学习中的数学知识" class="prev">PREV</a><a href="/2021/05/30/Jieba%E5%88%86%E8%AF%8D%E5%8E%9F%E7%90%86%E8%A7%A3%E8%AF%BB/" title="⭐️jieba分词原理解读" class="next">NEXT</a></div><div class="copyright"><p>© 2016 - 2021 <a target="_blank">John Doe</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <br> and <a href="https://github.com/Bulandent/hexo-theme-bubuzou" target="_blank">hexo-theme-bubuzou</a></p><p> <span style="padding-right: 6px;">闽ICP备16007301号-2</span></p></div><div class="totop"><i></i></div></div></footer><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/jquery-1.8.2.min.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/articleCatalog.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/main.js"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>