<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="1EB8XoOl0C"><meta name="google-site-verification" content="K7thEgdLm0UfRWJ5MGdF7sCcjClSzAlxFLPv2Oz5CGM"><title> ⭐️数据结构与算法之美（高级篇 & 实战篇） · Hexo</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="⭐️数据结构与算法之美（高级篇 &amp; 实战篇） - John Doe"><meta name="keywords"><meta name="author" content="John Doe"><link rel="short icon" href="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/favicon.ico"><link rel="stylesheet" href="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/bubuzou.css"><link rel="search" type="application/opensearchdescription+xml" href="http://example.com/atom.xml" title="Hexo"><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body><header><div class="header row"> <a href="/" class="logo-link"><img src="/iamges/logo.png"></a><ul id="nav_list" class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" data-hover="博文" class="nav-list-link">博文</a></li><li class="nav-list-item"><a href="/archives/" target="_self" data-hover="归档" class="nav-list-link">归档</a></li><li class="nav-list-item"><a href="/about/" target="_self" data-hover="关于" class="nav-list-link">关于</a></li></ul><div class="search"><a id="search_btn" href="#search"></a></div><div id="nav_btn" class="nav-btn"><span></span><span></span><span></span></div></div></header><div class="row scroll-con"><section class="container"><!-- for archive page--><div id="postAr" class="post"><article class="post-block"><h1 class="post-title">⭐️数据结构与算法之美（高级篇 & 实战篇）</h1><div class="post-info">2021-04-02</div><div class="post-content"><p>[TOC]</p>
<span id="more"></span>

<h1 id=""><a href="#" class="headerlink" title="======================================="></a>=======================================</h1><h1 id="高级篇-43-51（9讲）"><a href="#高级篇-43-51（9讲）" class="headerlink" title="高级篇 43-51（9讲）"></a>高级篇 43-51（9讲）</h1><h1 id="43-拓扑排序：如何确定代码源文件的编译依赖关系？"><a href="#43-拓扑排序：如何确定代码源文件的编译依赖关系？" class="headerlink" title="43 | 拓扑排序：如何确定代码源文件的编译依赖关系？"></a>43 | 拓扑排序：如何确定代码源文件的编译依赖关系？</h1><p>王争 2019-01-04</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83dzmq8wj30vq0hsjsw.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>07:12</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：8.80M 时长：09:36</p>
<p>从今天开始，我们就进入了专栏的高级篇。相对基础篇，高级篇涉及的知识点，都比较零散，不是太系统。所以，我会围绕一个实际软件开发的问题，在阐述具体解决方法的过程中，将涉及的知识点给你详细讲解出来。</p>
<p>所以，相较于基础篇“<strong>开篇问题 - 知识讲解 - 回答开篇 - 总结 - 课后思考</strong>”这样的文章结构，高级篇我稍作了些改变，大致分为这样几个部分：“<strong>问题阐述 - 算法解析 - 总结引申 - 课后思考</strong>”。</p>
<p>好了，现在，我们就进入高级篇的第一节，如何确定代码源文件的编译依赖关系？</p>
<p>我们知道，一个完整的项目往往会包含很多代码源文件。编译器在编译整个项目的时候，需要按照依赖关系，依次编译每个源文件。比如，A.cpp 依赖 B.cpp，那在编译的时候，编译器需要先编译 B.cpp，才能编译 A.cpp。</p>
<p>编译器通过分析源文件或者程序员事先写好的编译配置文件（比如 Makefile 文件），来获取这种局部的依赖关系。<strong>那编译器又该如何通过源文件两两之间的局部依赖关系，确定一个全局的编译顺序呢？</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83e0g84kj30vq0bhgmx.jpg" alt="img"></p>
<h2 id="算法解析"><a href="#算法解析" class="headerlink" title="算法解析"></a>算法解析</h2><p>这个问题的解决思路与“图”这种数据结构的一个经典算法“拓扑排序算法”有关。那什么是拓扑排序呢？这个概念很好理解，我们先来看一个生活中的拓扑排序的例子。</p>
<p>我们在穿衣服的时候都有一定的顺序，我们可以把这种顺序想成，衣服与衣服之间有一定的依赖关系。比如说，你必须先穿袜子才能穿鞋，先穿内裤才能穿秋裤。假设我们现在有八件衣服要穿，它们之间的两两依赖关系我们已经很清楚了，那如何安排一个穿衣序列，能够满足所有的两两之间的依赖关系？</p>
<p>这就是个拓扑排序问题。从这个例子中，你应该能想到，在很多时候，拓扑排序的序列并不是唯一的。你可以看我画的这幅图，我找到了好几种满足这些局部先后关系的穿衣序列。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83e0rrjaj30vq0hu0v3.jpg" alt="img"></p>
<p>弄懂了这个生活中的例子，开篇的关于编译顺序的问题，你应该也有思路了。开篇问题跟这个问题的模型是一样的，也可以抽象成一个拓扑排序问题。</p>
<p>拓扑排序的原理非常简单，我们的重点应该放到拓扑排序的实现上面。</p>
<p>我前面多次讲过，算法是构建在具体的数据结构之上的。针对这个问题，我们先来看下，如何将问题背景抽象成具体的数据结构？</p>
<p>我们可以把源文件与源文件之间的依赖关系，抽象成一个有向图。每个源文件对应图中的一个顶点，源文件之间的依赖关系就是顶点之间的边。</p>
<p>如果 a 先于 b 执行，也就是说 b 依赖于 a，那么就在顶点 a 和顶点 b 之间，构建一条从 a 指向 b 的边。而且，这个图不仅要是有向图，还要是一个有向无环图，也就是不能存在像 a-&gt;b-&gt;c-&gt;a 这样的循环依赖关系。因为图中一旦出现环，拓扑排序就无法工作了。实际上，拓扑排序本身就是基于有向无环图的一个算法。</p>
<p>public class Graph {</p>
<p>  private int v; // 顶点的个数</p>
<p>  private LinkedList<Integer> adj[]; // 邻接表</p>
<p>  public Graph(int v) {</p>
<p>​    this.v = v;</p>
<p>​    adj = new LinkedList[v];</p>
<p>​    for (int i=0; i&lt;v; ++i) {</p>
<p>​      adj[i] = new LinkedList&lt;&gt;();</p>
<p>​    }</p>
<p>  }</p>
<p>  public void addEdge(int s, int t) { // s先于t，边s-&gt;t</p>
<p>​    adj[s].add(t);</p>
<p>  }</p>
<p>}</p>
<p>数据结构定义好了，现在，我们来看，<strong>如何在这个有向无环图上，实现拓扑排序</strong>？</p>
<p>拓扑排序有两种实现方法，都不难理解。它们分别是 <strong>Kahn 算法</strong>和 <strong>DFS 深度优先搜索算法</strong>。我们依次来看下它们都是怎么工作的。</p>
<h3 id="1-Kahn-算法"><a href="#1-Kahn-算法" class="headerlink" title="1.Kahn 算法"></a>1.Kahn 算法</h3><p>Kahn 算法实际上用的是贪心算法思想，思路非常简单、好懂。</p>
<p>定义数据结构的时候，如果 s 需要先于 t 执行，那就添加一条 s 指向 t 的边。所以，如果某个顶点入度为 0， 也就表示，没有任何顶点必须先于这个顶点执行，那么这个顶点就可以执行了。</p>
<p>我们先从图中，找出一个入度为 0 的顶点，将其输出到拓扑排序的结果序列中（对应代码中就是把它打印出来），并且把这个顶点从图中删除（也就是把这个顶点可达的顶点的入度都减 1）。我们循环执行上面的过程，直到所有的顶点都被输出。最后输出的序列，就是满足局部依赖关系的拓扑排序。</p>
<p>我把 Kahn 算法用代码实现了一下，你可以结合着文字描述一块看下。不过，你应该能发现，这段代码实现更有技巧一些，并没有真正删除顶点的操作。代码中有详细的注释，你自己来看，我就不多解释了。</p>
<p>public void topoSortByKahn() {</p>
<p>  int[] inDegree = new int[v]; // 统计每个顶点的入度</p>
<p>  for (int i = 0; i &lt; v; ++i) {</p>
<p>​    for (int j = 0; j &lt; adj[i].size(); ++j) {</p>
<p>​      int w = adj[i].get(j); // i-&gt;w</p>
<p>​      inDegree[w]++;</p>
<p>​    }</p>
<p>  }</p>
<p>  LinkedList<Integer> queue = new LinkedList&lt;&gt;();</p>
<p>  for (int i = 0; i &lt; v; ++i) {</p>
<p>​    if (inDegree[i] == 0) queue.add(i);</p>
<p>  }</p>
<p>  while (!queue.isEmpty()) {</p>
<p>​    int i = queue.remove();</p>
<p>​    System.out.print(“-&gt;” + i);</p>
<p>​    for (int j = 0; j &lt; adj[i].size(); ++j) {</p>
<p>​      int k = adj[i].get(j);</p>
<p>​      inDegree[k]–;</p>
<p>​      if (inDegree[k] == 0) queue.add(k);</p>
<p>​    }</p>
<p>  }</p>
<p>}</p>
<h3 id="2-DFS-算法"><a href="#2-DFS-算法" class="headerlink" title="2.DFS 算法"></a>2.DFS 算法</h3><p>图上的深度优先搜索我们前面已经讲过了，实际上，拓扑排序也可以用深度优先搜索来实现。不过这里的名字要稍微改下，更加确切的说法应该是深度优先遍历，遍历图中的所有顶点，而非只是搜索一个顶点到另一个顶点的路径。</p>
<p>关于这个算法的实现原理，我先把代码贴在下面，下面给你具体解释。</p>
<p>public void topoSortByDFS() {</p>
<p>  // 先构建逆邻接表，边s-&gt;t表示，s依赖于t，t先于s</p>
<p>  LinkedList<Integer> inverseAdj[] = new LinkedList[v];</p>
<p>  for (int i = 0; i &lt; v; ++i) { // 申请空间</p>
<p>​    inverseAdj[i] = new LinkedList&lt;&gt;();</p>
<p>  }</p>
<p>  for (int i = 0; i &lt; v; ++i) { // 通过邻接表生成逆邻接表</p>
<p>​    for (int j = 0; j &lt; adj[i].size(); ++j) {</p>
<p>​      int w = adj[i].get(j); // i-&gt;w</p>
<p>​      inverseAdj[w].add(i); // w-&gt;i</p>
<p>​    }</p>
<p>  }</p>
<p>  boolean[] visited = new boolean[v];</p>
<p>  for (int i = 0; i &lt; v; ++i) { // 深度优先遍历图</p>
<p>​    if (visited[i] == false) {</p>
<p>​      visited[i] = true;</p>
<p>​      dfs(i, inverseAdj, visited);</p>
<p>​    }</p>
<p>  }</p>
<p>}</p>
<p>private void dfs(</p>
<p>​    int vertex, LinkedList<Integer> inverseAdj[], boolean[] visited) {</p>
<p>  for (int i = 0; i &lt; inverseAdj[vertex].size(); ++i) {</p>
<p>​    int w = inverseAdj[vertex].get(i);</p>
<p>​    if (visited[w] == true) continue;</p>
<p>​    visited[w] = true;</p>
<p>​    dfs(w, inverseAdj, visited);</p>
<p>  } // 先把vertex这个顶点可达的所有顶点都打印出来之后，再打印它自己</p>
<p>  System.out.print(“-&gt;” + vertex);</p>
<p>}</p>
<p>这个算法包含两个关键部分。</p>
<p>第一部分是<strong>通过邻接表构造逆邻接表</strong>。邻接表中，边 s-&gt;t 表示 s 先于 t 执行，也就是 t 要依赖 s。在逆邻接表中，边 s-&gt;t 表示 s 依赖于 t，s 后于 t 执行。为什么这么转化呢？这个跟我们这个算法的实现思想有关。</p>
<p>第二部分是这个算法的核心，也就是<strong>递归处理每个顶点</strong>。对于顶点 vertex 来说，我们先输出它可达的所有顶点，也就是说，先把它依赖的所有的顶点输出了，然后再输出自己。</p>
<p>到这里，用 Kahn 算法和 DFS 算法求拓扑排序的原理和代码实现都讲完了。我们来看下，<strong>这两个算法的时间复杂度分别是多少呢？</strong></p>
<p>从 Kahn 代码中可以看出来，每个顶点被访问了一次，每个边也都被访问了一次，所以，Kahn 算法的时间复杂度就是 O(V+E)（V 表示顶点个数，E 表示边的个数）。</p>
<p>DFS 算法的时间复杂度我们之前分析过。每个顶点被访问两次，每条边都被访问一次，所以时间复杂度也是 O(V+E)。</p>
<p>注意，这里的图可能不是连通的，有可能是有好几个不连通的子图构成，所以，E 并不一定大于 V，两者的大小关系不确定。所以，在表示时间复杂度的时候，V、E 都要考虑在内。</p>
<h2 id="总结引申"><a href="#总结引申" class="headerlink" title="总结引申"></a>总结引申</h2><p>在基础篇中，关于“图”，我们讲了图的定义和存储、图的广度和深度优先搜索。今天，我们又讲了一个关于图的算法，拓扑排序。</p>
<p>拓扑排序应用非常广泛，解决的问题的模型也非常一致。凡是需要通过局部顺序来推导全局顺序的，一般都能用拓扑排序来解决。除此之外，拓扑排序还能检测图中环的存在。对于 Kahn 算法来说，如果最后输出出来的顶点个数，少于图中顶点个数，图中还有入度不是 0 的顶点，那就说明，图中存在环。</p>
<p>关于图中环的检测，我们在递归那一节讲过一个例子，在查找最终推荐人的时候，可能会因为脏数据，造成存在循环推荐，比如，用户 A 推荐了用户 B，用户 B 推荐了用户 C，用户 C 又推荐了用户 A。如何避免这种脏数据导致的无限递归？这个问题，我当时留给你思考了，现在是时候解答了。</p>
<p>实际上，这就是环的检测问题。因为我们每次都只是查找一个用户的最终推荐人，所以，我们并不需要动用复杂的拓扑排序算法，而只需要记录已经访问过的用户 ID，当用户 ID 第二次被访问的时候，就说明存在环，也就说明存在脏数据。</p>
<p>HashSet<Integer> hashTable = new HashSet&lt;&gt;(); // 保存已经访问过的actorId</p>
<p>long findRootReferrerId(long actorId) {</p>
<p>  if (hashTable.contains(actorId)) { // 存在环</p>
<p>​    return;</p>
<p>  }</p>
<p>  hashTable.add(actorId);</p>
<p>  Long referrerId = </p>
<p>​       select referrer_id from [table] where actor_id = actorId;</p>
<p>  if (referrerId == null) return actorId;</p>
<p>  return findRootReferrerId(referrerId);</p>
<p>}</p>
<p>如果把这个问题改一下，我们想要知道，数据库中的所有用户之间的推荐关系了，有没有存在环的情况。这个问题，就需要用到拓扑排序算法了。我们把用户之间的推荐关系，从数据库中加载到内存中，然后构建成今天讲的这种有向图数据结构，再利用拓扑排序，就可以快速检测出是否存在环了。</p>
<h2 id="课后思考"><a href="#课后思考" class="headerlink" title="课后思考"></a>课后思考</h2><p>在今天的讲解中，我们用图表示依赖关系的时候，如果 a 先于 b 执行，我们就画一条从 a 到 b 的有向边；反过来，如果 a 先于 b，我们画一条从 b 到 a 的有向边，表示 b 依赖 a，那今天讲的 Kahn 算法和 DFS 算法还能否正确工作呢？如果不能，应该如何改造一下呢？</p>
<p>我们今天讲了两种拓扑排序算法的实现思路，Kahn 算法和 DFS 深度优先搜索算法，如果换做 BFS 广度优先搜索算法，还可以实现吗？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="44-最短路径：地图软件是如何计算出最优出行路径的？"><a href="#44-最短路径：地图软件是如何计算出最优出行路径的？" class="headerlink" title="44 | 最短路径：地图软件是如何计算出最优出行路径的？"></a>44 | 最短路径：地图软件是如何计算出最优出行路径的？</h1><p>王争 2019-01-07</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83f8bkd1j30vq0hsq4u.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:13</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：13.30M 时长：14:31</p>
<p>基础篇的时候，我们学习了图的两种搜索算法，深度优先搜索和广度优先搜索。这两种算法主要是针对无权图的搜索算法。针对有权图，也就是图中的每条边都有一个权重，我们该如何计算两点之间的最短路径（经过的边的权重和最小）呢？今天，我就从地图软件的路线规划问题讲起，带你看看常用的<strong>最短路径算法</strong>（Shortest Path Algorithm）。</p>
<p>像 Google 地图、百度地图、高德地图这样的地图软件，我想你应该经常使用吧？如果想从家开车到公司，你只需要输入起始、结束地址，地图就会给你规划一条最优出行路线。这里的最优，有很多种定义，比如最短路线、最少用时路线、最少红绿灯路线等等。<strong>作为一名软件开发工程师，你是否思考过，地图软件的最优路线是如何计算出来的吗？底层依赖了什么算法呢？</strong></p>
<h2 id="算法解析-1"><a href="#算法解析-1" class="headerlink" title="算法解析"></a>算法解析</h2><p>我们刚提到的最优问题包含三个：最短路线、最少用时和最少红绿灯。我们先解决最简单的，最短路线。</p>
<p>解决软件开发中的实际问题，最重要的一点就是<strong>建模</strong>，也就是将复杂的场景抽象成具体的数据结构。针对这个问题，我们该如何抽象成数据结构呢？</p>
<p>我们之前也提到过，图这种数据结构的表达能力很强，显然，把地图抽象成图最合适不过了。我们把每个岔路口看作一个顶点，岔路口与岔路口之间的路看作一条边，路的长度就是边的权重。如果路是单行道，我们就在两个顶点之间画一条有向边；如果路是双行道，我们就在两个顶点之间画两条方向不同的边。这样，整个地图就被抽象成一个有向有权图。</p>
<p>具体的代码实现，我放在下面了。于是，我们要求解的问题就转化为，在一个有向有权图中，求两个顶点间的最短路径。</p>
<p>public class Graph { // 有向有权图的邻接表表示</p>
<p>  private LinkedList<Edge> adj[]; // 邻接表</p>
<p>  private int v; // 顶点个数</p>
<p>  public Graph(int v) {</p>
<p>​    this.v = v;</p>
<p>​    this.adj = new LinkedList[v];</p>
<p>​    for (int i = 0; i &lt; v; ++i) {</p>
<p>​      this.adj[i] = new LinkedList&lt;&gt;();</p>
<p>​    }</p>
<p>  }</p>
<p>  public void addEdge(int s, int t, int w) { // 添加一条边</p>
<p>​    this.adj[s].add(new Edge(s, t, w));</p>
<p>  }</p>
<p>  private class Edge {</p>
<p>​    public int sid; // 边的起始顶点编号</p>
<p>​    public int tid; // 边的终止顶点编号</p>
<p>​    public int w; // 权重</p>
<p>​    public Edge(int sid, int tid, int w) {</p>
<p>​      this.sid = sid;</p>
<p>​      this.tid = tid;</p>
<p>​      this.w = w;</p>
<p>​    }</p>
<p>  }</p>
<p>  // 下面这个类是为了dijkstra实现用的</p>
<p>  private class Vertex {</p>
<p>​    public int id; // 顶点编号ID</p>
<p>​    public int dist; // 从起始顶点到这个顶点的距离</p>
<p>​    public Vertex(int id, int dist) {</p>
<p>​      this.id = id;</p>
<p>​      this.dist = dist;</p>
<p>​    }</p>
<p>  }</p>
<p>}</p>
<p>想要解决这个问题，有一个非常经典的算法，最短路径算法，更加准确地说，是<strong>单源最短路径算法</strong>（一个顶点到一个顶点）。提到最短路径算法，最出名的莫过于 Dijkstra 算法了。所以，我们现在来看，Dijkstra 算法是怎么工作的。</p>
<p>这个算法的原理稍微有点儿复杂，单纯的文字描述，不是很好懂。所以，我还是结合代码来讲解。</p>
<p>// 因为Java提供的优先级队列，没有暴露更新数据的接口，所以我们需要重新实现一个</p>
<p>private class PriorityQueue { // 根据vertex.dist构建小顶堆</p>
<p>  private Vertex[] nodes;</p>
<p>  private int count;</p>
<p>  public PriorityQueue(int v) {</p>
<p>​    this.nodes = new Vertex[v+1];</p>
<p>​    this.count = v;</p>
<p>  }</p>
<p>  public Vertex poll() { // TODO: 留给读者实现… }</p>
<p>  public void add(Vertex vertex) { // TODO: 留给读者实现…}</p>
<p>  // 更新结点的值，并且从下往上堆化，重新符合堆的定义。时间复杂度O(logn)。</p>
<p>  public void update(Vertex vertex) { // TODO: 留给读者实现…} </p>
<p>  public boolean isEmpty() { // TODO: 留给读者实现…}</p>
<p>}</p>
<p>public void dijkstra(int s, int t) { // 从顶点s到顶点t的最短路径</p>
<p>  int[] predecessor = new int[this.v]; // 用来还原最短路径</p>
<p>  Vertex[] vertexes = new Vertex[this.v];</p>
<p>  for (int i = 0; i &lt; this.v; ++i) {</p>
<p>​    vertexes[i] = new Vertex(i, Integer.MAX_VALUE);</p>
<p>  }</p>
<p>  PriorityQueue queue = new PriorityQueue(this.v);// 小顶堆</p>
<p>  boolean[] inqueue = new boolean[this.v]; // 标记是否进入过队列</p>
<p>  vertexes[s].dist = 0;</p>
<p>  queue.add(vertexes[s]);</p>
<p>  inqueue[s] = true;</p>
<p>  while (!queue.isEmpty()) {</p>
<p>​    Vertex minVertex= queue.poll(); // 取堆顶元素并删除</p>
<p>​    if (minVertex.id == t) break; // 最短路径产生了</p>
<p>​    for (int i = 0; i &lt; adj[minVertex.id].size(); ++i) {</p>
<p>​      Edge e = adj[minVertex.id].get(i); // 取出一条minVetex相连的边</p>
<p>​      Vertex nextVertex = vertexes[e.tid]; // minVertex–&gt;nextVertex</p>
<p>​      if (minVertex.dist + e.w &lt; nextVertex.dist) { // 更新next的dist</p>
<p>​        nextVertex.dist = minVertex.dist + e.w;</p>
<p>​        predecessor[nextVertex.id] = minVertex.id;</p>
<p>​        if (inqueue[nextVertex.id] == true) {</p>
<p>​          queue.update(nextVertex); // 更新队列中的dist值</p>
<p>​        } else {</p>
<p>​          queue.add(nextVertex);</p>
<p>​          inqueue[nextVertex.id] = true;</p>
<p>​        }</p>
<p>​      }</p>
<p>​    }</p>
<p>  }</p>
<p>  // 输出最短路径</p>
<p>  System.out.print(s);</p>
<p>  print(s, t, predecessor);</p>
<p>}</p>
<p>private void print(int s, int t, int[] predecessor) {</p>
<p>  if (s == t) return;</p>
<p>  print(s, predecessor[t], predecessor);</p>
<p>  System.out.print(“-&gt;” + t);</p>
<p>}</p>
<p>我们用 vertexes 数组，记录从起始顶点到每个顶点的距离（dist）。起初，我们把所有顶点的 dist 都初始化为无穷大（也就是代码中的 Integer.MAX_VALUE）。我们把起始顶点的 dist 值初始化为 0，然后将其放到优先级队列中。</p>
<p>我们从优先级队列中取出 dist 最小的顶点 minVertex，然后考察这个顶点可达的所有顶点（代码中的 nextVertex）。如果 minVertex 的 dist 值加上 minVertex 与 nextVertex 之间边的权重 w 小于 nextVertex 当前的 dist 值，也就是说，存在另一条更短的路径，它经过 minVertex 到达 nextVertex。那我们就把 nextVertex 的 dist 更新为 minVertex 的 dist 值加上 w。然后，我们把 nextVertex 加入到优先级队列中。重复这个过程，直到找到终止顶点 t 或者队列为空。</p>
<p>以上就是 Dijkstra 算法的核心逻辑。除此之外，代码中还有两个额外的变量，predecessor 数组和 inqueue 数组。</p>
<p>predecessor 数组的作用是为了还原最短路径，它记录每个顶点的前驱顶点。最后，我们通过递归的方式，将这个路径打印出来。打印路径的 print 递归代码我就不详细讲了，这个跟我们在图的搜索中讲的打印路径方法一样。如果不理解的话，你可以回过头去看下那一节。</p>
<p>inqueue 数组是为了避免将一个顶点多次添加到优先级队列中。我们更新了某个顶点的 dist 值之后，如果这个顶点已经在优先级队列中了，就不要再将它重复添加进去了。</p>
<p>看完了代码和文字解释，你可能还是有点懵，那我就举个例子，再给你解释一下。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83f5yxb4j30vq0nsq89.jpg" alt="img"></p>
<p>理解了 Dijkstra 的原理和代码实现，我们来看下，<strong>Dijkstra 算法的时间复杂度是多少？</strong></p>
<p>在刚刚的代码实现中，最复杂就是 while 循环嵌套 for 循环那部分代码了。while 循环最多会执行 V 次（V 表示顶点的个数），而内部的 for 循环的执行次数不确定，跟每个顶点的相邻边的个数有关，我们分别记作 E0，E1，E2，……，E(V-1)。如果我们把这 V 个顶点的边都加起来，最大也不会超过图中所有边的个数 E（E 表示边的个数）。</p>
<p>for 循环内部的代码涉及从优先级队列取数据、往优先级队列中添加数据、更新优先级队列中的数据，这样三个主要的操作。我们知道，优先级队列是用堆来实现的，堆中的这几个操作，时间复杂度都是 O(logV)（堆中的元素个数不会超过顶点的个数 V）。</p>
<p>所以，综合这两部分，再利用乘法原则，整个代码的时间复杂度就是 O(E*logV)。</p>
<p>弄懂了 Dijkstra 算法，我们再来回答之前的问题，如何计算最优出行路线？</p>
<p>从理论上讲，用 Dijkstra 算法可以计算出两点之间的最短路径。但是，你有没有想过，对于一个超级大地图来说，岔路口、道路都非常多，对应到图这种数据结构上来说，就有非常多的顶点和边。如果为了计算两点之间的最短路径，在一个超级大图上动用 Dijkstra 算法，遍历所有的顶点和边，显然会非常耗时。那我们有没有什么优化的方法呢？</p>
<p>做工程不像做理论，一定要给出个最优解。理论上算法再好，如果执行效率太低，也无法应用到实际的工程中。<strong>对于软件开发工程师来说，我们经常要根据问题的实际背景，对解决方案权衡取舍。类似出行路线这种工程上的问题，我们没有必要非得求出个绝对最优解。很多时候，为了兼顾执行效率，我们只需要计算出一个可行的次优解就可以了</strong>。</p>
<p>有了这个原则，你能想出刚刚那个问题的优化方案吗？</p>
<p>虽然地图很大，但是两点之间的最短路径或者说较好的出行路径，并不会很“发散”，只会出现在两点之间和两点附近的区块内。所以我们可以在整个大地图上，划出一个小的区块，这个小区块恰好可以覆盖住两个点，但又不会很大。我们只需要在这个小区块内部运行 Dijkstra 算法，这样就可以避免遍历整个大图，也就大大提高了执行效率。</p>
<p>不过你可能会说了，如果两点距离比较远，从北京海淀区某个地点，到上海黄浦区某个地点，那上面的这种处理方法，显然就不工作了，毕竟覆盖北京和上海的区块并不小。</p>
<p>我给你点提示，你可以现在打开地图 App，缩小放大一下地图，看下地图上的路线有什么变化，然后再思考，这个问题该怎么解决。</p>
<p>对于这样两点之间距离较远的路线规划，我们可以把北京海淀区或者北京看作一个顶点，把上海黄浦区或者上海看作一个顶点，先规划大的出行路线。比如，如何从北京到上海，必须要经过某几个顶点，或者某几条干道，然后再细化每个阶段的小路线。</p>
<p>这样，最短路径问题就解决了。我们再来看另外两个问题，最少时间和最少红绿灯。</p>
<p>前面讲最短路径的时候，每条边的权重是路的长度。在计算最少时间的时候，算法还是不变，我们只需要把边的权重，从路的长度变成经过这段路所需要的时间。不过，这个时间会根据拥堵情况时刻变化。如何计算车通过一段路的时间呢？这是一个蛮有意思的问题，你可以自己思考下。</p>
<p>每经过一条边，就要经过一个红绿灯。关于最少红绿灯的出行方案，实际上，我们只需要把每条边的权值改为 1 即可，算法还是不变，可以继续使用前面讲的 Dijkstra 算法。不过，边的权值为 1，也就相当于无权图了，我们还可以使用之前讲过的广度优先搜索算法。因为我们前面讲过，广度优先搜索算法计算出来的两点之间的路径，就是两点的最短路径。</p>
<p>不过，这里给出的所有方案都非常粗糙，只是为了给你展示，如何结合实际的场景，灵活地应用算法，让算法为我们所用，真实的地图软件的路径规划，要比这个复杂很多。而且，比起 Dijkstra 算法，地图软件用的更多的是类似 A* 的启发式搜索算法，不过也是在 Dijkstra 算法上的优化罢了，我们后面会讲到，这里暂且不展开。</p>
<h2 id="总结引申-1"><a href="#总结引申-1" class="headerlink" title="总结引申"></a>总结引申</h2><p>今天，我们学习了一种非常重要的图算法，<strong>Dijkstra 最短路径算法</strong>。实际上，最短路径算法还有很多，比如 Bellford 算法、Floyd 算法等等。如果感兴趣，你可以自己去研究。</p>
<p>关于 Dijkstra 算法，我只讲了原理和代码实现。对于正确性，我没有去证明。之所以这么做，是因为证明过程会涉及比较复杂的数学推导。这个并不是我们的重点，你只要掌握这个算法的思路就可以了。</p>
<p>这些算法实现思路非常经典，掌握了这些思路，我们可以拿来指导、解决其他问题。比如 Dijkstra 这个算法的核心思想，就可以拿来解决下面这个看似完全不相关的问题。这个问题是我之前工作中遇到的真实的问题，为了在较短的篇幅里把问题介绍清楚，我对背景做了一些简化。</p>
<p>我们有一个翻译系统，只能针对单个词来做翻译。如果要翻译一整个句子，我们需要将句子拆成一个一个的单词，再丢给翻译系统。针对每个单词，翻译系统会返回一组可选的翻译列表，并且针对每个翻译打一个分，表示这个翻译的可信程度。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83f7q0pnj30vq0f40uf.jpg" alt="img"></p>
<p>针对每个单词，我们从可选列表中，选择其中一个翻译，组合起来就是整个句子的翻译。每个单词的翻译的得分之和，就是整个句子的翻译得分。随意搭配单词的翻译，会得到一个句子的不同翻译。针对整个句子，我们希望计算出得分最高的前 k 个翻译结果，你会怎么编程来实现呢？</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83f70dihj30vq0dg75b.jpg" alt="img"></p>
<p>当然，最简单的办法还是借助回溯算法，穷举所有的排列组合情况，然后选出得分最高的前 k 个翻译结果。但是，这样做的时间复杂度会比较高，是 O(m^n)，其中，m 表示平均每个单词的可选翻译个数，n 表示一个句子中包含多少个单词。这个解决方案，你可以当作回溯算法的练习题，自己编程实现一下，我就不多说了。</p>
<p>实际上，这个问题可以借助 Dijkstra 算法的核心思想，非常高效地解决。每个单词的可选翻译是按照分数从大到小排列的，所以 <em>a</em>0<em>b</em>0<em>c</em>0 肯定是得分最高组合结果。我们把 <em>a</em>0<em>b</em>0<em>c</em>0 及得分作为一个对象，放入到优先级队列中。</p>
<p>我们每次从优先级队列中取出一个得分最高的组合，并基于这个组合进行扩展。扩展的策略是每个单词的翻译分别替换成下一个单词的翻译。比如 <em>a</em>0<em>b</em>0<em>c</em>0 扩展后，会得到三个组合，<em>a</em>1<em>b</em>0<em>c</em>0、<em>a</em>0<em>b</em>1<em>c</em>0、<em>a</em>0<em>b</em>0<em>c</em>1。我们把扩展之后的组合，加到优先级队列中。重复这个过程，直到获取到 k 个翻译组合或者队列为空。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83f6hmk1j30vq0m5jus.jpg" alt="img"></p>
<p>我们来看，这种实现思路的时间复杂度是多少？</p>
<p>假设句子包含 n 个单词，每个单词平均有 m 个可选的翻译，我们求得分最高的前 k 个组合结果。每次一个组合出队列，就对应着一个组合结果，我们希望得到 k 个，那就对应着 k 次出队操作。每次有一个组合出队列，就有 n 个组合入队列。优先级队列中出队和入队操作的时间复杂度都是 O(logX)，X 表示队列中的组合个数。所以，总的时间复杂度就是 O(k<em>n</em>logX)。那 X 到底是多少呢？</p>
<p>k 次出入队列，队列中的总数据不会超过 k<em>n，也就是说，出队、入队操作的时间复杂度是 O(log(k</em>n))。所以，总的时间复杂度就是 O(k<em>n</em>log(k*n))，比之前的指数级时间复杂度降低了很多。</p>
<h2 id="课后思考-1"><a href="#课后思考-1" class="headerlink" title="课后思考"></a>课后思考</h2><p>在计算最短时间的出行路线中，如何获得通过某条路的时间呢？这个题目很有意思，我之前面试的时候也被问到过，你可以思考看看。</p>
<p>今天讲的出行路线问题，我假设的是开车出行，那如果是公交出行呢？如果混合地铁、公交、步行，又该如何规划路线呢？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="45-位图：如何实现网页爬虫中的URL去重功能？"><a href="#45-位图：如何实现网页爬虫中的URL去重功能？" class="headerlink" title="45 | 位图：如何实现网页爬虫中的URL去重功能？"></a>45 | 位图：如何实现网页爬虫中的URL去重功能？</h1><p>王争 2019-01-09</p>
<p><img src="https://static001.geekbang.org/resource/image/01/38/01e51226d435eda4604acc019731c538.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:09</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：13.84M 时长：15:06</p>
<p>网页爬虫是搜索引擎中的非常重要的系统，负责爬取几十亿、上百亿的网页。爬虫的工作原理是，通过解析已经爬取页面中的网页链接，然后再爬取这些链接对应的网页。而<strong>同一个网页链接有可能被包含在多个页面中，这就会导致爬虫在爬取的过程中，重复爬取相同的网页。如果你是一名负责爬虫的工程师，你会如何避免这些重复的爬取呢？</strong></p>
<p>最容易想到的方法就是，我们记录已经爬取的网页链接（也就是 URL），在爬取一个新的网页之前，我们拿它的链接，在已经爬取的网页链接列表中搜索。如果存在，那就说明这个网页已经被爬取过了；如果不存在，那就说明这个网页还没有被爬取过，可以继续去爬取。等爬取到这个网页之后，我们将这个网页的链接添加到已经爬取的网页链接列表了。</p>
<p>思路非常简单，我想你应该很容易就能想到。不过，我们该如何记录已经爬取的网页链接呢？需要用什么样的数据结构呢？</p>
<h2 id="算法解析-2"><a href="#算法解析-2" class="headerlink" title="算法解析"></a>算法解析</h2><p>关于这个问题，我们可以先回想下，是否可以用我们之前学过的数据结构来解决呢？</p>
<p>这个问题要处理的对象是网页链接，也就是 URL，需要支持的操作有两个，添加一个 URL 和查询一个 URL。除了这两个功能性的要求之外，在非功能性方面，我们还要求这两个操作的执行效率要尽可能高。除此之外，因为我们处理的是上亿的网页链接，内存消耗会非常大，所以在存储效率上，我们要尽可能地高效。</p>
<p>我们回想一下，满足这些条件的数据结构有哪些呢？显然，散列表、红黑树、跳表这些动态数据结构，都能支持快速地插入、查找数据，但是在内存消耗方面，是否可以接受呢？</p>
<p>我们拿散列表来举例。假设我们要爬取 10 亿个网页（像 Google、百度这样的通用搜索引擎，爬取的网页可能会更多），为了判重，我们把这 10 亿网页链接存储在散列表中。你来估算下，大约需要多少内存？</p>
<p>假设一个 URL 的平均长度是 64 字节，那单纯存储这 10 亿个 URL，需要大约 60GB 的内存空间。因为散列表必须维持较小的装载因子，才能保证不会出现过多的散列冲突，导致操作的性能下降。而且，用链表法解决冲突的散列表，还会存储链表指针。所以，如果将这 10 亿个 URL 构建成散列表，那需要的内存空间会远大于 60GB，有可能会超过 100GB。</p>
<p>当然，对于一个大型的搜索引擎来说，即便是 100GB 的内存要求，其实也不算太高，我们可以采用分治的思想，用多台机器（比如 20 台内存是 8GB 的机器）来存储这 10 亿网页链接。这种分治的处理思路，我们讲过很多次了，这里就不详细说了。</p>
<p>对于爬虫的 URL 去重这个问题，刚刚讲到的分治加散列表的思路，已经是可以实实在在工作的了。不过，<strong>作为一个有追求的工程师，我们应该考虑，在添加、查询数据的效率以及内存消耗方面，是否还有进一步的优化空间呢？</strong></p>
<p>你可能会说，散列表中添加、查找数据的时间复杂度已经是 O(1)，还能有进一步优化的空间吗？实际上，我们前面也讲过，时间复杂度并不能完全代表代码的执行时间。大 O 时间复杂度表示法，会忽略掉常数、系数和低阶，并且统计的对象是语句的频度。不同的语句，执行时间也是不同的。时间复杂度只是表示执行时间随数据规模的变化趋势，并不能度量在特定的数据规模下，代码执行时间的多少。</p>
<p>如果时间复杂度中原来的系数是 10，我们现在能够通过优化，将系数降为 1，那在时间复杂度没有变化的情况下，执行效率就提高了 10 倍。对于实际的软件开发来说，10 倍效率的提升，显然是一个非常值得的优化。</p>
<p>如果我们用基于链表的方法解决冲突问题，散列表中存储的是 URL，那当查询的时候，通过哈希函数定位到某个链表之后，我们还需要依次比对每个链表中的 URL。这个操作是比较耗时的，主要有两点原因。</p>
<p>一方面，链表中的结点在内存中不是连续存储的，所以不能一下子加载到 CPU 缓存中，没法很好地利用到 CPU 高速缓存，所以数据访问性能方面会打折扣。</p>
<p>另一方面，链表中的每个数据都是 URL，而 URL 不是简单的数字，是平均长度为 64 字节的字符串。也就是说，我们要让待判重的 URL，跟链表中的每个 URL，做字符串匹配。显然，这样一个字符串匹配操作，比起单纯的数字比对，要慢很多。所以，基于这两点，执行效率方面肯定是有优化空间的。</p>
<p>对于内存消耗方面的优化，除了刚刚这种基于散列表的解决方案，貌似没有更好的法子了。实际上，如果要想内存方面有明显的节省，那就得换一种解决方案，也就是我们今天要着重讲的这种存储结构，<strong>布隆过滤器</strong>（Bloom Filter）。</p>
<p>在讲布隆过滤器前，我要先讲一下另一种存储结构，<strong>位图</strong>（BitMap）。因为，布隆过滤器本身就是基于位图的，是对位图的一种改进。</p>
<p>我们先来看一个跟开篇问题非常类似、但比那个稍微简单的问题。<strong>我们有 1 千万个整数，整数的范围在 1 到 1 亿之间。如何快速查找某个整数是否在这 1 千万个整数中呢？</strong></p>
<p>当然，这个问题还是可以用散列表来解决。不过，我们可以使用一种比较“特殊”的散列表，那就是位图。我们申请一个大小为 1 亿、数据类型为布尔类型（true 或者 false）的数组。我们将这 1 千万个整数作为数组下标，将对应的数组值设置成 true。比如，整数 5 对应下标为 5 的数组值设置为 true，也就是 array[5]=true。</p>
<p>当我们查询某个整数 K 是否在这 1 千万个整数中的时候，我们只需要将对应的数组值 array[K]取出来，看是否等于 true。如果等于 true，那说明 1 千万整数中包含这个整数 K；相反，就表示不包含这个整数 K。</p>
<p>不过，很多语言中提供的布尔类型，大小是 1 个字节的，并不能节省太多内存空间。实际上，表示 true 和 false 两个值，我们只需要用一个二进制位（bit）就可以了。<strong>那如何通过编程语言，来表示一个二进制位呢？</strong></p>
<p>这里就要用到位运算了。我们可以借助编程语言中提供的数据类型，比如 int、long、char 等类型，通过位运算，用其中的某个位表示某个数字。文字描述起来有点儿不好理解，我把位图的代码实现写了出来，你可以对照着代码看下，应该就能看懂了。</p>
<p>public class BitMap { // Java中char类型占16bit，也即是2个字节</p>
<p>  private char[] bytes;</p>
<p>  private int nbits;</p>
<p>  public BitMap(int nbits) {</p>
<p>​    this.nbits = nbits;</p>
<p>​    this.bytes = new char[nbits/16+1];</p>
<p>  }</p>
<p>  public void set(int k) {</p>
<p>​    if (k &gt; nbits) return;</p>
<p>​    int byteIndex = k / 16;</p>
<p>​    int bitIndex = k % 16;</p>
<p>​    bytes[byteIndex] |= (1 &lt;&lt; bitIndex);</p>
<p>  }</p>
<p>  public boolean get(int k) {</p>
<p>​    if (k &gt; nbits) return false;</p>
<p>​    int byteIndex = k / 16;</p>
<p>​    int bitIndex = k % 16;</p>
<p>​    return (bytes[byteIndex] &amp; (1 &lt;&lt; bitIndex)) != 0;</p>
<p>  }</p>
<p>}</p>
<p>从刚刚位图结构的讲解中，你应该可以发现，位图通过数组下标来定位数据，所以，访问效率非常高。而且，每个数字用一个二进制位来表示，在数字范围不大的情况下，所需要的内存空间非常节省。</p>
<p>比如刚刚那个例子，如果用散列表存储这 1 千万的数据，数据是 32 位的整型数，也就是需要 4 个字节的存储空间，那总共至少需要 40MB 的存储空间。如果我们通过位图的话，数字范围在 1 到 1 亿之间，只需要 1 亿个二进制位，也就是 12MB 左右的存储空间就够了。</p>
<p>关于位图，我们就讲完了，是不是挺简单的？不过，这里我们有个假设，就是数字所在的范围不是很大。如果数字的范围很大，比如刚刚那个问题，数字范围不是 1 到 1 亿，而是 1 到 10 亿，那位图的大小就是 10 亿个二进制位，也就是 120MB 的大小，消耗的内存空间，不降反增。</p>
<p>这个时候，布隆过滤器就要出场了。布隆过滤器就是为了解决刚刚这个问题，对位图这种数据结构的一种改进。</p>
<p>还是刚刚那个例子，数据个数是 1 千万，数据的范围是 1 到 10 亿。布隆过滤器的做法是，我们仍然使用一个 1 亿个二进制大小的位图，然后通过哈希函数，对数字进行处理，让它落在这 1 到 1 亿范围内。比如我们把哈希函数设计成 f(x)=x%n。其中，x 表示数字，n 表示位图的大小（1 亿），也就是，对数字跟位图的大小进行取模求余。</p>
<p>不过，你肯定会说，哈希函数会存在冲突的问题啊，一亿零一和 1 两个数字，经过你刚刚那个取模求余的哈希函数处理之后，最后的结果都是 1。这样我就无法区分，位图存储的是 1 还是一亿零一了。</p>
<p>为了降低这种冲突概率，当然我们可以设计一个复杂点、随机点的哈希函数。除此之外，还有其他方法吗？我们来看布隆过滤器的处理方法。既然一个哈希函数可能会存在冲突，那用多个哈希函数一块儿定位一个数据，是否能降低冲突的概率呢？我来具体解释一下，布隆过滤器是怎么做的。</p>
<p>我们使用 K 个哈希函数，对同一个数字进行求哈希值，那会得到 K 个不同的哈希值，我们分别记作 <em>X</em>1，<em>X</em>2，<em>X</em>3，…，<em>X**K</em>。我们把这 K 个数字作为位图中的下标，将对应的 BitMap[<em>X</em>1]，BitMap[<em>X</em>2]，BitMap[<em>X</em>3]，…，BitMap[<em>X**K</em>]都设置成 true，也就是说，我们用 K 个二进制位，来表示一个数字的存在。</p>
<p>当我们要查询某个数字是否存在的时候，我们用同样的 K 个哈希函数，对这个数字求哈希值，分别得到 <em>Y</em>1，<em>Y</em>2，<em>Y</em>3，…，<em>Y**K</em>。我们看这 K 个哈希值，对应位图中的数值是否都为 true，如果都是 true，则说明，这个数字存在，如果有其中任意一个不为 true，那就说明这个数字不存在。</p>
<p><img src="https://static001.geekbang.org/resource/image/94/ae/94630c1c3b7657f560a1825bd9d02cae.jpg" alt="img"></p>
<p>对于两个不同的数字来说，经过一个哈希函数处理之后，可能会产生相同的哈希值。但是经过 K 个哈希函数处理之后，K 个哈希值都相同的概率就非常低了。尽管采用 K 个哈希函数之后，两个数字哈希冲突的概率降低了，但是，这种处理方式又带来了新的问题，那就是容易误判。我们看下面这个例子。</p>
<p><img src="https://static001.geekbang.org/resource/image/d0/1a/d0a3326ef0037f64102163209301aa1a.jpg" alt="img"></p>
<p>布隆过滤器的误判有一个特点，那就是，它只会对存在的情况有误判。如果某个数字经过布隆过滤器判断不存在，那说明这个数字真的不存在，不会发生误判；如果某个数字经过布隆过滤器判断存在，这个时候才会有可能误判，有可能并不存在。不过，只要我们调整哈希函数的个数、位图大小跟要存储数字的个数之间的比例，那就可以将这种误判的概率降到非常低。</p>
<p>尽管布隆过滤器会存在误判，但是，这并不影响它发挥大作用。很多场景对误判有一定的容忍度。比如我们今天要解决的爬虫判重这个问题，即便一个没有被爬取过的网页，被误判为已经被爬取，对于搜索引擎来说，也并不是什么大事情，是可以容忍的，毕竟网页太多了，搜索引擎也不可能 100% 都爬取到。</p>
<p>弄懂了布隆过滤器，我们今天的爬虫网页去重的问题，就很简单了。</p>
<p>我们用布隆过滤器来记录已经爬取过的网页链接，假设需要判重的网页有 10 亿，那我们可以用一个 10 倍大小的位图来存储，也就是 100 亿个二进制位，换算成字节，那就是大约 1.2GB。之前我们用散列表判重，需要至少 100GB 的空间。相比来讲，布隆过滤器在存储空间的消耗上，降低了非常多。</p>
<p>那我们再来看下，利用布隆过滤器，在执行效率方面，是否比散列表更加高效呢？</p>
<p>布隆过滤器用多个哈希函数对同一个网页链接进行处理，CPU 只需要将网页链接从内存中读取一次，进行多次哈希计算，理论上讲这组操作是 CPU 密集型的。而在散列表的处理方式中，需要读取散列值相同（散列冲突）的多个网页链接，分别跟待判重的网页链接，进行字符串匹配。这个操作涉及很多内存数据的读取，所以是内存密集型的。我们知道 CPU 计算可能是要比内存访问更快速的，所以，理论上讲，布隆过滤器的判重方式，更加快速。</p>
<h2 id="总结引申-2"><a href="#总结引申-2" class="headerlink" title="总结引申"></a>总结引申</h2><p>今天，关于搜索引擎爬虫网页去重问题的解决，我们从散列表讲到位图，再讲到布隆过滤器。布隆过滤器非常适合这种不需要 100% 准确的、允许存在小概率误判的大规模判重场景。除了爬虫网页去重这个例子，还有比如统计一个大型网站的每天的 UV 数，也就是每天有多少用户访问了网站，我们就可以使用布隆过滤器，对重复访问的用户进行去重。</p>
<p>我们前面讲到，布隆过滤器的误判率，主要跟哈希函数的个数、位图的大小有关。当我们往布隆过滤器中不停地加入数据之后，位图中不是 true 的位置就越来越少了，误判率就越来越高了。所以，对于无法事先知道要判重的数据个数的情况，我们需要支持自动扩容的功能。</p>
<p>当布隆过滤器中，数据个数与位图大小的比例超过某个阈值的时候，我们就重新申请一个新的位图。后面来的新数据，会被放置到新的位图中。但是，如果我们要判断某个数据是否在布隆过滤器中已经存在，我们就需要查看多个位图，相应的执行效率就降低了一些。</p>
<p>位图、布隆过滤器应用如此广泛，很多编程语言都已经实现了。比如 Java 中的 BitSet 类就是一个位图，Redis 也提供了 BitMap 位图类，Google 的 Guava 工具包提供了 BloomFilter 布隆过滤器的实现。如果你感兴趣，你可以自己去研究下这些实现的源码。</p>
<h2 id="课后思考-2"><a href="#课后思考-2" class="headerlink" title="课后思考"></a>课后思考</h2><p>假设我们有 1 亿个整数，数据范围是从 1 到 10 亿，如何快速并且省内存地给这 1 亿个数据从小到大排序？</p>
<p>还记得我们在哈希函数（下）讲过的利用分治思想，用散列表以及哈希函数，实现海量图库中的判重功能吗？如果我们允许小概率的误判，那是否可以用今天的布隆过滤器来解决呢？你可以参照我们当时的估算方法，重新估算下，用布隆过滤器需要多少台机器？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="46-概率统计：如何利用朴素贝叶斯算法过滤垃圾短信？"><a href="#46-概率统计：如何利用朴素贝叶斯算法过滤垃圾短信？" class="headerlink" title="46 | 概率统计：如何利用朴素贝叶斯算法过滤垃圾短信？"></a>46 | 概率统计：如何利用朴素贝叶斯算法过滤垃圾短信？</h1><p>王争 2019-01-11</p>
<p><img src="https://static001.geekbang.org/resource/image/0d/0a/0dd3a82c32db9385c85d039448b7570a.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:08</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：13.37M 时长：14:35</p>
<p>上一节我们讲到，如何用位图、布隆过滤器，来过滤重复的数据。今天，我们再讲一个跟过滤相关的问题，如何过滤垃圾短信？</p>
<p>垃圾短信和骚扰电话，我想每个人都收到过吧？买房、贷款、投资理财、开发票，各种垃圾短信和骚扰电话，不胜其扰。<strong>如果你是一名手机应用开发工程师，让你实现一个简单的垃圾短信过滤功能以及骚扰电话拦截功能，该用什么样的数据结构和算法实现呢？</strong></p>
<h2 id="算法解析-3"><a href="#算法解析-3" class="headerlink" title="算法解析"></a>算法解析</h2><p>实际上，解决这个问题并不会涉及很高深的算法。今天，我就带你一块看下，如何利用简单的数据结构和算法，解决这种看似非常复杂的问题。</p>
<h3 id="1-基于黑名单的过滤器"><a href="#1-基于黑名单的过滤器" class="headerlink" title="1. 基于黑名单的过滤器"></a>1. 基于黑名单的过滤器</h3><p>我们可以维护一个骚扰电话号码和垃圾短信发送号码的黑名单。这个黑名单的收集，有很多途径，比如，我们可以从一些公开的网站上下载，也可以通过类似“360 骚扰电话拦截”的功能，通过用户自主标记骚扰电话来收集。对于被多个用户标记，并且标记个数超过一定阈值的号码，我们就可以定义为骚扰电话，并将它加入到我们的黑名单中。</p>
<p>如果黑名单中的电话号码不多的话，我们可以使用散列表、二叉树等动态数据结构来存储，对内存的消耗并不会很大。如果我们把每个号码看作一个字符串，并且假设平均长度是 16 个字节，那存储 50 万个电话号码，大约需要 10MB 的内存空间。即便是对于手机这样的内存有限的设备来说，这点内存的消耗也是可以接受的。</p>
<p>但是，如果黑名单中的电话号码很多呢？比如有 500 万个。这个时候，如果再用散列表存储，就需要大约 100MB 的存储空间。为了实现一个拦截功能，耗费用户如此多的手机内存，这显然有点儿不合理。</p>
<p>上一节我们讲了，布隆过滤器最大的特点就是比较省存储空间，所以，用它来解决这个问题再合适不过了。如果我们要存储 500 万个手机号码，我们把位图大小设置为 10 倍数据大小，也就是 5000 万，那也只需要使用 5000 万个二进制位（5000 万 bits），换算成字节，也就是不到 7MB 的存储空间。比起散列表的解决方案，内存的消耗减少了很多。</p>
<p>实际上，我们还有一种时间换空间的方法，可以将内存的消耗优化到极致。</p>
<p>我们可以把黑名单存储在服务器端上，把过滤和拦截的核心工作，交给服务器端来做。手机端只负责将要检查的号码发送给服务器端，服务器端通过查黑名单，判断这个号码是否应该被拦截，并将结果返回给手机端。</p>
<p>用这个解决思路完全不需要占用手机内存。不过，有利就有弊。我们知道，网络通信是比较慢的，所以，网络延迟就会导致处理速度降低。而且，这个方案还有个硬性要求，那就是只有在联网的情况下，才能正常工作。</p>
<p>基于黑名单的过滤器我就讲完了，不过，你可能还会说，布隆过滤器会有判错的概率呀！如果它把一个重要的电话或者短信，当成垃圾短信或者骚扰电话拦截了，对于用户来说，这是无法接受的。你说得没错，这是一个很大的问题。不过，我们现在先放一放，等三种过滤器都讲完之后，我再来解答。</p>
<h3 id="2-基于规则的过滤器"><a href="#2-基于规则的过滤器" class="headerlink" title="2. 基于规则的过滤器"></a>2. 基于规则的过滤器</h3><p>刚刚讲了一种基于黑名单的垃圾短信过滤方法，但是，如果某个垃圾短信发送者的号码并不在黑名单中，那这种方法就没办法拦截了。所以，基于黑名单的过滤方式，还不够完善，我们再继续看一种基于规则的过滤方式。</p>
<p>对于垃圾短信来说，我们还可以通过短信的内容，来判断某条短信是否是垃圾短信。我们预先设定一些规则，如果某条短信符合这些规则，我们就可以判定它是垃圾短信。实际上，规则可以有很多，比如下面这几个：</p>
<p>短信中包含特殊单词（或词语），比如一些非法、淫秽、反动词语等；</p>
<p>短信发送号码是群发号码，非我们正常的手机号码，比如 +60389585；</p>
<p>短信中包含回拨的联系方式，比如手机号码、微信、QQ、网页链接等，因为群发短信的号码一般都是无法回拨的；</p>
<p>短信格式花哨、内容很长，比如包含各种表情、图片、网页链接等；</p>
<p>符合已知垃圾短信的模板。垃圾短信一般都是重复群发，对于已经判定为垃圾短信的短信，我们可以抽象成模板，将获取到的短信与模板匹配，一旦匹配，我们就可以判定为垃圾短信。</p>
<p>当然，如果短信只是满足其中一条规则，如果就判定为垃圾短信，那会存在比较大的误判的情况。我们可以综合多条规则进行判断。比如，满足 2 条以上才会被判定为垃圾短信；或者每条规则对应一个不同的得分，满足哪条规则，我们就累加对应的分数，某条短信的总得分超过某个阈值，才会被判定为垃圾短信。</p>
<p>不过，我只是给出了一些制定规则的思路，具体落实到执行层面，其实还有很大的距离，还有很多细节需要处理。比如，第一条规则中，我们该如何定义特殊单词；第二条规则中，我们该如何定义什么样的号码是群发号码等等。限于篇幅，我就不一一详细展开来讲了。我这里只讲一下，如何定义特殊单词？</p>
<p>如果我们只是自己拍脑袋想，哪些单词属于特殊单词，那势必有比较大的主观性，也很容易漏掉某些单词。实际上，我们可以基于概率统计的方法，借助计算机强大的计算能力，找出哪些单词最常出现在垃圾短信中，将这些最常出现的单词，作为特殊单词，用来过滤短信。</p>
<p>不过这种方法的前提是，我们有大量的样本数据，也就是说，要有大量的短信（比如 1000 万条短信），并且我们还要求，每条短信都做好了标记，它是垃圾短信还是非垃圾短信。</p>
<p>我们对这 1000 万条短信，进行分词处理（借助中文或者英文分词算法），去掉“的、和、是”等没有意义的停用词（Stop words），得到 n 个不同的单词。针对每个单词，我们统计有多少个垃圾短信出现了这个单词，有多少个非垃圾短信会出现这个单词，进而求出每个单词出现在垃圾短信中的概率，以及出现在非垃圾短信中的概率。如果某个单词出现在垃圾短信中的概率，远大于出现在非垃圾短信中的概率，那我们就把这个单词作为特殊单词，用来过滤垃圾短信。</p>
<p>文字描述不好理解，我举个例子来解释一下。</p>
<p><img src="https://static001.geekbang.org/resource/image/05/c0/05b9358cac3721e746bbfec8b705cdc0.jpg" alt="img"></p>
<h3 id="3-基于概率统计的过滤器"><a href="#3-基于概率统计的过滤器" class="headerlink" title="3. 基于概率统计的过滤器"></a>3. 基于概率统计的过滤器</h3><p>基于规则的过滤器，看起来很直观，也很好理解，但是它也有一定的局限性。一方面，这些规则受人的思维方式局限，规则未免太过简单；另一方面，垃圾短信发送者可能会针对规则，精心设计短信，绕过这些规则的拦截。对此，我们再来看一种更加高级的过滤方式，基于概率统计的过滤方式。</p>
<p>这种基于概率统计的过滤方式，基础理论是基于朴素贝叶斯算法。为了让你更好地理解下面的内容，我们先通过一个非常简单的例子来看下，什么是朴素贝叶斯算法？</p>
<p>假设事件 A 是“小明不去上学”，事件 B 是“下雨了”。我们现在统计了一下过去 10 天的下雨情况和小明上学的情况，作为样本数据。</p>
<p><img src="https://static001.geekbang.org/resource/image/e8/32/e8a0bf4643453266c012e5384fc29932.jpg" alt="img"></p>
<p>我们来分析一下，这组样本有什么规律。在这 10 天中，有 4 天下雨，所以下雨的概率 P(B)=4/10。10 天中有 3 天，小明没有去上学，所以小明不去上学的概率 P(A)=3/10。在 4 个下雨天中，小明有 2 天没去上学，所以下雨天不去上学的概率 P(A|B)=2/4。在小明没有去上学的 3 天中，有 2 天下雨了，所以小明因为下雨而不上学的概率是 P(B|A)=2/3。实际上，这 4 个概率值之间，有一定的关系，这个关系就是朴素贝叶斯算法，我们用公式表示出来，就是下面这个样子。</p>
<p><img src="https://static001.geekbang.org/resource/image/fb/cc/fbef6a760f916941bc3128c2d32540cc.jpg" alt="img"></p>
<p>朴素贝叶斯算法是不是非常简单？我们用一个公式就可以将它概括。弄懂了朴素贝叶斯算法，我们再回到垃圾短信过滤这个问题上，看看如何利用朴素贝叶斯算法，来做垃圾短信的过滤。</p>
<p>基于概率统计的过滤器，是基于短信内容来判定是否是垃圾短信。而计算机没办法像人一样理解短信的含义。所以，我们需要把短信抽象成一组计算机可以理解并且方便计算的<strong>特征项</strong>，用这一组特征项代替短信本身，来做垃圾短信过滤。</p>
<p>我们可以通过分词算法，把一个短信分割成 n 个单词。这 n 个单词就是一组特征项，全权代表这个短信。因此，判定一个短信是否是垃圾短信这样一个问题，就变成了，判定同时包含这几个单词的短信是否是垃圾短信。</p>
<p>不过，这里我们并不像基于规则的过滤器那样，非黑即白，一个短信要么被判定为垃圾短信、要么被判定为非垃圾短息。我们使用概率，来表征一个短信是垃圾短信的可信程度。如果我们用公式将这个概率表示出来，就是下面这个样子：</p>
<p><img src="https://static001.geekbang.org/resource/image/b8/e7/b8f76a5fd26f785055b78ffe08ccfbe7.jpg" alt="img"></p>
<p>尽管我们有大量的短信样本，但是我们没法通过样本数据统计得到这个概率。为什么不可以呢？你可能会说，我只需要统计同时包含 <em>W</em>1，<em>W</em>2，<em>W</em>3，…，<em>W**n</em> 这 n 个单词的短信有多少个（我们假设有 x 个），然后看这里面属于垃圾短信的有几个（我们假设有 y 个），那包含 <em>W</em>1，<em>W</em>2，<em>W</em>3，…，<em>W**n</em> 这 n 个单词的短信是垃圾短信的概率就是 y/x。</p>
<p>理想很丰满，但现实往往很骨感。你忽视了非常重要的一点，那就是样本的数量再大，毕竟也是有限的，样本中不会有太多同时包含 <em>W</em>1，<em>W</em>2，<em>W</em>3，…，<em>W**n</em> 的短信的，甚至很多时候，样本中根本不存在这样的短信。没有样本，也就无法计算概率。所以这样的推理方式虽然正确，但是实践中并不好用。</p>
<p>这个时候，朴素贝叶斯公式就可以派上用场了。我们通过朴素贝叶斯公式，将这个概率的求解，分解为其他三个概率的求解。你可以看我画的图。那转化之后的三个概率是否可以通过样本统计得到呢？</p>
<p><img src="https://static001.geekbang.org/resource/image/39/ae/39c57b1a8a008e50a9f6cb8b7b9c9bae.jpg" alt="img"></p>
<p>P（<em>W</em>1，<em>W</em>2，<em>W</em>3，…，<em>W**n</em> 同时出现在一条短信中 | 短信是垃圾短信）这个概率照样无法通过样本来统计得到。但是我们可以基于下面这条著名的概率规则来计算。</p>
<p>独立事件发生的概率计算公式：P(A*B) = P(A)*P(B)</p>
<p>如果事件 A 和事件 B 是独立事件，两者的发生没有相关性，事件 A 发生的概率 P(A) 等于 p1，事件 B 发生的概率 P(B) 等于 p2，那两个同时发生的概率 P(A*B) 就等于 P(A)*P(B)。</p>
<p>基于这条独立事件发生概率的计算公式，我们可以把 P（W1，W2，W3，…，Wn 同时出现在一条短信中 | 短信是垃圾短信）分解为下面这个公式：</p>
<p><img src="https://static001.geekbang.org/resource/image/6c/f2/6c261a3f5312c515cf348cc59a5e73f2.jpg" alt="img"></p>
<p>其中，P（<em>W**i</em> 出现在短信中 | 短信是垃圾短信）表示垃圾短信中包含 <em>W**i</em> 这个单词的概率有多大。这个概率值通过统计样本很容易就能获得。我们假设垃圾短信有 y 个，其中包含 <em>W**i</em> 的有 x 个，那这个概率值就等于 x/y。</p>
<p>P（<em>W</em>1，<em>W</em>2，<em>W</em>3，…，<em>W**n</em> 同时出现在一条短信中 | 短信是垃圾短信）这个概率值，我们就计算出来了，我们再来看下剩下两个。</p>
<p>P（短信是垃圾短信）表示短信是垃圾短信的概率，这个很容易得到。我们把样本中垃圾短信的个数除以总样本短信个数，就是短信是垃圾短信的概率。</p>
<p>不过，P（<em>W</em>1，<em>W</em>2，<em>W</em>3，…，<em>W**n</em> 同时出现在一条短信中）这个概率还是不好通过样本统计得到，原因我们前面说过了，样本空间有限。不过，我们没必要非得计算这一部分的概率值。为什么这么说呢？</p>
<p>实际上，我们可以分别计算同时包含 <em>W</em>1，<em>W</em>2，<em>W</em>3，…，<em>W**n</em> 这 n 个单词的短信，是垃圾短信和非垃圾短信的概率。假设它们分别是 p1 和 p2。我们并不需要单纯地基于 p1 值的大小来判断是否是垃圾短信，而是通过对比 p1 和 p2 值的大小，来判断一条短信是否是垃圾短信。更细化一点讲，那就是，如果 p1 是 p2 的很多倍（比如 10 倍），我们才确信这条短信是垃圾短信。</p>
<p><img src="https://static001.geekbang.org/resource/image/0f/2a/0f0369a955ee8d15bd7d7958829d5b2a.jpg" alt="img"></p>
<p>基于这两个概率的倍数来判断是否是垃圾短信的方法，我们就可以不用计算 P（<em>W</em>1，<em>W</em>2，<em>W</em>3，…，<em>W**n</em> 同时出现在一条短信中）这一部分的值了，因为计算 p1 与 p2 的时候，都会包含这个概率值的计算，所以在求解 p1 和 p2 倍数（p1/p2）的时候，我们也就不需要这个值。</p>
<h2 id="总结引申-3"><a href="#总结引申-3" class="headerlink" title="总结引申"></a>总结引申</h2><p>今天，我们讲了基于黑名单、规则、概率统计三种垃圾短信的过滤方法，实际上，今天讲的这三种方法，还可以应用到很多类似的过滤、拦截的领域，比如垃圾邮件的过滤等等。</p>
<p>在讲黑名单过滤的时候，我讲到布隆过滤器可能会存在误判情况，可能会导致用户投诉。实际上，我们可以结合三种不同的过滤方式的结果，对同一个短信处理，如果三者都表明这个短信是垃圾短信，我们才把它当作垃圾短信拦截过滤，这样就会更精准。</p>
<p>当然，在实际的工程中，我们还需要结合具体的场景，以及大量的实验，不断去调整策略，权衡垃圾短信判定的<strong>准确率</strong>（是否会把不是垃圾的短信错判为垃圾短信）和<strong>召回率</strong>（是否能把所有的垃圾短信都找到），来实现我们的需求。</p>
<h2 id="课后思考-3"><a href="#课后思考-3" class="headerlink" title="课后思考"></a>课后思考</h2><p>关于垃圾短信过滤和骚扰电话的拦截，我们可以一块儿头脑风暴一下，看看你还有没有其他方法呢？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="47-向量空间：如何实现一个简单的音乐推荐系统？"><a href="#47-向量空间：如何实现一个简单的音乐推荐系统？" class="headerlink" title="47 | 向量空间：如何实现一个简单的音乐推荐系统？"></a>47 | 向量空间：如何实现一个简单的音乐推荐系统？</h1><p>王争 2019-01-14</p>
<p><img src="https://static001.geekbang.org/resource/image/1d/5b/1d138ccc0ec51be75ec7e790f895c25b.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:07</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：7.67M 时长：08:22</p>
<p>很多人都喜爱听歌，以前我们用 MP3 听歌，现在直接通过音乐 App 在线就能听歌。而且，各种音乐 App 的功能越来越强大，不仅可以自己选歌听，还可以根据你听歌的口味偏好，给你推荐可能会喜爱的音乐，而且有时候，推荐的音乐还非常适合你的口味，甚至会惊艳到你！如此智能的一个功能，你知道它是怎么实现的吗？</p>
<h2 id="算法解析-4"><a href="#算法解析-4" class="headerlink" title="算法解析"></a>算法解析</h2><p>实际上，要解决这个问题，并不需要特别高深的理论。解决思路的核心思想非常简单、直白，用两句话就能总结出来。</p>
<p>找到跟你口味偏好相似的用户，把他们爱听的歌曲推荐给你；</p>
<p>找出跟你喜爱的歌曲特征相似的歌曲，把这些歌曲推荐给你。</p>
<p>接下来，我就分别讲解一下这两种思路的具体实现方法。</p>
<h3 id="1-基于相似用户做推荐"><a href="#1-基于相似用户做推荐" class="headerlink" title="1. 基于相似用户做推荐"></a>1. 基于相似用户做推荐</h3><p>如何找到跟你口味偏好相似的用户呢？或者说如何定义口味偏好相似呢？实际上，思路也很简单，我们把跟你听类似歌曲的人，看作口味相似的用户。你可以看我下面画的这个图。我用“1”表示“喜爱”，用“0”笼统地表示“不发表意见”。从图中我们可以看出，你跟小明共同喜爱的歌曲最多，有 5 首。于是，我们就可以说，小明跟你的口味非常相似。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83goc1xej30vq0f4q51.jpg" alt="img"></p>
<p>我们只需要遍历所有的用户，对比每个用户跟你共同喜爱的歌曲个数，并且设置一个阈值，如果你和某个用户共同喜爱的歌曲个数超过这个阈值，我们就把这个用户看作跟你口味相似的用户，把这个用户喜爱但你还没听过的歌曲，推荐给你。</p>
<p>不过，刚刚的这个解决方案中有一个问题，我们如何知道用户喜爱哪首歌曲呢？也就是说，如何定义用户对某首歌曲的喜爱程度呢？</p>
<p>实际上，我们可以通过用户的行为，来定义这个喜爱程度。我们给每个行为定义一个得分，得分越高表示喜爱程度越高。</p>
<p><img src="https://static001.geekbang.org/resource/image/93/a6/93c26a89303a748199528fdd998ebba6.jpg" alt="img"></p>
<p>还是刚刚那个例子，我们如果把每个人对每首歌曲的喜爱程度表示出来，就是下面这个样子。图中，某个人对某首歌曲是否喜爱，我们不再用“1”或者“0”来表示，而是对应一个具体的分值。</p>
<p><img src="https://static001.geekbang.org/resource/image/05/a9/056552502f1cf4fdf331488e0eed5fa9.jpg" alt="img"></p>
<p>有了这样一个用户对歌曲的喜爱程度的对应表之后，如何来判断两个用户是否口味相似呢？</p>
<p>显然，我们不能再像之前那样，采用简单的计数来统计两个用户之间的相似度。还记得我们之前讲字符串相似度度量时，提到的编辑距离吗？这里的相似度度量，我们可以使用另外一个距离，那就是<strong>欧几里得距离</strong>（Euclidean distance）。欧几里得距离是用来计算两个向量之间的距离的。这个概念中有两个关键词，向量和距离，我来给你解释一下。</p>
<p>一维空间是一条线，我们用 1，2，3……这样单个的数，来表示一维空间中的某个位置；二维空间是一个面，我们用（1，3）（4，2）（2，2）……这样的两个数，来表示二维空间中的某个位置；三维空间是一个立体空间，我们用（1，3，5）（3，1，7）（2，4，3）……这样的三个数，来表示三维空间中的某个位置。一维、二维、三维应该都不难理解，那更高维中的某个位置该如何表示呢？</p>
<p>类比一维、二维、三维的表示方法，K 维空间中的某个位置，我们可以写作（<em>X</em>1，<em>X</em>2，<em>X</em>3，…，<em>X**K</em>）。这种表示方法就是<strong>向量</strong>（vector）。我们知道，二维、三维空间中，两个位置之间有距离的概念，类比到高纬空间，同样也有距离的概念，这就是我们说的两个向量之间的距离。</p>
<p>那如何计算两个向量之间的距离呢？我们还是可以类比到二维、三维空间中距离的计算方法。通过类比，我们就可以得到两个向量之间距离的计算公式。这个计算公式就是欧几里得距离的计算公式：</p>
<p><img src="https://static001.geekbang.org/resource/image/f4/12/f4d1d906c076688a43380f82e47dce12.jpg" alt="img"></p>
<p>我们把每个用户对所有歌曲的喜爱程度，都用一个向量表示。我们计算出两个向量之间的欧几里得距离，作为两个用户的口味相似程度的度量。从图中的计算可以看出，小明与你的欧几里得距离距离最小，也就是说，你俩在高维空间中靠得最近，所以，我们就断定，小明跟你的口味最相似。</p>
<p><img src="https://static001.geekbang.org/resource/image/3e/89/3e145a3054c1abdea5d3f207d13e9b89.jpg" alt="img"></p>
<h3 id="2-基于相似歌曲做推荐"><a href="#2-基于相似歌曲做推荐" class="headerlink" title="2. 基于相似歌曲做推荐"></a>2. 基于相似歌曲做推荐</h3><p>刚刚我们讲了基于相似用户的歌曲推荐方法，但是，如果用户是一个新用户，我们还没有收集到足够多的行为数据，这个时候该如何推荐呢？我们现在再来看另外一种推荐方法，基于相似歌曲的推荐方法，也就是说，如果某首歌曲跟你喜爱的歌曲相似，我们就把它推荐给你。</p>
<p>如何判断两首歌曲是否相似呢？对于人来说，这个事情可能会比较简单，但是对于计算机来说，判断两首歌曲是否相似，那就需要通过量化的数据来表示了。我们应该通过什么数据来量化两个歌曲之间的相似程度呢？</p>
<p>最容易想到的是，我们对歌曲定义一些特征项，比如是伤感的还是愉快的，是摇滚还是民谣，是柔和的还是高亢的等等。类似基于相似用户的推荐方法，我们给每个歌曲的每个特征项打一个分数，这样每个歌曲就都对应一个特征项向量。我们可以基于这个特征项向量，来计算两个歌曲之间的欧几里得距离。欧几里得距离越小，表示两个歌曲的相似程度越大。</p>
<p>但是，要实现这个方案，需要有一个前提，那就是我们能够找到足够多，并且能够全面代表歌曲特点的特征项，除此之外，我们还要人工给每首歌标注每个特征项的得分。对于收录了海量歌曲的音乐 App 来说，这显然是一个非常大的工程。此外，人工标注有很大的主观性，也会影响到推荐的准确性。</p>
<p>既然基于歌曲特征项计算相似度不可行，那我们就换一种思路。对于两首歌，如果喜欢听的人群都是差不多的，那侧面就可以反映出，这两首歌比较相似。如图所示，每个用户对歌曲有不同的喜爱程度，我们依旧通过上一个解决方案中定义得分的标准，来定义喜爱程度。</p>
<p><img src="https://static001.geekbang.org/resource/image/a3/ff/a324908e162a60efea4bd7c47c04a6ff.jpg" alt="img"></p>
<p>你有没有发现，这个图跟基于相似用户推荐中的图几乎一样。只不过这里把歌曲和用户主次颠倒了一下。基于相似用户的推荐方法中，针对每个用户，我们将对各个歌曲的喜爱程度作为向量。基于相似歌曲的推荐思路中，针对每个歌曲，我们将每个用户的打分作为向量。</p>
<p>有了每个歌曲的向量表示，我们通过计算向量之间的欧几里得距离，来表示歌曲之间的相似度。欧几里得距离越小，表示两个歌曲越相似。然后，我们就在用户已经听过的歌曲中，找出他喜爱程度较高的歌曲。然后，我们找出跟这些歌曲相似度很高的其他歌曲，推荐给他。</p>
<h2 id="总结引申-4"><a href="#总结引申-4" class="headerlink" title="总结引申"></a>总结引申</h2><p>实际上，这个问题是<strong>推荐系统</strong>（Recommendation System）里最典型的一类问题。之所以讲这部分内容，主要还是想给你展示，算法的强大之处，利用简单的向量空间的欧几里得距离，就能解决如此复杂的问题。不过，今天，我只给你讲解了基本的理论，实践中遇到的问题还有很多，比如冷启动问题，产品初期积累的数据不多，不足以做推荐等等。这些更加深奥的内容，你可以之后自己在实践中慢慢探索。</p>
<h2 id="课后思考-4"><a href="#课后思考-4" class="headerlink" title="课后思考"></a>课后思考</h2><p>关于今天讲的推荐算法，你还能想到其他应用场景吗？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="48-B-树：MySQL数据库索引是如何实现的？"><a href="#48-B-树：MySQL数据库索引是如何实现的？" class="headerlink" title="48 | B+树：MySQL数据库索引是如何实现的？"></a>48 | B+树：MySQL数据库索引是如何实现的？</h1><p>王争 2019-01-16</p>
<p><img src="https://static001.geekbang.org/resource/image/36/dc/36475968a64a81ceb13b8be7a1f20edc.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:23</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：12.84M 时长：14:00</p>
<p>作为一个软件开发工程师，你对数据库肯定再熟悉不过了。作为主流的数据存储系统，它在我们的业务开发中，有着举足轻重的地位。在工作中，为了加速数据库中数据的查找速度，我们常用的处理思路是，对表中数据创建索引。那你是否思考过，<strong>数据库索引是如何实现的呢？底层使用的是什么数据结构和算法呢？</strong></p>
<h2 id="算法解析-5"><a href="#算法解析-5" class="headerlink" title="算法解析"></a>算法解析</h2><p>思考的过程比结论更重要。跟着我学习了这么多节课，很多同学已经意识到这一点，比如 Jerry 银银同学。我感到很开心。所以，今天的讲解，我会尽量还原这个解决方案的思考过程，让你知其然，并且知其所以然。</p>
<h3 id="1-解决问题的前提是定义清楚问题"><a href="#1-解决问题的前提是定义清楚问题" class="headerlink" title="1. 解决问题的前提是定义清楚问题"></a>1. 解决问题的前提是定义清楚问题</h3><p>如何定义清楚问题呢？除了对问题进行详细的调研，还有一个办法，那就是，通过<strong>对一些模糊的需求进行假设，来限定<strong><strong>要</strong></strong>解决的问题的范围</strong>。</p>
<p>如果你对数据库的操作非常了解，针对我们现在这个问题，你就能把索引的需求定义得非常清楚。但是，对于大部分软件工程师来说，我们可能只了解一小部分常用的 SQL 语句，所以，这里我们假设要解决的问题，只包含这样两个常用的需求：</p>
<p>根据某个值查找数据，比如 select * from user where id=1234；</p>
<p>根据区间值来查找某些数据，比如 select * from user where id &gt; 1234 and id &lt; 2345。</p>
<p>除了这些功能性需求之外，这种问题往往还会涉及一些非功能性需求，比如安全、性能、用户体验等等。限于专栏要讨论的主要是数据结构和算法，对于非功能性需求，我们着重考虑<strong>性能方面</strong>的需求。性能方面的需求，我们主要考察时间和空间两方面，也就是<strong>执行效率和存储空间</strong>。</p>
<p>在执行效率方面，我们希望通过索引，查询数据的效率尽可能地高；在存储空间方面，我们希望索引不要消耗太多的内存空间。</p>
<h3 id="2-尝试用学过的数据结构解决这个问题"><a href="#2-尝试用学过的数据结构解决这个问题" class="headerlink" title="2. 尝试用学过的数据结构解决这个问题"></a>2. 尝试用学过的数据结构解决这个问题</h3><p>问题的需求大致定义清楚了，我们现在回想一下，能否利用已经学习过的数据结构解决这个问题呢？支持快速查询、插入等操作的动态数据结构，我们已经学习过散列表、平衡二叉查找树、跳表。</p>
<p>我们先来看<strong>散列表</strong>。散列表的查询性能很好，时间复杂度是 O(1)。但是，散列表不能支持按照区间快速查找数据。所以，散列表不能满足我们的需求。</p>
<p>我们再来看<strong>平衡二叉查找树</strong>。尽管平衡二叉查找树查询的性能也很高，时间复杂度是 O(logn)。而且，对树进行中序遍历，我们还可以得到一个从小到大有序的数据序列，但这仍然不足以支持按照区间快速查找数据。</p>
<p>我们再来看<strong>跳表</strong>。跳表是在链表之上加上多层索引构成的。它支持快速地插入、查找、删除数据，对应的时间复杂度是 O(logn)。并且，跳表也支持按照区间快速地查找数据。我们只需要定位到区间起点值对应在链表中的结点，然后从这个结点开始，顺序遍历链表，直到区间终点对应的结点为止，这期间遍历得到的数据就是满足区间值的数据。</p>
<p><img src="https://static001.geekbang.org/resource/image/49/65/492206afe5e2fef9f683c7cff83afa65.jpg" alt="img"></p>
<p>这样看来，跳表是可以解决这个问题。实际上，数据库索引所用到的数据结构跟跳表非常相似，叫作 B+ 树。不过，它是通过二叉查找树演化过来的，而非跳表。为了给你还原发明 B+ 树的整个思考过程，所以，接下来，我还要从二叉查找树讲起，看它是如何一步一步被改造成 B+ 树的。</p>
<h3 id="3-改造二叉查找树来解决这个问题"><a href="#3-改造二叉查找树来解决这个问题" class="headerlink" title="3. 改造二叉查找树来解决这个问题"></a>3. 改造二叉查找树来解决这个问题</h3><p>为了让二叉查找树支持按照区间来查找数据，我们可以对它进行这样的改造：树中的节点并不存储数据本身，而是只是作为索引。除此之外，我们把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的。经过改造之后的二叉树，就像图中这样，看起来是不是很像跳表呢？</p>
<p><img src="https://static001.geekbang.org/resource/image/25/f4/25700c1dc28ce094eed3ffac394531f4.jpg" alt="img"></p>
<p>改造之后，如果我们要求某个区间的数据。我们只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，我们再顺着链表往后遍历，直到链表中的结点数据值大于区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。</p>
<p><img src="https://static001.geekbang.org/resource/image/1c/cc/1cf179c03c702a6ef5b9336f5b1eaecc.jpg" alt="img"></p>
<p>但是，我们要为几千万、上亿的数据构建索引，如果将索引存储在内存中，尽管内存访问的速度非常快，查询的效率非常高，但是，占用的内存会非常多。</p>
<p>比如，我们给一亿个数据构建二叉查找树索引，那索引中会包含大约 1 亿个节点，每个节点假设占用 16 个字节，那就需要大约 1GB 的内存空间。给一张表建立索引，我们需要 1GB 的内存空间。如果我们要给 10 张表建立索引，那对内存的需求是无法满足的。如何解决这个索引占用太多内存的问题呢？</p>
<p>我们可以借助时间换空间的思路，把索引存储在硬盘中，而非内存中。我们都知道，硬盘是一个非常慢速的存储设备。通常内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。</p>
<p>这种将索引存储在硬盘中的方案，尽管减少了内存消耗，但是在数据查找的过程中，需要读取磁盘中的索引，因此数据查询效率就相应降低很多。</p>
<p>二叉查找树，经过改造之后，支持区间查找的功能就实现了。不过，为了节省内存，如果把树存储在硬盘中，那么每个节点的读取（或者访问），都对应一次磁盘 IO 操作。树的高度就等于每次查询数据时磁盘 IO 操作的次数。</p>
<p>我们前面讲到，比起内存读写操作，磁盘 IO 操作非常耗时，所以我们优化的重点就是尽量减少磁盘 IO 操作，也就是，尽量降低树的高度。那如何降低树的高度呢？</p>
<p>我们来看下，如果我们把索引构建成 m 叉树，高度是不是比二叉树要小呢？如图所示，给 16 个数据构建二叉树索引，树的高度是 4，查找一个数据，就需要 4 个磁盘 IO 操作（如果根节点存储在内存中，其他节点存储在磁盘中），如果对 16 个数据构建五叉树索引，那高度只有 2，查找一个数据，对应只需要 2 次磁盘操作。如果 m 叉树中的 m 是 100，那对一亿个数据构建索引，树的高度也只是 3，最多只要 3 次磁盘 IO 就能获取到数据。磁盘 IO 变少了，查找数据的效率也就提高了。</p>
<p><img src="https://static001.geekbang.org/resource/image/69/59/69d4c48c1257dcb7dd6077d961b86259.jpg" alt="img"></p>
<p><img src="https://static001.geekbang.org/resource/image/76/cc/769687f57190a826a8f6f82793491ccc.jpg" alt="img"></p>
<p>如果我们将 m 叉树实现 B+ 树索引，用代码实现出来，就是下面这个样子（假设我们给 int 类型的数据库字段添加索引，所以代码中的 keywords 是 int 类型的）：</p>
<p>/**</p>
<p> * 这是B+树非叶子节点的定义。</p>
<p> *</p>
<p> * 假设keywords=[3, 5, 8, 10]</p>
<p> * 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF)</p>
<p> * 5个区间分别对应：children[0]…children[4]</p>
<p> *</p>
<p> * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：</p>
<p> * PAGE_SIZE = (m-1)<em>4[keywordss大小]+m</em>8[children大小]</p>
<p> */</p>
<p>public class BPlusTreeNode {</p>
<p>  public static int m = 5; // 5叉树</p>
<p>  public int[] keywords = new int[m-1]; // 键值，用来划分数据区间</p>
<p>  public BPlusTreeNode[] children = new BPlusTreeNode[m];//保存子节点指针</p>
<p>}</p>
<p>/**</p>
<p> * 这是B+树中叶子节点的定义。</p>
<p> *</p>
<p> * B+树中的叶子节点跟内部节点是不一样的,</p>
<p> * 叶子节点存储的是值，而非区间。</p>
<p> * 这个定义里，每个叶子节点存储3个数据行的键值及地址信息。</p>
<p> *</p>
<p> * k值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：</p>
<p> * PAGE_SIZE = k<em>4[keyw..大小]+k</em>8[dataAd..大小]+8[prev大小]+8[next大小]</p>
<p> */</p>
<p>public class BPlusTreeLeafNode {</p>
<p>  public static int k = 3;</p>
<p>  public int[] keywords = new int[k]; // 数据的键值</p>
<p>  public long[] dataAddress = new long[k]; // 数据地址</p>
<p>  public BPlusTreeLeafNode prev; // 这个结点在链表中的前驱结点</p>
<p>  public BPlusTreeLeafNode next; // 这个结点在链表中的后继结点</p>
<p>}</p>
<p>我稍微解释一下这段代码。</p>
<p>对于相同个数的数据构建 m 叉树索引，m 叉树中的 m 越大，那树的高度就越小，那 m 叉树中的 m 是不是越大越好呢？到底多大才最合适呢？</p>
<p>不管是内存中的数据，还是磁盘中的数据，操作系统都是按页（一页大小通常是 4KB，这个值可以通过 getconfig PAGE_SIZE 命令查看）来读取的，一次会读一页的数据。如果要读取的数据量超过一页的大小，就会触发多次 IO 操作。所以，我们在选择 m 大小的时候，要尽量让每个节点的大小等于一个页的大小。读取一个节点，只需要一次磁盘 IO 操作。</p>
<p><img src="https://static001.geekbang.org/resource/image/ea/30/ea4472fd7bb7fa948532c8c8ba334430.jpg" alt="img"></p>
<p>尽管索引可以提高数据库的查询效率，但是，作为一名开发工程师，你应该也知道，索引有利也有弊，它也会让写入数据的效率下降。这是为什么呢？</p>
<p>数据的写入过程，会涉及索引的更新，这是索引导致写入变慢的主要原因。</p>
<p>对于一个 B+ 树来说，m 值是根据页的大小事先计算好的，也就是说，每个节点最多只能有 m 个子节点。在往数据库中写入数据的过程中，这样就有可能使索引中某些节点的子节点个数超过 m，这个节点的大小超过了一个页的大小，读取这样一个节点，就会导致多次磁盘 IO 操作。我们该如何解决这个问题呢？</p>
<p>实际上，处理思路并不复杂。我们只需要将这个节点分裂成两个节点。但是，节点分裂之后，其上层父节点的子节点个数就有可能超过 m 个。不过这也没关系，我们可以用同样的方法，将父节点也分裂成两个节点。这种级联反应会从下往上，一直影响到根节点。这个分裂过程，你可以结合着下面这个图一块看，会更容易理解（图中的 B+ 树是一个三叉树。我们限定叶子节点中，数据的个数超过 2 个就分裂节点；非叶子节点中，子节点的个数超过 3 个就分裂节点）。</p>
<p><img src="https://static001.geekbang.org/resource/image/18/e0/1800bc80e1e05b32a042ff6873e6c2e0.jpg" alt="img"></p>
<p>正是因为要时刻保证 B+ 树索引是一个 m 叉树，所以，索引的存在会导致数据库写入的速度降低。实际上，不光写入数据会变慢，删除数据也会变慢。这是为什么呢？</p>
<p>我们在删除某个数据的时候，也要对应地更新索引节点。这个处理思路有点类似跳表中删除数据的处理思路。频繁的数据删除，就会导致某些节点中，子节点的个数变得非常少，长此以往，如果每个节点的子节点都比较少，势必会影响索引的效率。</p>
<p>我们可以设置一个阈值。在 B+ 树中，这个阈值等于 m/2。如果某个节点的子节点个数小于 m/2，我们就将它跟相邻的兄弟节点合并。不过，合并之后节点的子节点个数有可能会超过 m。针对这种情况，我们可以借助插入数据时候的处理方法，再分裂节点。</p>
<p>文字描述不是很直观，我举了一个删除操作的例子，你可以对比着看下（图中的 B+ 树是一个五叉树。我们限定叶子节点中，数据的个数少于 2 个就合并节点；非叶子节点中，子节点的个数少于 3 个就合并节点。）。</p>
<p><img src="https://static001.geekbang.org/resource/image/17/18/1730e34450dad29f062e76536622c918.jpg" alt="img"></p>
<p>数据库索引以及 B+ 树的由来，到此就讲完了。你有没有发现，B+ 树的结构和操作，跟跳表非常类似。理论上讲，对跳表稍加改造，也可以替代 B+ 树，作为数据库的索引实现的。</p>
<p>B+ 树发明于 1972 年，跳表发明于 1989 年，我们可以大胆猜想下，跳表的作者有可能就是受了 B+ 树的启发，才发明出跳表来的。不过，这个也无从考证了。</p>
<h2 id="总结引申-5"><a href="#总结引申-5" class="headerlink" title="总结引申"></a>总结引申</h2><p>今天，我们讲解了数据库索引实现，依赖的底层数据结构，B+ 树。它通过存储在磁盘的多叉树结构，做到了时间、空间的平衡，既保证了执行效率，又节省了内存。</p>
<p>前面的讲解中，为了一步一步详细地给你介绍 B+ 树的由来，内容看起来比较零散。为了方便你掌握和记忆，我这里再总结一下 B+ 树的特点：</p>
<p>每个节点中子节点的个数不能超过 m，也不能小于 m/2；</p>
<p>根节点的子节点个数可以不超过 m/2，这是一个例外；</p>
<p>m 叉树只存储索引，并不真正存储数据，这个有点儿类似跳表；</p>
<p>通过链表将叶子节点串联在一起，这样可以方便按区间查找；</p>
<p>一般情况，根节点会被存储在内存中，其他节点存储在磁盘中。</p>
<p>除了 B+ 树，你可能还听说过 B 树、B- 树，我这里简单提一下。实际上，B- 树就是 B 树，英文翻译都是 B-Tree，这里的“-”并不是相对 B+ 树中的“+”，而只是一个连接符。这个很容易误解，所以我强调下。</p>
<p>而 B 树实际上是低级版的 B+ 树，或者说 B+ 树是 B 树的改进版。B 树跟 B+ 树的不同点主要集中在这几个地方：</p>
<p>B+ 树中的节点不存储数据，只是索引，而 B 树中的节点存储数据；</p>
<p>B 树中的叶子节点并不需要链表来串联。</p>
<p>也就是说，B 树只是一个每个节点的子节点个数不能小于 m/2 的 m 叉树。</p>
<h2 id="课后思考-5"><a href="#课后思考-5" class="headerlink" title="课后思考"></a>课后思考</h2><p>B+ 树中，将叶子节点串起来的链表，是单链表还是双向链表？为什么？</p>
<p>我们对平衡二叉查找树进行改造，将叶子节点串在链表中，就支持了按照区间来查找数据。我们在散列表（下）讲到，散列表也经常跟链表一块使用，如果我们把散列表中的结点，也用链表串起来，能否支持按照区间查找数据呢？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="49-搜索：如何用A-搜索算法实现游戏中的寻路功能？"><a href="#49-搜索：如何用A-搜索算法实现游戏中的寻路功能？" class="headerlink" title="49 | 搜索：如何用A*搜索算法实现游戏中的寻路功能？"></a>49 | 搜索：如何用A*搜索算法实现游戏中的寻路功能？</h1><p>王争 2019-01-18</p>
<p><img src="https://static001.geekbang.org/resource/image/05/b3/059c152aaa2cf0203a48bc6137052fb3.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:22</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：10.28M 时长：11:13</p>
<p>魔兽世界、仙剑奇侠传这类 MMRPG 游戏，不知道你有没有玩过？在这些游戏中，有一个非常重要的功能，那就是人物角色自动寻路。<strong>当人物处于游戏地图中的某个位置的时候，我们用鼠标点击另外一个相对较远的位置，人物就会自动地绕过障碍物走过去。玩过这么多游戏，不知你是否思考过，这个功能是怎么实现的呢？</strong></p>
<h2 id="算法解析-6"><a href="#算法解析-6" class="headerlink" title="算法解析"></a>算法解析</h2><p>实际上，这是一个非常典型的搜索问题。人物的起点就是他当下所在的位置，终点就是鼠标点击的位置。我们需要在地图中，找一条从起点到终点的路径。这条路径要绕过地图中所有障碍物，并且看起来要是一种非常聪明的走法。所谓“聪明”，笼统地解释就是，走的路不能太绕。理论上讲，最短路径显然是最聪明的走法，是这个问题的最优解。</p>
<p>不过，在第 44 节最优出行路线规划问题中，我们也讲过，如果图非常大，那 Dijkstra 最短路径算法的执行耗时会很多。在真实的软件开发中，我们面对的是超级大的地图和海量的寻路请求，算法的执行效率太低，这显然是无法接受的。</p>
<p>实际上，像出行路线规划、游戏寻路，这些真实软件开发中的问题，一般情况下，我们都不需要非得求最优解（也就是最短路径）。在权衡路线规划质量和执行效率的情况下，我们只需要寻求一个次优解就足够了。那<strong>如何快速找出一条接近于最短路线的次优路线呢？</strong></p>
<p>这个快速的路径规划算法，就是我们今天要学习的 <strong>A* 算法</strong>。实际上，A* 算法是对 Dijkstra 算法的优化和改造。如何将 Dijkstra 算法改造成 A* 算法呢？为了更好地理解接下来要讲的内容，我建议你先温习下第 44 节中的 Dijkstra 算法的实现原理。</p>
<p>Dijkstra 算法有点儿类似 BFS 算法，它每次找到跟起点最近的顶点，往外扩展。这种往外扩展的思路，其实有些盲目。为什么这么说呢？我举一个例子来给你解释一下。下面这个图对应一个真实的地图，每个顶点在地图中的位置，我们用一个二维坐标（x，y）来表示，其中，x 表示横坐标，y 表示纵坐标。</p>
<p><img src="https://static001.geekbang.org/resource/image/11/dd/11840cc13071fe2da67675338e46cadd.jpg" alt="img"></p>
<p>在 Dijkstra 算法的实现思路中，我们用一个优先级队列，来记录已经遍历到的顶点以及这个顶点与起点的路径长度。顶点与起点路径长度越小，就越先被从优先级队列中取出来扩展，从图中举的例子可以看出，尽管我们找的是从 s 到 t 的路线，但是最先被搜索到的顶点依次是 1，2，3。通过肉眼来观察，这个搜索方向跟我们期望的路线方向（s 到 t 是从西向东）是反着的，路线搜索的方向明显“跑偏”了。</p>
<p>之所以会“跑偏”，那是因为我们是按照顶点与起点的路径长度的大小，来安排出队列顺序的。与起点越近的顶点，就会越早出队列。我们并没有考虑到这个顶点到终点的距离，所以，在地图中，尽管 1，2，3 三个顶点离起始顶点最近，但离终点却越来越远。</p>
<p>如果我们综合更多的因素，把这个顶点到终点可能还要走多远，也考虑进去，综合来判断哪个顶点该先出队列，那是不是就可以避免“跑偏”呢？</p>
<p>当我们遍历到某个顶点的时候，从起点走到这个顶点的路径长度是确定的，我们记作 g(i)（i 表示顶点编号）。但是，从这个顶点到终点的路径长度，我们是未知的。虽然确切的值无法提前知道，但是我们可以用其他估计值来代替。</p>
<p>这里我们可以通过这个顶点跟终点之间的直线距离，也就是欧几里得距离，来近似地估计这个顶点跟终点的路径长度（注意：路径长度跟直线距离是两个概念）。我们把这个距离记作 h(i)（i 表示这个顶点的编号），专业的叫法是<strong>启发函数</strong>（heuristic function）。因为欧几里得距离的计算公式，会涉及比较耗时的开根号计算，所以，我们一般通过另外一个更加简单的距离计算公式，那就是<strong>曼哈顿距离</strong>（Manhattan distance）。曼哈顿距离是两点之间横纵坐标的距离之和。计算的过程只涉及加减法、符号位反转，所以比欧几里得距离更加高效。</p>
<p>int hManhattan(Vertex v1, Vertex v2) { // Vertex表示顶点，后面有定义</p>
<p>  return Math.abs(v1.x - v2.x) + Math.abs(v1.y - v2.y);</p>
<p>}</p>
<p>原来只是单纯地通过顶点与起点之间的路径长度 g(i)，来判断谁先出队列，现在有了顶点到终点的路径长度估计值，我们通过两者之和 f(i)=g(i)+h(i)，来判断哪个顶点该最先出队列。综合两部分，我们就能有效避免刚刚讲的“跑偏”。这里 f(i) 的专业叫法是<strong>估价函数</strong>（evaluation function）。</p>
<p>从刚刚的描述，我们可以发现，A* 算法就是对 Dijkstra 算法的简单改造。实际上，代码实现方面，我们也只需要稍微改动几行代码，就能把 Dijkstra 算法的代码实现，改成 A* 算法的代码实现。</p>
<p>在 A* 算法的代码实现中，顶点 Vertex 类的定义，跟 Dijkstra 算法中的定义，稍微有点儿区别，多了 x，y 坐标，以及刚刚提到的 f(i) 值。图 Graph 类的定义跟 Dijkstra 算法中的定义一样。为了避免重复，我这里就没有再贴出来了。</p>
<p>private class Vertex {</p>
<p>  public int id; // 顶点编号ID</p>
<p>  public int dist; // 从起始顶点，到这个顶点的距离，也就是g(i)</p>
<p>  public int f; // 新增：f(i)=g(i)+h(i)</p>
<p>  public int x, y; // 新增：顶点在地图中的坐标（x, y）</p>
<p>  public Vertex(int id, int x, int y) {</p>
<p>​    this.id = id;</p>
<p>​    this.x = x;</p>
<p>​    this.y = y;</p>
<p>​    this.f = Integer.MAX_VALUE;</p>
<p>​    this.dist = Integer.MAX_VALUE;</p>
<p>  }</p>
<p>}</p>
<p>// Graph类的成员变量，在构造函数中初始化</p>
<p>Vertex[] vertexes = new Vertex[this.v];</p>
<p>// 新增一个方法，添加顶点的坐标</p>
<p>public void addVetex(int id, int x, int y) {</p>
<p>  vertexes[id] = new Vertex(id, x, y)</p>
<p>}</p>
<p>A* 算法的代码实现的主要逻辑是下面这段代码。它跟 Dijkstra 算法的代码实现，主要有 3 点区别：</p>
<p>优先级队列构建的方式不同。A* 算法是根据 f 值（也就是刚刚讲到的 f(i)=g(i)+h(i)）来构建优先级队列，而 Dijkstra 算法是根据 dist 值（也就是刚刚讲到的 g(i)）来构建优先级队列；</p>
<p>A* 算法在更新顶点 dist 值的时候，会同步更新 f 值；</p>
<p>循环结束的条件也不一样。Dijkstra 算法是在终点出队列的时候才结束，A* 算法是一旦遍历到终点就结束。</p>
<p>public void astar(int s, int t) { // 从顶点s到顶点t的路径</p>
<p>  int[] predecessor = new int[this.v]; // 用来还原路径</p>
<p>  // 按照vertex的f值构建的小顶堆，而不是按照dist</p>
<p>  PriorityQueue queue = new PriorityQueue(this.v);</p>
<p>  boolean[] inqueue = new boolean[this.v]; // 标记是否进入过队列</p>
<p>  vertexes[s].dist = 0;</p>
<p>  vertexes[s].f = 0;</p>
<p>  queue.add(vertexes[s]);</p>
<p>  inqueue[s] = true;</p>
<p>  while (!queue.isEmpty()) {</p>
<p>​    Vertex minVertex = queue.poll(); // 取堆顶元素并删除</p>
<p>​    for (int i = 0; i &lt; adj[minVertex.id].size(); ++i) {</p>
<p>​      Edge e = adj[minVertex.id].get(i); // 取出一条minVetex相连的边</p>
<p>​      Vertex nextVertex = vertexes[e.tid]; // minVertex–&gt;nextVertex</p>
<p>​      if (minVertex.dist + e.w &lt; nextVertex.dist) { // 更新next的dist,f</p>
<p>​        nextVertex.dist = minVertex.dist + e.w;</p>
<p>​        nextVertex.f </p>
<p>​           = nextVertex.dist+hManhattan(nextVertex, vertexes[t]);</p>
<p>​        predecessor[nextVertex.id] = minVertex.id;</p>
<p>​        if (inqueue[nextVertex.id] == true) {</p>
<p>​          queue.update(nextVertex);</p>
<p>​        } else {</p>
<p>​          queue.add(nextVertex);</p>
<p>​          inqueue[nextVertex.id] = true;</p>
<p>​        }</p>
<p>​      }</p>
<p>​      if (nextVertex.id == t) { // 只要到达t就可以结束while了</p>
<p>​        queue.clear(); // 清空queue，才能推出while循环</p>
<p>​        break; </p>
<p>​      }</p>
<p>​    }</p>
<p>  }</p>
<p>  // 输出路径</p>
<p>  System.out.print(s);</p>
<p>  print(s, t, predecessor); // print函数请参看Dijkstra算法的实现</p>
<p>}</p>
<p><strong>尽管 A* 算法可以更加快速地找到从起点到终点的路线，但是它并不能像 Dijkstra 算法那样，找到最短路线。这是为什么呢？</strong></p>
<p>要找出起点 s 到终点 t 的最短路径，最简单的方法是，通过回溯穷举所有从 s 到达 t 的不同路径，然后对比找出最短的那个。不过很显然，回溯算法的执行效率非常低，是指数级的。</p>
<p><img src="https://static001.geekbang.org/resource/image/38/4a/38ebd9aab387669465226fc7f644064a.jpg" alt="img"></p>
<p>Dijkstra 算法在此基础之上，利用动态规划的思想，对回溯搜索进行了剪枝，只保留起点到某个顶点的最短路径，继续往外扩展搜索。动态规划相较于回溯搜索，只是换了一个实现思路，但它实际上也考察到了所有从起点到终点的路线，所以才能得到最优解。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83hwf25mj30vq0jfwg8.jpg" alt="img"></p>
<p>A* 算法之所以不能像 Dijkstra 算法那样，找到最短路径，主要原因是两者的 while 循环结束条件不一样。刚刚我们讲过，Dijkstra 算法是在终点出队列的时候才结束，A* 算法是一旦遍历到终点就结束。对于 Dijkstra 算法来说，当终点出队列的时候，终点的 dist 值是优先级队列中所有顶点的最小值，即便再运行下去，终点的 dist 值也不会再被更新了。对于 A* 算法来说，一旦遍历到终点，我们就结束 while 循环，这个时候，终点的 dist 值未必是最小值。</p>
<p>A* 算法利用贪心算法的思路，每次都找 f 值最小的顶点出队列，一旦搜索到终点就不在继续考察其他顶点和路线了。所以，它并没有考察所有的路线，也就不可能找出最短路径了。</p>
<p>搞懂了 A* 算法，我们再来看下，<strong>如何借助 A* 算法解决今天的游戏寻路问题？</strong></p>
<p>要利用 A* 算法解决这个问题，我们只需要把地图，抽象成图就可以了。不过，游戏中的地图跟第 44 节中讲的我们平常用的地图是不一样的。因为游戏中的地图并不像我们现实生活中那样，存在规划非常清晰的道路，更多的是宽阔的荒野、草坪等。所以，我们没法利用 44 节中讲到的抽象方法，把岔路口抽象成顶点，把道路抽象成边。</p>
<p>实际上，我们可以换一种抽象的思路，把整个地图分割成一个一个的小方块。在某一个方块上的人物，只能往上下左右四个方向的方块上移动。我们可以把每个方块看作一个顶点。两个方块相邻，我们就在它们之间，连两条有向边，并且边的权值都是 1。所以，这个问题就转化成了，在一个有向有权图中，找某个顶点到另一个顶点的路径问题。将地图抽象成边权值为 1 的有向图之后，我们就可以套用 A* 算法，来实现游戏中人物的自动寻路功能了。</p>
<h2 id="总结引申-6"><a href="#总结引申-6" class="headerlink" title="总结引申"></a>总结引申</h2><p>我们今天讲的 A* 算法属于一种<strong>启发式搜索算法</strong>（Heuristically Search Algorithm）。实际上，启发式搜索算法并不仅仅只有 A* 算法，还有很多其他算法，比如 IDA* 算法、蚁群算法、遗传算法、模拟退火算法等。如果感兴趣，你可以自行研究下。</p>
<p>启发式搜索算法利用估价函数，避免“跑偏”，贪心地朝着最有可能到达终点的方向前进。这种算法找出的路线，并不是最短路线。但是，实际的软件开发中的路线规划问题，我们往往并不需要非得找最短路线。所以，鉴于启发式搜索算法能很好地平衡路线质量和执行效率，它在实际的软件开发中的应用更加广泛。实际上，在第 44 节中，我们讲到的地图 App 中的出行路线规划问题，也可以利用启发式搜索算法来实现。</p>
<h2 id="课后思考-6"><a href="#课后思考-6" class="headerlink" title="课后思考"></a>课后思考</h2><p>我们之前讲的“迷宫问题”是否可以借助 A* 算法来更快速地找到一个走出去的路线呢？如果可以，请具体讲讲该怎么来做；如果不可以，请说说原因。</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="50-索引：如何在海量数据中快速查找某个数据？"><a href="#50-索引：如何在海量数据中快速查找某个数据？" class="headerlink" title="50 | 索引：如何在海量数据中快速查找某个数据？"></a>50 | 索引：如何在海量数据中快速查找某个数据？</h1><p>王争 2019-01-21</p>
<p><img src="https://static001.geekbang.org/resource/image/2b/10/2bcfe50eece74bbeb969e253f30c4b10.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:10</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：9.82M 时长：10:42</p>
<p>在第 48 节中，我们讲了 MySQL 数据库索引的实现原理。MySQL 底层依赖的是 B+ 树这种数据结构。留言里有同学问我，那<strong>类似 Redis 这样的 Key-Value 数据库中的索引，又是怎么实现的呢？底层依赖的又是什么数据结构呢？</strong></p>
<p>今天，我就来讲一下索引这种常用的技术解决思路，底层往往会依赖哪些数据结构。同时，通过索引这个应用场景，我也带你回顾一下，之前我们学过的几种支持动态集合的数据结构。</p>
<h2 id="为什么需要索引？"><a href="#为什么需要索引？" class="headerlink" title="为什么需要索引？"></a>为什么需要索引？</h2><p>在实际的软件开发中，业务纷繁复杂，功能千变万化，但是，万变不离其宗。如果抛开这些业务和功能的外壳，其实它们的本质都可以抽象为“对数据的存储和计算”。对应到数据结构和算法中，那“存储”需要的就是数据结构，“计算”需要的就是算法。</p>
<p>对于存储的需求，功能上无外乎增删改查。这其实并不复杂。但是，一旦存储的数据很多，那性能就成了这些系统要关注的重点，特别是在一些跟存储相关的基础系统（比如 MySQL 数据库、分布式文件系统等）、中间件（比如消息中间件 RocketMQ 等）中。</p>
<p>“如何节省存储空间、如何提高数据增删改查的执行效率”，这样的问题就成了设计的重点。而这些系统的实现，都离不开一个东西，那就是<strong>索引</strong>。不夸张地说，索引设计得好坏，直接决定了这些系统是否优秀。</p>
<p>索引这个概念，非常好理解。你可以类比书籍的目录来理解。如果没有目录，我们想要查找某个知识点的时候，就要一页一页翻。通过目录，我们就可以快速定位相关知识点的页数，查找的速度也会有质的提高。</p>
<h2 id="索引的需求定义"><a href="#索引的需求定义" class="headerlink" title="索引的需求定义"></a>索引的需求定义</h2><p>索引的概念不难理解，我想你应该已经搞明白。接下来，我们就分析一下，在设计索引的过程中，需要考虑到的一些因素，换句话说就是，我们该如何定义清楚需求呢？</p>
<p>对于系统设计需求，我们一般可以从<strong>功能性需求</strong>和<strong>非功能性需求</strong>两方面来分析，这个我们之前也说过。因此，这个问题也不例外。</p>
<h3 id="1-功能性需求"><a href="#1-功能性需求" class="headerlink" title="1. 功能性需求"></a>1. 功能性需求</h3><p>对于功能性需求需要考虑的点，我把它们大致概括成下面这几点。</p>
<p><strong>数据是格式化数据还是非格式化数据</strong>？要构建索引的原始数据，类型有很多。我把它分为两类，一类是结构化数据，比如，MySQL 中的数据；另一类是非结构化数据，比如搜索引擎中网页。对于非结构化数据，我们一般需要做预处理，提取出查询关键词，对关键词构建索引。</p>
<p><strong>数据是静态数据还是动态数据</strong>？如果原始数据是一组静态数据，也就是说，不会有数据的增加、删除、更新操作，所以，我们在构建索引的时候，只需要考虑查询效率就可以了。这样，索引的构建就相对简单些。不过，大部分情况下，我们都是对动态数据构建索引，也就是说，我们不仅要考虑到索引的查询效率，在原始数据更新的同时，我们还需要动态地更新索引。支持动态数据集合的索引，设计起来相对也要更加复杂些。</p>
<p><strong>索引存储在内存还是硬盘</strong>？如果索引存储在内存中，那查询的速度肯定要比存储在磁盘中的高。但是，如果原始数据量很大的情况下，对应的索引可能也会很大。这个时候，因为内存有限，我们可能就不得不将索引存储在磁盘中了。实际上，还有第三种情况，那就是一部分存储在内存，一部分存储在磁盘，这样就可以兼顾内存消耗和查询效率。</p>
<p><strong>单值查找还是区间查找</strong>？所谓单值查找，也就是根据查询关键词等于某个值的数据。这种查询需求最常见。所谓区间查找，就是查找关键词处于某个区间值的所有数据。你可以类比 MySQL 数据库的查询需求，自己想象一下。实际上，不同的应用场景，查询的需求会多种多样。</p>
<p><strong>单关键词查找还是多关键词组合查找</strong>？比如，搜索引擎中构建的索引，既要支持一个关键词的查找，比如“数据结构”，也要支持组合关键词查找，比如“数据结构 AND 算法”。对于单关键词的查找，索引构建起来相对简单些。对于多关键词查询来说，要分多种情况。像 MySQL 这种结构化数据的查询需求，我们可以实现针对多个关键词的组合，建立索引；对于像搜索引擎这样的非结构数据的查询需求，我们可以针对单个关键词构建索引，然后通过集合操作，比如求并集、求交集等，计算出多个关键词组合的查询结果。</p>
<p>实际上，不同的场景，不同的原始数据，对于索引的需求也会千差万别。我这里只列举了一些比较有共性的需求。</p>
<h3 id="2-非功能性需求"><a href="#2-非功能性需求" class="headerlink" title="2. 非功能性需求"></a>2. 非功能性需求</h3><p>讲完了功能性需求，我们再来看，索引设计的非功能性需求。</p>
<p><strong>不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大</strong>。如果存储在内存中，索引对占用存储空间的限制就会非常苛刻。毕竟内存空间非常有限，一个中间件启动后就占用几个 GB 的内存，开发者显然是无法接受的。如果存储在硬盘中，那索引对占用存储空间的限制，稍微会放宽一些。但是，我们也不能掉以轻心。因为，有时候，索引对存储空间的消耗会超过原始数据。</p>
<p><strong>在考虑索引查询效率的同时，我们还要考虑索引的维护成本</strong>。索引的目的是提高查询效率，但是，基于动态数据集合构建的索引，我们还要考虑到，索引的维护成本。因为在原始数据动态增删改的同时，我们也需要动态地更新索引。而索引的更新势必会影响到增删改操作的性能。</p>
<h2 id="构建索引常用的数据结构有哪些？"><a href="#构建索引常用的数据结构有哪些？" class="headerlink" title="构建索引常用的数据结构有哪些？"></a>构建索引常用的数据结构有哪些？</h2><p>我刚刚从很宏观的角度，总结了在索引设计的过程中，需要考虑的一些共性因素。现在，我们就来看，对于不同需求的索引结构，底层一般使用哪种数据结构。</p>
<p>实际上，常用来构建索引的数据结构，就是我们之前讲过的几种支持动态数据集合的数据结构。比如，散列表、红黑树、跳表、B+ 树。除此之外，位图、布隆过滤器可以作为辅助索引，有序数组可以用来对静态数据构建索引。</p>
<p>我们知道，<strong>散列表</strong>增删改查操作的性能非常好，时间复杂度是 O(1)。一些键值数据库，比如 Redis、Memcache，就是使用散列表来构建索引的。这类索引，一般都构建在内存中。</p>
<p><strong>红黑树</strong>作为一种常用的平衡二叉查找树，数据插入、删除、查找的时间复杂度是 O(logn)，也非常适合用来构建内存索引。Ext 文件系统中，对磁盘块的索引，用的就是红黑树。</p>
<p><strong>B+ 树</strong>比起红黑树来说，更加适合构建存储在磁盘中的索引。B+ 树是一个多叉树，所以，对相同个数的数据构建索引，B+ 树的高度要低于红黑树。当借助索引查询数据的时候，读取 B+ 树索引，需要的磁盘 IO 次数会更少。所以，大部分关系型数据库的索引，比如 MySQL、Oracle，都是用 B+ 树来实现的。</p>
<p><strong>跳表</strong>也支持快速添加、删除、查找数据。而且，我们通过灵活调整索引结点个数和数据个数之间的比例，可以很好地平衡索引对内存的消耗及其查询效率。Redis 中的有序集合，就是用跳表来构建的。</p>
<p>除了散列表、红黑树、B+ 树、跳表之外，位图和布隆过滤器这两个数据结构，也可以用于索引中，辅助存储在磁盘中的索引，加速数据查找的效率。我们来看下，具体是怎么做的？</p>
<p>我们知道，<strong>布隆过滤器</strong>有一定的判错率。但是，我们可以规避它的短处，发挥它的长处。尽管对于判定存在的数据，有可能并不存在，但是对于判定不存在的数据，那肯定就不存在。而且，布隆过滤器还有一个更大的特点，那就是内存占用非常少。我们可以针对数据，构建一个布隆过滤器，并且存储在内存中。当要查询数据的时候，我们可以先通过布隆过滤器，判定是否存在。如果通过布隆过滤器判定数据不存在，那我们就没有必要读取磁盘中的索引了。对于数据不存在的情况，数据查询就更加快速了。</p>
<p>实际上，有序数组也可以被作为索引。如果数据是静态的，也就是不会有插入、删除、更新操作，那我们可以把数据的关键词（查询用的）抽取出来，组织成有序数组，然后利用二分查找算法来快速查找数据。</p>
<h2 id="总结引申-7"><a href="#总结引申-7" class="headerlink" title="总结引申"></a>总结引申</h2><p>今天这节算是一节总结课。我从索引这个非常常用的技术方案，给你展示了散列表、红黑树、跳表、位图、布隆过滤器、有序数组这些数据结构的应用场景。学习完这节课之后，不知道你对这些数据结构以及索引，有没有更加清晰的认识呢？</p>
<p>从这一节内容中，你应该可以看出，架构设计离不开数据结构和算法。要想成长为一个优秀的业务架构师、基础架构师，数据结构和算法的根基一定要打稳。因为，那些看似很惊艳的架构设计思路，实际上，都是来自最常用的数据结构和算法。</p>
<h2 id="课后思考-7"><a href="#课后思考-7" class="headerlink" title="课后思考"></a>课后思考</h2><p>你知道基础系统、中间件、开源软件等系统中，有哪些用到了索引吗？这些系统的索引是如何实现的呢？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="51-并行算法：如何利用并行处理提高算法的执行效率？"><a href="#51-并行算法：如何利用并行处理提高算法的执行效率？" class="headerlink" title="51 | 并行算法：如何利用并行处理提高算法的执行效率？"></a>51 | 并行算法：如何利用并行处理提高算法的执行效率？</h1><p>王争 2019-01-23</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83j1mu3aj30vq0hs0us.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:09</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：9.22M 时长：10:03</p>
<p>时间复杂度是衡量算法执行效率的一种标准。但是，时间复杂度并不能跟性能划等号。在真实的软件开发中，即便在不降低时间复杂度的情况下，也可以通过一些优化手段，提升代码的执行效率。毕竟，对于实际的软件开发来说，即便是像 10%、20% 这样微小的性能提升，也是非常可观的。</p>
<p>算法的目的就是为了提高代码执行的效率。那<strong>当算法无法再继续优化的情况下，我们该如何来进一步提高执行效率呢</strong>？我们今天就讲一种非常简单但又非常好用的优化方法，那就是并行计算。今天，我就通过几个例子，给你展示一下，<strong>如何借助并行计算的处理思想对算法进行改造？</strong></p>
<h2 id="并行排序"><a href="#并行排序" class="headerlink" title="并行排序"></a>并行排序</h2><p>假设我们要给大小为 8GB 的数据进行排序，并且，我们机器的内存可以一次性容纳这么多数据。对于排序来说，最常用的就是时间复杂度为 O(nlogn) 的三种排序算法，归并排序、快速排序、堆排序。从理论上讲，这个排序问题，已经很难再从算法层面优化了。而利用并行的处理思想，我们可以很轻松地将这个给 8GB 数据排序问题的执行效率提高很多倍。具体的实现思路有下面两种。</p>
<p><strong>第一种是对归并排序并行化处理</strong>。我们可以将这 8GB 的数据划分成 16 个小的数据集合，每个集合包含 500MB 的数据。我们用 16 个线程，并行地对这 16 个 500MB 的数据集合进行排序。这 16 个小集合分别排序完成之后，我们再将这 16 个有序集合合并。</p>
<p><strong>第二种是对快速排序并行化处理</strong>。我们通过扫描一遍数据，找到数据所处的范围区间。我们把这个区间从小到大划分成 16 个小区间。我们将 8GB 的数据划分到对应的区间中。针对这 16 个小区间的数据，我们启动 16 个线程，并行地进行排序。等到 16 个线程都执行结束之后，得到的数据就是有序数据了。</p>
<p>对比这两种处理思路，它们利用的都是分治的思想，对数据进行分片，然后并行处理。它们的区别在于，第一种处理思路是，先随意地对数据分片，排序之后再合并。第二种处理思路是，先对数据按照大小划分区间，然后再排序，排完序就不需要再处理了。这个跟归并和快排的区别如出一辙。</p>
<p>这里我还要多说几句，如果要排序的数据规模不是 8GB，而是 1TB，那问题的重点就不是算法的执行效率了，而是数据的读取效率。因为 1TB 的数据肯定是存在硬盘中，无法一次性读取到内存中，这样在排序的过程中，就会有频繁地磁盘数据的读取和写入。如何减少磁盘的 IO 操作，减少磁盘数据读取和写入的总量，就变成了优化的重点。不过这个不是我们这节要讨论的重点，你可以自己思考下。</p>
<h2 id="并行查找"><a href="#并行查找" class="headerlink" title="并行查找"></a>并行查找</h2><p>我们知道，散列表是一种非常适合快速查找的数据结构。</p>
<p>如果我们是给动态数据构建索引，在数据不断加入的时候，散列表的装载因子就会越来越大。为了保证散列表性能不下降，我们就需要对散列表进行动态扩容。对如此大的散列表进行动态扩容，一方面比较耗时，另一方面比较消耗内存。比如，我们给一个 2GB 大小的散列表进行扩容，扩展到原来的 1.5 倍，也就是 3GB 大小。这个时候，实际存储在散列表中的数据只有不到 2GB，所以内存的利用率只有 60%，有 1GB 的内存是空闲的。</p>
<p>实际上，我们可以将数据随机分割成 k 份（比如 16 份），每份中的数据只有原来的 1/k，然后我们针对这 k 个小数据集合分别构建散列表。这样，散列表的维护成本就变低了。当某个小散列表的装载因子过大的时候，我们可以单独对这个散列表进行扩容，而其他散列表不需要进行扩容。</p>
<p>还是刚才那个例子，假设现在有 2GB 的数据，我们放到 16 个散列表中，每个散列表中的数据大约是 150MB。当某个散列表需要扩容的时候，我们只需要额外增加 150*0.5=75MB 的内存（假设还是扩容到原来的 1.5 倍）。无论从扩容的执行效率还是内存的利用率上，这种多个小散列表的处理方法，都要比大散列表高效。</p>
<p>当我们要查找某个数据的时候，我们只需要通过 16 个线程，并行地在这 16 个散列表中查找数据。这样的查找性能，比起一个大散列表的做法，也并不会下降，反倒有可能提高。</p>
<p>当往散列表中添加数据的时候，我们可以选择将这个新数据放入装载因子最小的那个散列表中，这样也有助于减少散列冲突。</p>
<h2 id="并行字符串匹配"><a href="#并行字符串匹配" class="headerlink" title="并行字符串匹配"></a>并行字符串匹配</h2><p>我们前面学过，在文本中查找某个关键词这样一个功能，可以通过字符串匹配算法来实现。我们之前学过的字符串匹配算法有 KMP、BM、RK、BF 等。当在一个不是很长的文本中查找关键词的时候，这些字符串匹配算法中的任何一个，都可以表现得非常高效。但是，如果我们处理的是超级大的文本，那处理的时间可能就会变得很长，那有没有办法加快匹配速度呢？</p>
<p>我们可以把大的文本，分割成 k 个小文本。假设 k 是 16，我们就启动 16 个线程，并行地在这 16 个小文本中查找关键词，这样整个查找的性能就提高了 16 倍。16 倍效率的提升，从理论的角度来说并不多。但是，对于真实的软件开发来说，这显然是一个非常可观的优化。</p>
<p>不过，这里还有一个细节要处理，那就是原本包含在大文本中的关键词，被一分为二，分割到两个小文本中，这就会导致尽管大文本中包含这个关键词，但在这 16 个小文本中查找不到它。实际上，这个问题也不难解决，我们只需要针对这种特殊情况，做一些特殊处理就可以了。</p>
<p>我们假设关键词的长度是 m。我们在每个小文本的结尾和开始各取 m 个字符串。前一个小文本的末尾 m 个字符和后一个小文本的开头 m 个字符，组成一个长度是 2m 的字符串。我们再拿关键词，在这个长度为 2m 的字符串中再重新查找一遍，就可以补上刚才的漏洞了。</p>
<h2 id="并行搜索"><a href="#并行搜索" class="headerlink" title="并行搜索"></a>并行搜索</h2><p>前面我们学习过好几种搜索算法，它们分别是广度优先搜索、深度优先搜索、Dijkstra 最短路径算法、A* 启发式搜索算法。对于广度优先搜索算法，我们也可以将其改造成并行算法。</p>
<p>广度优先搜索是一种逐层搜索的搜索策略。基于当前这一层顶点，我们可以启动多个线程，并行地搜索下一层的顶点。在代码实现方面，原来广度优先搜索的代码实现，是通过一个队列来记录已经遍历到但还没有扩展的顶点。现在，经过改造之后的并行广度优先搜索算法，我们需要利用两个队列来完成扩展顶点的工作。</p>
<p>假设这两个队列分别是队列 A 和队列 B。多线程并行处理队列 A 中的顶点，并将扩展得到的顶点存储在队列 B 中。等队列 A 中的顶点都扩展完成之后，队列 A 被清空，我们再并行地扩展队列 B 中的顶点，并将扩展出来的顶点存储在队列 A。这样两个队列循环使用，就可以实现并行广度优先搜索算法。</p>
<h2 id="总结引申-8"><a href="#总结引申-8" class="headerlink" title="总结引申"></a>总结引申</h2><p>上一节，我们通过实际软件开发中的“索引”这一技术点，回顾了之前学过的一些支持动态数据集合的数据结构。今天，我们又通过“并行算法”这个话题，回顾了之前学过的一些算法。</p>
<p>今天的内容比较简单，没有太复杂的知识点。我通过一些例子，比如并行排序、查找、搜索、字符串匹配，给你展示了并行处理的实现思路，也就是对数据进行分片，对没有依赖关系的任务，并行地执行。</p>
<p>并行计算是一个工程上的实现思路，尽管跟算法关系不大，但是，在实际的软件开发中，它确实可以非常巧妙地提高程序的运行效率，是一种非常好用的性能优化手段。</p>
<p>特别是，当要处理的数据规模达到一定程度之后，我们无法通过继续优化算法，来提高执行效率 的时候，我们就需要在实现的思路上做文章，利用更多的硬件资源，来加快执行的效率。所以，在很多超大规模数据处理中，并行处理的思想，应用非常广泛，比如 MapReduce 实际上就是一种并行计算框架。</p>
<h2 id="课后思考-8"><a href="#课后思考-8" class="headerlink" title="课后思考"></a>课后思考</h2><p>假设我们有 n 个任务，为了提高执行的效率，我们希望能并行执行任务，但是各个任务之间又有一定的依赖关系，如何根据依赖关系找出可以并行执行的任务？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="-1"><a href="#-1" class="headerlink" title="======================================="></a>=======================================</h1><h1 id="实战篇-52-56（5讲）"><a href="#实战篇-52-56（5讲）" class="headerlink" title="实战篇 52-56（5讲）"></a>实战篇 52-56（5讲）</h1><h1 id="52-算法实战（一）：剖析Redis常用数据类型对应的数据结构"><a href="#52-算法实战（一）：剖析Redis常用数据类型对应的数据结构" class="headerlink" title="52 | 算法实战（一）：剖析Redis常用数据类型对应的数据结构"></a>52 | 算法实战（一）：剖析Redis常用数据类型对应的数据结构</h1><p>王争 2019-01-25</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83tygu57j30vq0hsgn2.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:19</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：11.29M 时长：12:19</p>
<p>到此为止，专栏前三部分我们全部讲完了。从今天开始，我们就正式进入实战篇的部分。这部分我主要通过一些开源项目、经典系统，真枪实弹地教你，如何将数据结构和算法应用到项目中。所以这部分的内容，更多的是知识点的回顾，相对于基础篇、高级篇的内容，其实这部分会更加容易看懂。</p>
<p>不过，我希望你不要只是看懂就完了。你要多举一反三地思考，自己接触过的开源项目、基础框架、中间件中，都用过哪些数据结构和算法。你也可以想一想，在自己做的项目中，有哪些可以用学过的数据结构和算法进一步优化。这样的学习效果才会更好。</p>
<p>好了，今天我就带你一块儿看下，<strong>经典数据库 Redis 中的常用数据类型，底层都是用哪种数据结构实现的？</strong></p>
<h2 id="Redis-数据库介绍"><a href="#Redis-数据库介绍" class="headerlink" title="Redis 数据库介绍"></a>Redis 数据库介绍</h2><p>Redis 是一种键值（Key-Value）数据库。相对于关系型数据库（比如 MySQL），Redis 也被叫作<strong>非关系型数据库</strong>。</p>
<p>像 MySQL 这样的关系型数据库，表的结构比较复杂，会包含很多字段，可以通过 SQL 语句，来实现非常复杂的查询需求。而 Redis 中只包含“键”和“值”两部分，只能通过“键”来查询“值”。正是因为这样简单的存储结构，也让 Redis 的读写效率非常高。</p>
<p>除此之外，Redis 主要是作为内存数据库来使用，也就是说，数据是存储在内存中的。尽管它经常被用作内存数据库，但是，它也支持将数据存储在硬盘中。这一点，我们后面会介绍。</p>
<p>Redis 中，键的数据类型是字符串，但是为了丰富数据存储的方式，方便开发者使用，值的数据类型有很多，常用的数据类型有这样几种，它们分别是字符串、列表、字典、集合、有序集合。</p>
<p>“字符串（string）”这种数据类型非常简单，对应到数据结构里，就是<strong>字符串</strong>。你应该非常熟悉，这里我就不多介绍了。我们着重看下，其他四种比较复杂点的数据类型，看看它们底层都依赖了哪些数据结构。</p>
<h2 id="列表（list）"><a href="#列表（list）" class="headerlink" title="列表（list）"></a>列表（list）</h2><p>我们先来看列表。列表这种数据类型支持存储一组数据。这种数据类型对应两种实现方法，一种是<strong>压缩列表</strong>（ziplist），另一种是双向循环链表。</p>
<p>当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面两个条件：</p>
<p>列表中保存的单个数据（有可能是字符串类型的）小于 64 字节；</p>
<p>列表中数据个数少于 512 个。</p>
<p>关于压缩列表，我这里稍微解释一下。它并不是基础数据结构，而是 Redis 自己设计的一种数据存储结构。它有点儿类似数组，通过一片连续的内存空间，来存储数据。不过，它跟数组不同的一点是，它允许存储的数据大小不同。具体的存储结构也非常简单，你可以看我下面画的这幅图。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83tytehrj30vq0al75q.jpg" alt="img"></p>
<p>现在，我们来看看，压缩列表中的“压缩”两个字该如何理解？</p>
<p>听到“压缩”两个字，直观的反应就是节省内存。之所以说这种存储结构节省内存，是相较于数组的存储思路而言的。我们知道，数组要求每个元素的大小相同，如果我们要存储不同长度的字符串，那我们就需要用最大长度的字符串大小作为元素的大小（假设是 20 个字节）。那当我们存储小于 20 个字节长度的字符串的时候，便会浪费部分存储空间。听起来有点儿拗口，我画个图解释一下。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83tzalluj30vq0bjwfv.jpg" alt="img"></p>
<p>压缩列表这种存储结构，一方面比较节省内存，另一方面可以支持不同类型数据的存储。而且，因为数据存储在一片连续的内存空间，通过键来获取值为列表类型的数据，读取的效率也非常高。</p>
<p>当列表中存储的数据量比较大的时候，也就是不能同时满足刚刚讲的两个条件的时候，列表就要通过双向循环链表来实现了。</p>
<p>在链表里，我们已经讲过双向循环链表这种数据结构了，如果不记得了，你可以先回去复习一下。这里我们着重看一下 Redis 中双向链表的编码实现方式。</p>
<p>Redis 的这种双向链表的实现方式，非常值得借鉴。它额外定义一个 list 结构体，来组织链表的首、尾指针，还有长度等信息。这样，在使用的时候就会非常方便。</p>
<p>// 以下是C语言代码，因为Redis是用C语言实现的。</p>
<p>typedef struct listnode {</p>
<p>  struct listNode *prev;</p>
<p>  struct listNode *next;</p>
<p>  void *value;</p>
<p>} listNode;</p>
<p>typedef struct list {</p>
<p>  listNode *head;</p>
<p>  listNode *tail;</p>
<p>  unsigned long len;</p>
<p>  // ….省略其他定义</p>
<p>} list;</p>
<h2 id="字典（hash）"><a href="#字典（hash）" class="headerlink" title="字典（hash）"></a>字典（hash）</h2><p>字典类型用来存储一组数据对。每个数据对又包含键值两部分。字典类型也有两种实现方式。一种是我们刚刚讲到的<strong>压缩列表</strong>，另一种是<strong>散列表</strong>。</p>
<p>同样，只有当存储的数据量比较小的情况下，Redis 才使用压缩列表来实现字典类型。具体需要满足两个条件：</p>
<p>字典中保存的键和值的大小都要小于 64 字节；</p>
<p>字典中键值对的个数要小于 512 个。</p>
<p>当不能同时满足上面两个条件的时候，Redis 就使用散列表来实现字典类型。Redis 使用MurmurHash2这种运行速度快、随机性好的哈希算法作为哈希函数。对于哈希冲突问题，Redis 使用链表法来解决。除此之外，Redis 还支持散列表的动态扩容、缩容。</p>
<p>当数据动态增加之后，散列表的装载因子会不停地变大。为了避免散列表性能的下降，当装载因子大于 1 的时候，Redis 会触发扩容，将散列表扩大为原来大小的 2 倍左右（具体值需要计算才能得到，如果感兴趣，你可以去阅读源码）。</p>
<p>当数据动态减少之后，为了节省内存，当装载因子小于 0.1 的时候，Redis 就会触发缩容，缩小为字典中数据个数的大约 2 倍大小（这个值也是计算得到的，如果感兴趣，你也可以去阅读源码）。</p>
<p>我们前面讲过，扩容缩容要做大量的数据搬移和哈希值的重新计算，所以比较耗时。针对这个问题，Redis 使用我们在散列表（中）讲的渐进式扩容缩容策略，将数据的搬移分批进行，避免了大量数据一次性搬移导致的服务停顿。</p>
<h2 id="集合（set）"><a href="#集合（set）" class="headerlink" title="集合（set）"></a>集合（set）</h2><p>集合这种数据类型用来存储一组不重复的数据。这种数据类型也有两种实现方法，一种是基于有序数组，另一种是基于散列表。</p>
<p>当要存储的数据，同时满足下面这样两个条件的时候，Redis 就采用有序数组，来实现集合这种数据类型。</p>
<p>存储的数据都是整数；</p>
<p>存储的数据元素个数不超过 512 个。</p>
<p>当不能同时满足这两个条件的时候，Redis 就使用散列表来存储集合中的数据。</p>
<h2 id="有序集合（sortedset）"><a href="#有序集合（sortedset）" class="headerlink" title="有序集合（sortedset）"></a>有序集合（sortedset）</h2><p>有序集合这种数据类型，我们在跳表里已经详细讲过了。它用来存储一组数据，并且每个数据会附带一个得分。通过得分的大小，我们将数据组织成跳表这样的数据结构，以支持快速地按照得分值、得分区间获取数据。</p>
<p>实际上，跟 Redis 的其他数据类型一样，有序集合也并不仅仅只有跳表这一种实现方式。当数据量比较小的时候，Redis 会用压缩列表来实现有序集合。具体点说就是，使用压缩列表来实现有序集合的前提，有这样两个：</p>
<p>所有数据的大小都要小于 64 字节；</p>
<p>元素个数要小于 128 个。</p>
<h2 id="数据结构持久化"><a href="#数据结构持久化" class="headerlink" title="数据结构持久化"></a>数据结构持久化</h2><p>尽管 Redis 经常会被用作内存数据库，但是，它也支持数据落盘，也就是将内存中的数据存储到硬盘中。这样，当机器断电的时候，存储在 Redis 中的数据也不会丢失。在机器重新启动之后，Redis 只需要再将存储在硬盘中的数据，重新读取到内存，就可以继续工作了。</p>
<p>刚刚我们讲到，Redis 的数据格式由“键”和“值”两部分组成。而“值”又支持很多数据类型，比如字符串、列表、字典、集合、有序集合。像字典、集合等类型，底层用到了散列表，散列表中有指针的概念，而指针指向的是内存中的存储地址。 那 Redis 是如何将这样一个跟具体内存地址有关的数据结构存储到磁盘中的呢？</p>
<p>实际上，Redis 遇到的这个问题并不特殊，很多场景中都会遇到。我们把它叫作<strong>数据结构的持久化问题</strong>，或者<strong>对象的持久化问题</strong>。这里的“持久化”，你可以笼统地理解为“存储到磁盘”。</p>
<p>如何将数据结构持久化到硬盘？我们主要有两种解决思路。</p>
<p>第一种是清除原有的存储结构，只将数据存储到磁盘中。当我们需要从磁盘还原数据到内存的时候，再重新将数据组织成原来的数据结构。实际上，Redis 采用的就是这种持久化思路。</p>
<p>不过，这种方式也有一定的弊端。那就是数据从硬盘还原到内存的过程，会耗用比较多的时间。比如，我们现在要将散列表中的数据存储到磁盘。当我们从磁盘中，取出数据重新构建散列表的时候，需要重新计算每个数据的哈希值。如果磁盘中存储的是几 GB 的数据，那重构数据结构的耗时就不可忽视了。</p>
<p>第二种方式是保留原来的存储格式，将数据按照原有的格式存储在磁盘中。我们拿散列表这样的数据结构来举例。我们可以将散列表的大小、每个数据被散列到的槽的编号等信息，都保存在磁盘中。有了这些信息，我们从磁盘中将数据还原到内存中的时候，就可以避免重新计算哈希值。</p>
<h2 id="总结引申-9"><a href="#总结引申-9" class="headerlink" title="总结引申"></a>总结引申</h2><p>今天，我们学习了 Redis 中常用数据类型底层依赖的数据结构，总结一下大概有这五种：<strong>压缩列表</strong>（可以看作一种特殊的数组）、<strong>有序数组</strong>、<strong>链表</strong>、<strong>散列表</strong>、<strong>跳表</strong>。实际上，Redis 就是这些常用数据结构的封装。</p>
<p>你有没有发现，有了数据结构和算法的基础之后，再去阅读 Redis 的源码，理解起来就容易多了？很多原来觉得很深奥的设计思想，是不是就都会觉得顺理成章了呢？</p>
<p>还是那句话，夯实基础很重要。同样是看源码，有些人只能看个热闹，了解一些皮毛，无法形成自己的知识结构，不能化为己用，过不几天就忘了。而有些人基础很好，不但能知其然，还能知其所以然，从而真正理解作者设计的动机。这样不但能有助于我们理解所用的开源软件，还能为我们自己创新添砖加瓦。</p>
<h2 id="课后思考-9"><a href="#课后思考-9" class="headerlink" title="课后思考"></a>课后思考</h2><p>你有没有发现，在数据量比较小的情况下，Redis 中的很多数据类型，比如字典、有序集合等，都是通过多种数据结构来实现的，为什么会这样设计呢？用一种固定的数据结构来实现，不是更加简单吗？</p>
<p>我们讲到数据结构持久化有两种方法。对于二叉查找树这种数据结构，我们如何将它持久化到磁盘中呢？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="53-算法实战（二）：剖析搜索引擎背后的经典数据结构和算法"><a href="#53-算法实战（二）：剖析搜索引擎背后的经典数据结构和算法" class="headerlink" title="53 | 算法实战（二）：剖析搜索引擎背后的经典数据结构和算法"></a>53 | 算法实战（二）：剖析搜索引擎背后的经典数据结构和算法</h1><p>王争 2019-01-28</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83q7zhnvj30vq0hsmy3.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:00</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：19.22M 时长：20:59</p>
<p>像百度、Google 这样的搜索引擎，在我们平时的工作、生活中，几乎天天都会用到。如果我们把搜索引擎也当作一个互联网产品的话，那它跟社交、电商这些类型的产品相比，有一个非常大的区别，那就是，它是一个技术驱动的产品。所谓技术驱动是指，搜索引擎实现起来，技术难度非常大，技术的好坏直接决定了这个产品的核心竞争力。</p>
<p>在搜索引擎的设计与实现中，会用到大量的算法。有很多针对特定问题的算法，也有很多我们专栏中讲到的基础算法。所以，百度、Google 这样的搜索引擎公司，在面试的时候，会格外重视考察候选人的算法能力。</p>
<p><strong>今天我就借助搜索引擎，这样一个非常有技术含量的产品，来给你展示一下，数据结构和算法是如何应用在其中的。</strong></p>
<h2 id="整体系统介绍"><a href="#整体系统介绍" class="headerlink" title="整体系统介绍"></a>整体系统介绍</h2><p>像 Google 这样的大型商用搜索引擎，有成千上万的工程师，十年如一日地对它进行优化改进，所以，它所包含的技术细节非常多。我很难、也没有这个能力，通过一篇文章把所有细节都讲清楚，当然这也不是我们专栏所专注的内容。</p>
<p>所以，接下来的讲解，我主要给你展示，如何在一台机器上（假设这台机器的内存是 8GB， 硬盘是 100 多 GB），通过少量的代码，实现一个小型搜索引擎。不过，麻雀虽小，五脏俱全。跟大型搜索引擎相比，实现这样一个小型搜索引擎所用到的理论基础是相通的。</p>
<p>搜索引擎大致可以分为四个部分：<strong>搜集</strong>、<strong>分析</strong>、<strong>索引</strong>、<strong>查询</strong>。其中，搜集，就是我们常说的利用爬虫爬取网页。分析，主要负责网页内容抽取、分词，构建临时索引，计算 PageRank 值这几部分工作。索引，主要负责通过分析阶段得到的临时索引，构建倒排索引。查询，主要负责响应用户的请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果给用户。</p>
<p>接下来，我就按照网页处理的生命周期，从这四个阶段，依次来给你讲解，一个网页从被爬取到最终展示给用户，这样一个完整的过程。与此同时，我会穿插讲解，这个过程中需要用到哪些数据结构和算法。</p>
<h2 id="搜集"><a href="#搜集" class="headerlink" title="搜集"></a>搜集</h2><p>现在，互联网越来越发达，网站越来越多，对应的网页也就越来越多。对于搜索引擎来说，它事先并不知道网页都在哪里。打个比方来说就是，我们只知道海里面有很多鱼，但却并不知道鱼在哪里。那搜索引擎是如何爬取网页的呢？</p>
<p>搜索引擎把整个互联网看作数据结构中的有向图，把每个页面看作一个顶点。如果某个页面中包含另外一个页面的链接，那我们就在两个顶点之间连一条有向边。我们可以利用图的遍历搜索算法，来遍历整个互联网中的网页。</p>
<p>我们前面介绍过两种图的遍历方法，深度优先和广度优先。搜索引擎采用的是广度优先搜索策略。具体点讲的话，那就是，我们先找一些比较知名的网页（专业的叫法是权重比较高）的链接（比如新浪主页网址、腾讯主页网址等），作为种子网页链接，放入到队列中。爬虫按照广度优先的策略，不停地从队列中取出链接，然后去爬取对应的网页，解析出网页里包含的其他网页链接，再将解析出来的链接添加到队列中。</p>
<p>基本的原理就是这么简单。但落实到实现层面，还有很多技术细节。我下面借助搜集阶段涉及的几个重要文件，来给你解释一下搜集工程都有哪些关键技术细节。</p>
<h3 id="1-待爬取网页链接文件：links-bin"><a href="#1-待爬取网页链接文件：links-bin" class="headerlink" title="1. 待爬取网页链接文件：links.bin"></a>1. 待爬取网页链接文件：links.bin</h3><p>在广度优先搜索爬取页面的过程中，爬虫会不停地解析页面链接，将其放到队列中。于是，队列中的链接就会越来越多，可能会多到内存放不下。所以，我们用一个存储在磁盘中的文件（links.bin）来作为广度优先搜索中的队列。爬虫从 links.bin 文件中，取出链接去爬取对应的页面。等爬取到网页之后，将解析出来的链接，直接存储到 links.bin 文件中。</p>
<p>这样用文件来存储网页链接的方式，还有其他好处。比如，支持断点续爬。也就是说，当机器断电之后，网页链接不会丢失；当机器重启之后，还可以从之前爬取到的位置继续爬取。</p>
<p>关于如何解析页面获取链接，我额外多说几句。我们可以把整个页面看作一个大的字符串，然后利用字符串匹配算法，在这个大字符串中，搜索<link>这样一个网页标签，然后顺序读取<link></link>之间的字符串。这其实就是网页链接。</p>
<h3 id="2-网页判重文件：bloom-filter-bin"><a href="#2-网页判重文件：bloom-filter-bin" class="headerlink" title="2. 网页判重文件：bloom_filter.bin"></a>2. 网页判重文件：bloom_filter.bin</h3><p>如何避免重复爬取相同的网页呢？这个问题我们在位图那一节已经讲过了。使用布隆过滤器，我们就可以快速并且非常节省内存地实现网页的判重。</p>
<p>不过，还是刚刚那个问题，如果我们把布隆过滤器存储在内存中，那机器宕机重启之后，布隆过滤器就被清空了。这样就可能导致大量已经爬取的网页会被重复爬取。</p>
<p>这个问题该怎么解决呢？我们可以定期地（比如每隔半小时）将布隆过滤器持久化到磁盘中，存储在 bloom_filter.bin 文件中。这样，即便出现机器宕机，也只会丢失布隆过滤器中的部分数据。当机器重启之后，我们就可以重新读取磁盘中的 bloom_filter.bin 文件，将其恢复到内存中。</p>
<h3 id="3-原始网页存储文件：doc-raw-bin"><a href="#3-原始网页存储文件：doc-raw-bin" class="headerlink" title="3. 原始网页存储文件：doc_raw.bin"></a>3. 原始网页存储文件：doc_raw.bin</h3><p>爬取到网页之后，我们需要将其存储下来，以备后面离线分析、索引之用。那如何存储海量的原始网页数据呢？</p>
<p>如果我们把每个网页都存储为一个独立的文件，那磁盘中的文件就会非常多，数量可能会有几千万，甚至上亿。常用的文件系统显然不适合存储如此多的文件。所以，我们可以把多个网页存储在一个文件中。每个网页之间，通过一定的标识进行分隔，方便后续读取。具体的存储格式，如下图所示。其中，doc_id 这个字段是网页的编号，我们待会儿再解释。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83q5m3u0j30vq0hw3zl.jpg" alt="img"></p>
<p>当然，这样的一个文件也不能太大，因为文件系统对文件的大小也有一定的限制。所以，我们可以设置每个文件的大小不能超过一定的值（比如 1GB）。随着越来越多的网页被添加到文件中，文件的大小就会越来越大，当超过 1GB 的时候，我们就创建一个新的文件，用来存储新爬取的网页。</p>
<p>假设一台机器的硬盘大小是 100GB 左右，一个网页的平均大小是 64KB。那在一台机器上，我们可以存储 100 万到 200 万左右的网页。假设我们的机器的带宽是 10MB，那下载 100GB 的网页，大约需要 10000 秒。也就是说，爬取 100 多万的网页，也就是只需要花费几小时的时间。</p>
<h3 id="4-网页链接及其编号的对应文件：doc-id-bin"><a href="#4-网页链接及其编号的对应文件：doc-id-bin" class="headerlink" title="4. 网页链接及其编号的对应文件：doc_id.bin"></a>4. 网页链接及其编号的对应文件：doc_id.bin</h3><p>刚刚我们提到了网页编号这个概念，我现在解释一下。网页编号实际上就是给每个网页分配一个唯一的 ID，方便我们后续对网页进行分析、索引。那如何给网页编号呢？</p>
<p>我们可以按照网页被爬取的先后顺序，从小到大依次编号。具体是这样做的：我们维护一个中心的计数器，每爬取到一个网页之后，就从计数器中拿一个号码，分配给这个网页，然后计数器加一。在存储网页的同时，我们将网页链接跟编号之间的对应关系，存储在另一个 doc_id.bin 文件中。</p>
<p><strong>爬虫在爬取网页的过程中，涉及的四个重要的文件，我就介绍完了。其中，links.bin 和 bloom_filter.bin 这两个文件是爬虫自身所用的。另外的两个（doc_raw.bin、doc_id.bin）是作为搜集阶段的成果，供后面的分析、索引、查询用的。</strong></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>网页爬取下来之后，我们需要对网页进行离线分析。分析阶段主要包括两个步骤，第一个是抽取网页文本信息，第二个是分词并创建临时索引。我们逐一来讲解。</p>
<h3 id="1-抽取网页文本信息"><a href="#1-抽取网页文本信息" class="headerlink" title="1. 抽取网页文本信息"></a>1. 抽取网页文本信息</h3><p>网页是半结构化数据，里面夹杂着各种标签、JavaScript 代码、CSS 样式。对于搜索引擎来说，它只关心网页中的文本信息，也就是，网页显示在浏览器中时，能被用户肉眼看到的那部分信息。我们如何从半结构化的网页中，抽取出搜索引擎关系的文本信息呢？</p>
<p>我们之所以把网页叫作半结构化数据，是因为它本身是按照一定的规则来书写的。这个规则就是 <strong>HTML 语法规范</strong>。我们依靠 HTML 标签来抽取网页中的文本信息。这个抽取的过程，大体可以分为两步。</p>
<p>第一步是去掉 JavaScript 代码、CSS 格式以及下拉框中的内容（因为下拉框在用户不操作的情况下，也是看不到的）。也就是<style></style>，<script></script>，<option></option>这三组标签之间的内容。我们可以利用 AC 自动机这种多模式串匹配算法，在网页这个大字符串中，一次性查找<style>, <script>, <option>这三个关键词。当找到某个关键词出现的位置之后，我们只需要依次往后遍历，直到对应结束标签（</style>, </script>, &lt;/option）为止。而这期间遍历到的字符串连带着标签就应该从网页中删除。</p>
<p>第二步是去掉所有 HTML 标签。这一步也是通过字符串匹配算法来实现的。过程跟第一步类似，我就不重复讲了。</p>
<h3 id="2-分词并创建临时索引"><a href="#2-分词并创建临时索引" class="headerlink" title="2. 分词并创建临时索引"></a>2. 分词并创建临时索引</h3><p>经过上面的处理之后，我们就从网页中抽取出了我们关心的文本信息。接下来，我们要对文本信息进行分词，并且创建临时索引。</p>
<p>对于英文网页来说，分词非常简单。我们只需要通过空格、标点符号等分隔符，将每个单词分割开来就可以了。但是，对于中文来说，分词就复杂太多了。我这里介绍一种比较简单的思路，基于字典和规则的分词方法。</p>
<p>其中，字典也叫词库，里面包含大量常用的词语（我们可以直接从网上下载别人整理好的）。我们借助词库并采用最长匹配规则，来对文本进行分词。所谓最长匹配，也就是匹配尽可能长的词语。我举个例子解释一下。</p>
<p>比如要分词的文本是“中国人民解放了”，我们词库中有“中国”“中国人”“中国人民”“中国人民解放军”这几个词，那我们就取最长匹配，也就是“中国人民”划为一个词，而不是把“中国”、“中国人”划为一个词。具体到实现层面，我们可以将词库中的单词，构建成 Trie 树结构，然后拿网页文本在 Trie 树中匹配。</p>
<p>每个网页的文本信息在分词完成之后，我们都得到一组单词列表。我们把单词与网页之间的对应关系，写入到一个临时索引文件中（tmp_Index.bin），这个临时索引文件用来构建倒排索引文件。临时索引文件的格式如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83q9eu6uj30vq0gkwg5.jpg" alt="img"></p>
<p>在临时索引文件中，我们存储的是单词编号，也就是图中的 term_id，而非单词本身。这样做的目的主要是为了节省存储的空间。那这些单词的编号是怎么来的呢？</p>
<p>给单词编号的方式，跟给网页编号类似。我们维护一个计数器，每当从网页文本信息中分割出一个新的单词的时候，我们就从计数器中取一个编号，分配给它，然后计数器加一。</p>
<p>在这个过程中，我们还需要使用散列表，记录已经编过号的单词。在对网页文本信息分词的过程中，我们拿分割出来的单词，先到散列表中查找，如果找到，那就直接使用已有的编号；如果没有找到，我们再去计数器中拿号码，并且将这个新单词以及编号添加到散列表中。</p>
<p>当所有的网页处理（分词及写入临时索引）完成之后，我们再将这个单词跟编号之间的对应关系，写入到磁盘文件中，并命名为 term_id.bin。</p>
<p><strong>经过分析阶段，我们得到了两个重要的文件。它们分别是临时索引文件（tmp_index.bin）和单词编号文件（term_id.bin）。</strong></p>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>索引阶段主要负责将分析阶段产生的临时索引，构建成倒排索引。倒排索引（ Inverted index）中记录了每个单词以及包含它的网页列表。文字描述比较难理解，我画了一张倒排索引的结构图，你一看就明白。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qk5d5tj30vq0ih0v9.jpg" alt="img"></p>
<p>我们刚刚讲到，在临时索引文件中，记录的是单词跟每个包含它的文档之间的对应关系。那如何通过临时索引文件，构建出倒排索引文件呢？这是一个非常典型的算法问题，你可以先自己思考一下，再看我下面的讲解。</p>
<p>解决这个问题的方法有很多。考虑到临时索引文件很大，无法一次性加载到内存中，搜索引擎一般会选择使用<strong>多路归并排序</strong>的方法来实现。</p>
<p>我们先对临时索引文件，按照单词编号的大小进行排序。因为临时索引很大，所以一般基于内存的排序算法就没法处理这个问题了。我们可以用之前讲到的归并排序的处理思想，将其分割成多个小文件，先对每个小文件独立排序，最后再合并在一起。当然，实际的软件开发中，我们其实可以直接利用 MapReduce 来处理。</p>
<p>临时索引文件排序完成之后，相同的单词就被排列到了一起。我们只需要顺序地遍历排好序的临时索引文件，就能将每个单词对应的网页编号列表找出来，然后把它们存储在倒排索引文件中。具体的处理过程，我画成了一张图。通过图，你应该更容易理解。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qe6tugj30vq0jrgn1.jpg" alt="img"></p>
<p>除了倒排文件之外，我们还需要一个文件，来记录每个单词编号在倒排索引文件中的偏移位置。我们把这个文件命名为 term_offset.bin。这个文件的作用是，帮助我们快速地查找某个单词编号在倒排索引中存储的位置，进而快速地从倒排索引中读取单词编号对应的网页编号列表。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83q9vborj30vq0fdta7.jpg" alt="img"></p>
<p><strong>经过索引阶段的处理，我们得到了两个有价值的文件，它们分别是倒排索引文件（index.bin）和记录单词编号在索引文件中的偏移位置的文件（term_offset.bin）。</strong></p>
<h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><p>前面三个阶段的处理，只是为了最后的查询做铺垫。因此，现在我们就要利用之前产生的几个文件，来实现最终的用户搜索功能。</p>
<p>doc_id.bin：记录网页链接和编号之间的对应关系。</p>
<p>term_id.bin：记录单词和编号之间的对应关系。</p>
<p>index.bin：倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表。</p>
<p>term_offsert.bin：记录每个单词编号在倒排索引文件中的偏移位置。</p>
<p>这四个文件中，除了倒排索引文件（index.bin）比较大之外，其他的都比较小。为了方便快速查找数据，我们将其他三个文件都加载到内存中，并且组织成散列表这种数据结构。</p>
<p>当用户在搜索框中，输入某个查询文本的时候，我们先对用户输入的文本进行分词处理。假设分词之后，我们得到 k 个单词。</p>
<p>我们拿这 k 个单词，去 term_id.bin 对应的散列表中，查找对应的单词编号。经过这个查询之后，我们得到了这 k 个单词对应的单词编号。</p>
<p>我们拿这 k 个单词编号，去 term_offset.bin 对应的散列表中，查找每个单词编号在倒排索引文件中的偏移位置。经过这个查询之后，我们得到了 k 个偏移位置。</p>
<p>我们拿这 k 个偏移位置，去倒排索引（index.bin）中，查找 k 个单词对应的包含它的网页编号列表。经过这一步查询之后，我们得到了 k 个网页编号列表。</p>
<p>我们针对这 k 个网页编号列表，统计每个网页编号出现的次数。具体到实现层面，我们可以借助散列表来进行统计。统计得到的结果，我们按照出现次数的多少，从小到大排序。出现次数越多，说明包含越多的用户查询单词（用户输入的搜索文本，经过分词之后的单词）。</p>
<p>经过这一系列查询，我们就得到了一组排好序的网页编号。我们拿着网页编号，去 doc_id.bin 文件中查找对应的网页链接，分页显示给用户就可以了。</p>
<h2 id="总结引申-10"><a href="#总结引申-10" class="headerlink" title="总结引申"></a>总结引申</h2><p>今天，我给你展示了一个小型搜索引擎的设计思路。这只是一个搜索引擎设计的基本原理，有很多优化、细节我们并未涉及，比如计算网页权重的PageRank算法、计算查询结果排名的tf-idf模型等等。</p>
<p>在讲解的过程中，我们涉及的数据结构和算法有：图、散列表、Trie 树、布隆过滤器、单模式字符串匹配算法、AC 自动机、广度优先遍历、归并排序等。如果对其中哪些内容不清楚，你可以回到对应的章节进行复习。</p>
<p>最后，如果有时间的话，我强烈建议你，按照我的思路，自己写代码实现一个简单的搜索引擎。这样写出来的，即便只是一个 demo，但对于你深入理解数据结构和算法，也是很有帮助的。</p>
<h2 id="课后思考-10"><a href="#课后思考-10" class="headerlink" title="课后思考"></a>课后思考</h2><p>图的遍历方法有两种，深度优先和广度优先。我们讲到，搜索引擎中的爬虫是通过广度优先策略来爬取网页的。搜索引擎为什么选择广度优先策略，而不是深度优先策略呢？</p>
<p>大部分搜索引擎在结果显示的时候，都支持摘要信息和网页快照。实际上，你只需要对我今天讲的设计思路，稍加改造，就可以支持这两项功能。你知道如何改造吗？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="54-算法实战（三）：剖析高性能队列Disruptor背后的数据结构和算法"><a href="#54-算法实战（三）：剖析高性能队列Disruptor背后的数据结构和算法" class="headerlink" title="54 | 算法实战（三）：剖析高性能队列Disruptor背后的数据结构和算法"></a>54 | 算法实战（三）：剖析高性能队列Disruptor背后的数据结构和算法</h1><p>王争 2019-01-30</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qjrnw9j30vq0hsabq.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:00</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：10.67M 时长：11:38</p>
<p>Disruptor 你是否听说过呢？它是一种内存消息队列。从功能上讲，它其实有点儿类似 Kafka。不过，和 Kafka 不同的是，Disruptor 是线程之间用于消息传递的队列。它在 Apache Storm、Camel、Log4j 2 等很多知名项目中都有广泛应用。</p>
<p>之所以如此受青睐，主要还是因为它的性能表现非常优秀。它比 Java 中另外一个非常常用的内存消息队列 ArrayBlockingQueue（ABS）的性能，要高一个数量级，可以算得上是最快的内存消息队列了。它还因此获得过 Oracle 官方的 Duke 大奖。</p>
<p>如此高性能的内存消息队列，在设计和实现上，必然有它独到的地方。今天，我们就来一块儿看下，<strong>Disruptor 是如何做到如此高性能的？其底层依赖了哪些数据结构和算法？</strong></p>
<h2 id="基于循环队列的“生产者-消费者模型”"><a href="#基于循环队列的“生产者-消费者模型”" class="headerlink" title="基于循环队列的“生产者 - 消费者模型”"></a>基于循环队列的“生产者 - 消费者模型”</h2><p>什么是内存消息队列？对很多业务工程师或者前端工程师来说，可能会比较陌生。不过，如果我说“生产者 - 消费者模型”，估计大部分人都知道。在这个模型中，“生产者”生产数据，并且将数据放到一个中心存储容器中。之后，“消费者”从中心存储容器中，取出数据消费。</p>
<p>这个模型非常简单、好理解，那你有没有思考过，这里面存储数据的中心存储容器，是用什么样的数据结构来实现的呢？</p>
<p>实际上，实现中心存储容器最常用的一种数据结构，就是我们在第 9 节讲的队列。队列支持数据的先进先出。正是这个特性，使得数据被消费的顺序性可以得到保证，也就是说，早被生产的数据就会早被消费。</p>
<p>我们在第 9 节讲过，队列有两种实现思路。一种是基于链表实现的链式队列，另一种是基于数组实现的顺序队列。不同的需求背景下，我们会选择不同的实现方式。</p>
<p>如果我们要实现一个无界队列，也就是说，队列的大小事先不确定，理论上可以支持无限大。这种情况下，我们适合选用链表来实现队列。因为链表支持快速地动态扩容。如果我们要实现一个有界队列，也就是说，队列的大小事先确定，当队列中数据满了之后，生产者就需要等待。直到消费者消费了数据，队列有空闲位置的时候，生产者才能将数据放入。</p>
<p>实际上，相较于无界队列，有界队列的应用场景更加广泛。毕竟，我们的机器内存是有限的。而无界队列占用的内存数量是不可控的。对于实际的软件开发来说，这种不可控的因素，就会有潜在的风险。在某些极端情况下，无界队列就有可能因为内存持续增长，而导致 OOM（Out of Memory）错误。</p>
<p>在第 9 节中，我们还讲过一种特殊的顺序队列，循环队列。我们讲过，非循环的顺序队列在添加、删除数据的工程中，会涉及数据的搬移操作，导致性能变差。而循环队列正好可以解决这个数据搬移的问题，所以，性能更加好。所以，大部分用到顺序队列的场景中，我们都选择用顺序队列中的循环队列。</p>
<p>实际上，<strong>循环队列这种数据结构，就是我们今天要讲的内存消息队列的雏形。</strong>我借助循环队列，实现了一个最简单的“生产者 - 消费者模型”。对应的代码我贴到这里，你可以看看。</p>
<p>为了方便你理解，对于生产者和消费者之间操作的同步，我并没有用到线程相关的操作。而是采用了“当队列满了之后，生产者就轮训等待；当队列空了之后，消费者就轮训等待”这样的措施。</p>
<p>public class Queue {</p>
<p>  private Long[] data;</p>
<p>  private int size = 0, head = 0, tail = 0;</p>
<p>  public Queue(int size) {</p>
<p>​    this.data = new Long[size];</p>
<p>​    this.size = size;</p>
<p>  }</p>
<p>  public boolean add(Long element) {</p>
<p>​    if ((tail + 1) % size == head) return false;</p>
<p>​    data[tail] = element;</p>
<p>​    tail = (tail + 1) % size;</p>
<p>​    return true;</p>
<p>  }</p>
<p>  public Long poll() {</p>
<p>​    if (head == tail) return null;</p>
<p>​    long ret = data[head];</p>
<p>​    head = (head + 1) % size;</p>
<p>​    return ret;</p>
<p>  }</p>
<p>}</p>
<p>public class Producer {</p>
<p>  private Queue queue;</p>
<p>  public Producer(Queue queue) {</p>
<p>​    this.queue = queue;</p>
<p>  }</p>
<p>  public void produce(Long data) throws InterruptedException {</p>
<p>​    while (!queue.add(data)) {</p>
<p>​      Thread.sleep(100);</p>
<p>​    }</p>
<p>  }</p>
<p>}</p>
<p>public class Consumer {</p>
<p>  private Queue queue;</p>
<p>  public Consumer(Queue queue) {</p>
<p>​    this.queue = queue;</p>
<p>  }</p>
<p>  public void comsume() throws InterruptedException {</p>
<p>​    while (true) {</p>
<p>​      Long data = queue.poll();</p>
<p>​      if (data == null) {</p>
<p>​        Thread.sleep(100);</p>
<p>​      } else {</p>
<p>​        // TODO:…消费数据的业务逻辑…</p>
<p>​      }</p>
<p>​    }</p>
<p>  }</p>
<p>}</p>
<h2 id="基于加锁的并发“生产者-消费者模型”"><a href="#基于加锁的并发“生产者-消费者模型”" class="headerlink" title="基于加锁的并发“生产者 - 消费者模型”"></a>基于加锁的并发“生产者 - 消费者模型”</h2><p>实际上，刚刚的“生产者 - 消费者模型”实现代码，是不完善的。为什么这么说呢？</p>
<p>如果我们只有一个生产者往队列中写数据，一个消费者从队列中读取数据，那上面的代码是没有问题的。但是，如果有多个生产者在并发地往队列中写入数据，或者多个消费者并发地从队列中消费数据，那上面的代码就不能正确工作了。我来给你讲讲为什么。</p>
<p>在多个生产者或者多个消费者并发操作队列的情况下，刚刚的代码主要会有下面两个问题：</p>
<p>多个生产者写入的数据可能会互相覆盖；</p>
<p>多个消费者可能会读取重复的数据。</p>
<p>因为第一个问题和第二个问题产生的原理是类似的。所以，我着重讲解第一个问题是如何产生的以及该如何解决。对于第二个问题，你可以类比我对第一个问题的解决思路自己来想一想。</p>
<p>两个线程同时往队列中添加数据，也就相当于两个线程同时执行类 Queue 中的 add() 函数。我们假设队列的大小 size 是 10，当前的 tail 指向下标 7，head 指向下标 3，也就是说，队列中还有空闲空间。这个时候，线程 1 调用 add() 函数，往队列中添加一个值为 12 的数据；线程 2 调用 add() 函数，往队列中添加一个值为 15 的数据。在极端情况下，本来是往队列中添加了两个数据（12 和 15），最终可能只有一个数据添加成功，另一个数据会被覆盖。这是为什么呢？</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qkl4agj30vq0hzjtc.jpg" alt="img"></p>
<p>为了方便你查看队列 Queue 中的 add() 函数，我把它从上面的代码中摘录出来，贴在这里。</p>
<p>public boolean add(Long element) {</p>
<p>  if ((tail + 1) % size == head) return false;</p>
<p>  data[tail] = element;</p>
<p>  tail = (tail + 1) % size;</p>
<p>  return true;</p>
<p>}</p>
<p>从这段代码中，我们可以看到，第 3 行给 data[tail]赋值，然后第 4 行才给 tail 的值加一。赋值和 tail 加一两个操作，并非原子操作。这就会导致这样的情况发生：当线程 1 和线程 2 同时执行 add() 函数的时候，线程 1 先执行完了第 3 行语句，将 data[7]（tail 等于 7）的值设置为 12。在线程 1 还未执行到第 4 行语句之前，也就是还未将 tail 加一之前，线程 2 执行了第 3 行语句，又将 data[7]的值设置为 15，也就是说，那线程 2 插入的数据覆盖了线程 1 插入的数据。原本应该插入两个数据（12 和 15）的，现在只插入了一个数据（15）。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qdpg1qj30vq0i7djb.jpg" alt="img"></p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83ql2atzj30vq0iptc7.jpg" alt="img"></p>
<p>那如何解决这种线程并发往队列中添加数据时，导致的数据覆盖、运行不正确问题呢？</p>
<p>最简单的处理方法就是给这段代码加锁，同一时间只允许一个线程执行 add() 函数。这就相当于将这段代码的执行，由并行改成了串行，也就不存在我们刚刚说的问题了。</p>
<p>不过，天下没有免费的午餐，加锁将并行改成串行，必然导致多个生产者同时生产数据的时候，执行效率的下降。当然，我们可以继续优化代码，用CAS（compare and swap，比较并交换）操作等减少加锁的粒度，但是，这不是我们这节的重点。我们直接看 Disruptor 的处理方法。</p>
<h2 id="基于无锁的并发“生产者-消费者模型”"><a href="#基于无锁的并发“生产者-消费者模型”" class="headerlink" title="基于无锁的并发“生产者 - 消费者模型”"></a>基于无锁的并发“生产者 - 消费者模型”</h2><p>尽管 Disruptor 的源码读起来很复杂，但是基本思想其实非常简单。实际上，它是换了一种队列和“生产者 - 消费者模型”的实现思路。</p>
<p>之前的实现思路中，队列只支持两个操作，添加数据和读取并移除数据，分别对应代码中的 add() 函数和 poll() 函数，而 Disruptor 采用了另一种实现思路。</p>
<p>对于生产者来说，它往队列中添加数据之前，先申请可用空闲存储单元，并且是批量地申请连续的 n 个（n≥1）存储单元。当申请到这组连续的存储单元之后，后续往队列中添加元素，就可以不用加锁了，因为这组存储单元是这个线程独享的。不过，从刚刚的描述中，我们可以看出，申请存储单元的过程是需要加锁的。</p>
<p>对于消费者来说，处理的过程跟生产者是类似的。它先去申请一批连续可读的存储单元（这个申请的过程也是需要加锁的），当申请到这批存储单元之后，后续的读取操作就可以不用加锁了。</p>
<p>不过，还有一个需要特别注意的地方，那就是，如果生产者 A 申请到了一组连续的存储单元，假设是下标为 3 到 6 的存储单元，生产者 B 紧跟着申请到了下标是 7 到 9 的存储单元，那在 3 到 6 没有完全写入数据之前，7 到 9 的数据是无法读取的。这个也是 Disruptor 实现思路的一个弊端。</p>
<p>文字描述不好理解，我画了一个图，给你展示一下这个操作过程。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83q8frqkj30vq0ns0y1.jpg" alt="img"></p>
<p>实际上，Disruptor 采用的是 RingBuffer 和 AvailableBuffer 这两个结构，来实现我刚刚讲的功能。不过，因为我们主要聚焦在数据结构和算法上，所以我对这两种结构做了简化，但是基本思想是一致的。如果你对 Disruptor 感兴趣，可以去阅读一下它的源码。</p>
<h2 id="总结引申-11"><a href="#总结引申-11" class="headerlink" title="总结引申"></a>总结引申</h2><p>今天，我讲了如何实现一个高性能的并发队列。这里的“并发”两个字，实际上就是多线程安全的意思。</p>
<p>常见的内存队列往往采用循环队列来实现。这种实现方法，对于只有一个生产者和一个消费者的场景，已经足够了。但是，当存在多个生产者或者多个消费者的时候，单纯的循环队列的实现方式，就无法正确工作了。</p>
<p>这主要是因为，多个生产者在同时往队列中写入数据的时候，在某些情况下，会存在数据覆盖的问题。而多个消费者同时消费数据，在某些情况下，会存在消费重复数据的问题。</p>
<p>针对这个问题，最简单、暴力的解决方法就是，对写入和读取过程加锁。这种处理方法，相当于将原来可以并行执行的操作，强制串行执行，相应地就会导致操作性能的下降。</p>
<p>为了在保证逻辑正确的前提下，尽可能地提高队列在并发情况下的性能，Disruptor 采用了“两阶段写入”的方法。在写入数据之前，先加锁申请批量的空闲存储单元，之后往队列中写入数据的操作就不需要加锁了，写入的性能因此就提高了。Disruptor 对消费过程的改造，跟对生产过程的改造是类似的。它先加锁申请批量的可读取的存储单元，之后从队列中读取数据的操作也就不需要加锁了，读取的性能因此也就提高了。</p>
<p>你可能会觉得这个优化思路非常简单。实际上，不管架构设计还是产品设计，往往越简单的设计思路，越能更好地解决问题。正所谓“大道至简”，就是这个意思。</p>
<h2 id="课后思考-11"><a href="#课后思考-11" class="headerlink" title="课后思考"></a>课后思考</h2><p>为了提高存储性能，我们往往通过分库分表的方式设计数据库表。假设我们有 8 张表用来存储用户信息。这个时候，每张用户表中的 ID 字段就不能通过自增的方式来产生了。因为这样的话，就会导致不同表之间的用户 ID 值重复。</p>
<p>为了解决这个问题，我们需要实现一个 ID 生成器，可以为所有的用户表生成唯一的 ID 号。那现在问题是，如何设计一个高性能、支持并发的、能够生成全局唯一 ID 的 ID 生成器呢？</p>
<p>欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>
<h1 id="55-算法实战（四）：剖析微服务接口鉴权限流背后的数据结构和算法"><a href="#55-算法实战（四）：剖析微服务接口鉴权限流背后的数据结构和算法" class="headerlink" title="55 | 算法实战（四）：剖析微服务接口鉴权限流背后的数据结构和算法"></a>55 | 算法实战（四）：剖析微服务接口鉴权限流背后的数据结构和算法</h1><p>王争 2019-02-01</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qc78w3j30vq0hsdgu.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:00</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：13.42M 时长：14:38</p>
<p>微服务是最近几年才兴起的概念。简单点讲，就是把复杂的大应用，解耦拆分成几个小的应用。这样做的好处有很多。比如，这样有利于团队组织架构的拆分，毕竟团队越大协作的难度越大；再比如，每个应用都可以独立运维，独立扩容，独立上线，各个应用之间互不影响。不用像原来那样，一个小功能上线，整个大应用都要重新发布。</p>
<p>不过，有利就有弊。大应用拆分成微服务之后，服务之间的调用关系变得更复杂，平台的整体复杂熵升高，出错的概率、debug 问题的难度都高了好几个数量级。所以，为了解决这些问题，服务治理便成了微服务的一个技术重点。</p>
<p>所谓服务治理，简单点讲，就是管理微服务，保证平台整体正常、平稳地运行。服务治理涉及的内容比较多，比如鉴权、限流、降级、熔断、监控告警等等。这些服务治理功能的实现，底层依赖大量的数据结构和算法。今天，我就拿其中的鉴权和限流这两个功能，来带你看看，它们的实现过程中都要用到哪些数据结构和算法。</p>
<h2 id="鉴权背景介绍"><a href="#鉴权背景介绍" class="headerlink" title="鉴权背景介绍"></a>鉴权背景介绍</h2><p>以防你之前可能对微服务没有太多了解，所以我对鉴权的背景做了简化。</p>
<p>假设我们有一个微服务叫用户服务（User Service）。它提供很多用户相关的接口，比如获取用户信息、注册、登录等，给公司内部的其他应用使用。但是，并不是公司内部所有应用，都可以访问这个用户服务，也并不是每个有访问权限的应用，都可以访问用户服务的所有接口。</p>
<p>我举了一个例子给你讲解一下，你可以看我画的这幅图。这里面，只有 A、B、C、D 四个应用可以访问用户服务，并且，每个应用只能访问用户服务的部分接口。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qi5fahj30vq0j0dhg.jpg" alt="img"></p>
<p>要实现接口鉴权功能，我们需要事先将应用对接口的访问权限规则设置好。当某个应用访问其中一个接口的时候，我们就可以拿应用的请求 URL，在规则中进行匹配。如果匹配成功，就说明允许访问；如果没有可以匹配的规则，那就说明这个应用没有这个接口的访问权限，我们就拒绝服务。</p>
<h2 id="如何实现快速鉴权？"><a href="#如何实现快速鉴权？" class="headerlink" title="如何实现快速鉴权？"></a>如何实现快速鉴权？</h2><p>接口的格式有很多，有类似 Dubbo 这样的 RPC 接口，也有类似 Spring Cloud 这样的 HTTP 接口。不同接口的鉴权实现方式是类似的，我这里主要拿 HTTP 接口给你讲解。</p>
<p>鉴权的原理比较简单、好理解。那具体到实现层面，我们该用什么数据结构来存储规则呢？用户请求 URL 在规则中快速匹配，又该用什么样的算法呢？</p>
<p>实际上，不同的规则和匹配模式，对应的数据结构和匹配算法也是不一样的。所以，关于这个问题，我继续细化为三个更加详细的需求给你讲解。</p>
<h3 id="1-如何实现精确匹配规则？"><a href="#1-如何实现精确匹配规则？" class="headerlink" title="1. 如何实现精确匹配规则？"></a>1. 如何实现精确匹配规则？</h3><p>我们先来看最简单的一种匹配模式。只有当请求 URL 跟规则中配置的某个接口精确匹配时，这个请求才会被接受、处理。为了方便你理解，我举了一个例子，你可以看一下。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qabiztj30vq0nstbz.jpg" alt="img"></p>
<p>不同的应用对应不同的规则集合。我们可以采用散列表来存储这种对应关系。我这里着重讲下，每个应用对应的规则集合，该如何存储和匹配。</p>
<p>针对这种匹配模式，我们可以将每个应用对应的权限规则，存储在一个字符串数组中。当用户请求到来时，我们拿用户的请求 URL，在这个字符串数组中逐一匹配，匹配的算法就是我们之前学过的字符串匹配算法（比如 KMP、BM、BF 等）。</p>
<p>规则不会经常变动，所以，为了加快匹配速度，我们可以按照字符串的大小给规则排序，把它组织成有序数组这种数据结构。当要查找某个 URL 能否匹配其中某条规则的时候，我们可以采用二分查找算法，在有序数组中进行匹配。</p>
<p>而二分查找算法的时间复杂度是 O(logn)（n 表示规则的个数），这比起时间复杂度是 O(n) 的顺序遍历快了很多。对于规则中接口长度比较长，并且鉴权功能调用量非常大的情况，这种优化方法带来的性能提升还是非常可观的 。</p>
<h3 id="2-如何实现前缀匹配规则？"><a href="#2-如何实现前缀匹配规则？" class="headerlink" title="2. 如何实现前缀匹配规则？"></a>2. 如何实现前缀匹配规则？</h3><p>我们再来看一种稍微复杂的匹配模式。只要某条规则可以匹配请求 URL 的前缀，我们就说这条规则能够跟这个请求 URL 匹配。同样，为了方便你理解这种匹配模式，我还是举一个例子说明一下。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qej52aj30vq0kz0ws.jpg" alt="img"></p>
<p>不同的应用对应不同的规则集合。我们采用散列表来存储这种对应关系。我着重讲一下，每个应用的规则集合，最适合用什么样的数据结构来存储。</p>
<p>在Trie 树那节，我们讲到，Trie 树非常适合用来做前缀匹配。所以，针对这个需求，我们可以将每个用户的规则集合，组织成 Trie 树这种数据结构。</p>
<p>不过，Trie 树中的每个节点不是存储单个字符，而是存储接口被“/”分割之后的子目录（比如“/user/name”被分割为“user”“name”两个子目录）。因为规则并不会经常变动，所以，在 Trie 树中，我们可以把每个节点的子节点们，组织成有序数组这种数据结构。在匹配的过程中，我们可以利用二分查找算法，决定从一个节点应该跳到哪一个子节点。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83q8y6q9j30vq0ep0v8.jpg" alt="img"></p>
<h3 id="3-如何实现模糊匹配规则？"><a href="#3-如何实现模糊匹配规则？" class="headerlink" title="3. 如何实现模糊匹配规则？"></a>3. 如何实现模糊匹配规则？</h3><p>如果我们的规则更加复杂，规则中包含通配符，比如“*<em>”表示匹配任意多个子目录，“</em>”表示匹配任意一个子目录。只要用户请求 URL 可以跟某条规则模糊匹配，我们就说这条规则适用于这个请求。为了方便你理解，我举一个例子来解释一下。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qasmf0j30vq0l00vd.jpg" alt="img"></p>
<p>不同的应用对应不同的规则集合。我们还是采用散列表来存储这种对应关系。这点我们刚才讲过了，这里不再重复说了。我们着重看下，每个用户对应的规则集合，该用什么数据结构来存储？针对这种包含通配符的模糊匹配，我们又该使用什么算法来实现呢？</p>
<p>还记得我们在回溯算法那节讲的正则表达式的例子吗？我们可以借助正则表达式那个例子的解决思路，来解决这个问题。我们采用回溯算法，拿请求 URL 跟每条规则逐一进行模糊匹配。如何用回溯算法进行模糊匹配，这部分我就不重复讲了。你如果忘记了，可以回到相应章节复习一下。</p>
<p>不过，这个解决思路的时间复杂度是非常高的。我们需要拿每一个规则，跟请求 URL 匹配一遍。那有没有办法可以继续优化一下呢？</p>
<p>实际上，我们可以结合实际情况，挖掘出这样一个隐形的条件，那就是，并不是每条规则都包含通配符，包含通配符的只是少数。于是，我们可以把不包含通配符的规则和包含通配符的规则分开处理。</p>
<p>我们把不包含通配符的规则，组织成有序数组或者 Trie 树（具体组织成什么结构，视具体的需求而定，是精确匹配，就组织成有序数组，是前缀匹配，就组织成 Trie 树），而这一部分匹配就会非常高效。剩下的是少数包含通配符的规则，我们只要把它们简单存储在一个数组中就可以了。尽管匹配起来会比较慢，但是毕竟这种规则比较少，所以这种方法也是可以接受的。</p>
<p>当接收到一个请求 URL 之后，我们可以先在不包含通配符的有序数组或者 Trie 树中查找。如果能够匹配，就不需要继续在通配符规则中匹配了；如果不能匹配，就继续在通配符规则中查找匹配。</p>
<h2 id="限流背景介绍"><a href="#限流背景介绍" class="headerlink" title="限流背景介绍"></a>限流背景介绍</h2><p>讲完了鉴权的实现思路，我们再来看一下限流。</p>
<p>所谓限流，顾名思义，就是对接口调用的频率进行限制。比如每秒钟不能超过 100 次调用，超过之后，我们就拒绝服务。限流的原理听起来非常简单，但它在很多场景中，发挥着重要的作用。比如在秒杀、大促、双 11、618 等场景中，限流已经成为了保证系统平稳运行的一种标配的技术解决方案。</p>
<p>按照不同的限流粒度，限流可以分为很多种类型。比如给每个接口限制不同的访问频率，或者给所有接口限制总的访问频率，又或者更细粒度地限制某个应用对某个接口的访问频率等等。</p>
<p>不同粒度的限流功能的实现思路都差不多，所以，我今天主要针对限制所有接口总的访问频率这样一个限流需求来讲解。其他粒度限流需求的实现思路，你可以自己思考。</p>
<h2 id="如何实现精准限流？"><a href="#如何实现精准限流？" class="headerlink" title="如何实现精准限流？"></a>如何实现精准限流？</h2><p>最简单的限流算法叫<strong>固定时间窗口限流算法</strong>。这种算法是如何工作的呢？首先我们需要选定一个时间起点，之后每当有接口请求到来，我们就将计数器加一。如果在当前时间窗口内，根据限流规则（比如每秒钟最大允许 100 次访问请求），出现累加访问次数超过限流值的情况时，我们就拒绝后续的访问请求。当进入下一个时间窗口之后，计数器就清零重新计数。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qf6j0xj30vq0l7go3.jpg" alt="img"></p>
<p>这种基于固定时间窗口的限流算法的缺点是，限流策略过于粗略，无法应对两个时间窗口临界时间内的突发流量。这是怎么回事呢？我举一个例子给你解释一下。</p>
<p>假设我们的限流规则是，每秒钟不能超过 100 次接口请求。第一个 1s 时间窗口内，100 次接口请求都集中在最后 10ms 内。在第二个 1s 的时间窗口内，100 次接口请求都集中在最开始的 10ms 内。虽然两个时间窗口内流量都符合限流要求（≤100 个请求），但在两个时间窗口临界的 20ms 内，会集中有 200 次接口请求。固定时间窗口限流算法并不能对这种情况做限制，所以，集中在这 20ms 内的 200 次请求就有可能压垮系统。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qbt181j30vq0ey75q.jpg" alt="img"></p>
<p>为了解决这个问题，我们可以对固定时间窗口限流算法稍加改造。我们可以限制任意时间窗口（比如 1s）内，接口请求数都不能超过某个阈值（ 比如 100 次）。因此，相对于固定时间窗口限流算法，这个算法叫<strong>滑动时间窗口限流算法</strong>。</p>
<p>流量经过滑动时间窗口限流算法整形之后，可以保证任意一个 1s 的时间窗口内，都不会超过最大允许的限流值，从流量曲线上来看会更加平滑。那具体到实现层面，我们该如何来做呢？</p>
<p>我们假设限流的规则是，在任意 1s 内，接口的请求次数都不能大于 K 次。我们就维护一个大小为 K+1 的循环队列，用来记录 1s 内到来的请求。注意，这里循环队列的大小等于限流次数加一，因为循环队列存储数据时会浪费一个存储单元。</p>
<p>当有新的请求到来时，我们将与这个新请求的时间间隔超过 1s 的请求，从队列中删除。然后，我们再来看循环队列中是否有空闲位置。如果有，则把新请求存储在队列尾部（tail 指针所指的位置）；如果没有，则说明这 1 秒内的请求次数已经超过了限流值 K，所以这个请求被拒绝服务。</p>
<p>为了方便你理解，我举一个例子，给你解释一下。在这个例子中，我们假设限流的规则是，任意 1s 内，接口的请求次数都不能大于 6 次。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qlmnykj30vq0nsq7m.jpg" alt="img"></p>
<p>即便滑动时间窗口限流算法可以保证任意时间窗口内，接口请求次数都不会超过最大限流值，但是仍然不能防止，在细时间粒度上访问过于集中的问题。</p>
<p>比如我刚刚举的那个例子，第一个 1s 的时间窗口内，100 次请求都集中在最后 10ms 中，也就是说，基于时间窗口的限流算法，不管是固定时间窗口还是滑动时间窗口，只能在选定的时间粒度上限流，对选定时间粒度内的更加细粒度的访问频率不做限制。</p>
<p>实际上，针对这个问题，还有很多更加平滑的限流算法，比如令牌桶算法、漏桶算法等。如果感兴趣，你可以自己去研究一下。</p>
<h2 id="总结引申-12"><a href="#总结引申-12" class="headerlink" title="总结引申"></a>总结引申</h2><p>今天，我们讲解了跟微服务相关的接口鉴权和限流功能的实现思路。现在，我稍微总结一下。</p>
<p>关于鉴权，我们讲了三种不同的规则匹配模式。不管是哪种匹配模式，我们都可以用散列表来存储不同应用对应的不同规则集合。对于每个应用的规则集合的存储，三种匹配模式使用不同的数据结构。</p>
<p>对于第一种精确匹配模式，我们利用有序数组来存储每个应用的规则集合，并且通过二分查找和字符串匹配算法，来匹配请求 URL 与规则。对于第二种前缀匹配模式，我们利用 Trie 树来存储每个应用的规则集合。对于第三种模糊匹配模式，我们采用普通的数组来存储包含通配符的规则，通过回溯算法，来进行请求 URL 与规则的匹配。</p>
<p>关于限流，我们讲了两种限流算法，第一种是固定时间窗口限流算法，第二种是滑动时间窗口限流算法。对于滑动时间窗口限流算法，我们用了之前学习过的循环队列来实现。比起固定时间窗口限流算法，它对流量的整形效果更好，流量更加平滑。</p>
<p>从今天的学习中，我们也可以看出，对于基础架构工程师来说，如果不精通数据结构和算法，我们就很难开发出性能卓越的基础架构、中间件。这其实就体现了数据结构和算法的重要性。</p>
<h2 id="课后思考-12"><a href="#课后思考-12" class="headerlink" title="课后思考"></a>课后思考</h2><p>除了用循环队列来实现滑动时间窗口限流算法之外，我们是否还可以用其他数据结构来实现呢？请对比一下这些数据结构跟循环队列在解决这个问题时的优劣之处。</p>
<p>分析一下鉴权那部分内容中，前缀匹配算法的时间复杂度和空间复杂度。</p>
<p>最后，有个消息提前通知你一下。本节是专栏的倒数第二节课了，不知道学到现在，你掌握得怎么样呢？为了帮你复习巩固，做到真正掌握这些知识，我针对专栏涉及的数据结构和算法，精心编制了一套练习题。从正月初一到初七，每天发布一篇。你要做好准备哦！</p>
<h1 id="56-算法实战（五）：如何用学过的数据结构和算法实现一个短网址系统？"><a href="#56-算法实战（五）：如何用学过的数据结构和算法实现一个短网址系统？" class="headerlink" title="56 | 算法实战（五）：如何用学过的数据结构和算法实现一个短网址系统？"></a>56 | 算法实战（五）：如何用学过的数据结构和算法实现一个短网址系统？</h1><p>王争 2019-02-04</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qcvbalj30vq0hst9l.jpg" alt="img"></p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAABCAYAAACVOl3IAAAAJUlEQVQYV2N89+7df0FBQQYQeP/+PZgGAUJilMpjs4dSM5H1AwBwSSrLZGasMwAAAABJRU5ErkJggg==" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAAHUlEQVQYV2N89+7dfwYoEBQUBLPev3/PgItNSC0ADtASy+rFJtUAAAAASUVORK5CYII=" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAABCAYAAACPBr1GAAAAJ0lEQVQYV2N89+7dfwYoEBQUBLPev3/PQAkbm3mE7KBUHpd7YeYCAE50MMsBeqQ5AAAAAElFTkSuQmCC" alt="img"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAABCAYAAAAB3PQ6AAAAHUlEQVQYV2N89+7df0FBQQYQeP/+PQMxbLBiIAAAQE0Py6Kq0rkAAAAASUVORK5CYII=" alt="img"></p>
<p></p>
<p>00:00</p>
<p><a href="javascript:;">1.25x<em></em></a></p>
<p>讲述：冯永吉 大小：13.98M 时长：15:15</p>
<p>短网址服务你用过吗？如果我们在微博里发布一条带网址的信息，微博会把里面的网址转化成一个更短的网址。我们只要访问这个短网址，就相当于访问原始的网址。比如下面这两个网址，尽管长度不同，但是都可以跳转到我的一个 GitHub 开源项目里。其中，第二个网址就是通过新浪提供的短网址服务生成的。</p>
<p>原始网址：<a target="_blank" rel="noopener" href="https://github.com/wangzheng0822/ratelimiter4j">https://github.com/wangzheng0822/ratelimiter4j</a></p>
<p>短网址：<a target="_blank" rel="noopener" href="http://t.cn/EtR9QEG">http://t.cn/EtR9QEG</a></p>
<p>从功能上讲，短网址服务其实非常简单，就是把一个长的网址转化成一个短的网址。作为一名软件工程师，你是否思考过，这样一个简单的功能，是如何实现的呢？底层都依赖了哪些数据结构和算法呢？</p>
<h2 id="短网址服务整体介绍"><a href="#短网址服务整体介绍" class="headerlink" title="短网址服务整体介绍"></a>短网址服务整体介绍</h2><p>刚刚我们讲了，短网址服务的一个核心功能，就是把原始的长网址转化成短网址。除了这个功能之外，短网址服务还有另外一个必不可少的功能。那就是，当用户点击短网址的时候，短网址服务会将浏览器重定向为原始网址。这个过程是如何实现的呢？</p>
<p>为了方便你理解，我画了一张对比图，你可以看下。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83q77pb9j30vq0ktq4f.jpg" alt="img"></p>
<p>从图中我们可以看出，浏览器会先访问短网址服务，通过短网址获取到原始网址，再通过原始网址访问到页面。不过这部分功能并不是我们今天要讲的重点。我们重点来看，如何将长网址转化成短网址？</p>
<h2 id="如何通过哈希算法生成短网址？"><a href="#如何通过哈希算法生成短网址？" class="headerlink" title="如何通过哈希算法生成短网址？"></a>如何通过哈希算法生成短网址？</h2><p>我们前面学过哈希算法。哈希算法可以将一个不管多长的字符串，转化成一个长度固定的哈希值。我们可以利用哈希算法，来生成短网址。</p>
<p>前面我们已经提过一些哈希算法了，比如 MD5、SHA 等。但是，实际上，我们并不需要这些复杂的哈希算法。在生成短网址这个问题上，毕竟，我们不需要考虑反向解密的难度，所以我们只需要关心哈希算法的计算速度和冲突概率。</p>
<p>能够满足这样要求的哈希算法有很多，其中比较著名并且应用广泛的一个哈希算法，那就是MurmurHash 算法。尽管这个哈希算法在 2008 年才被发明出来，但现在它已经广泛应用到 Redis、MemCache、Cassandra、HBase、Lucene 等众多著名的软件中。</p>
<p>MurmurHash 算法提供了两种长度的哈希值，一种是 32bits，一种是 128bits。为了让最终生成的短网址尽可能短，我们可以选择 32bits 的哈希值。对于开头那个 GitHub 网址，经过 MurmurHash 计算后，得到的哈希值就是 181338494。我们再拼上短网址服务的域名，就变成了最终的短网址 <a target="_blank" rel="noopener" href="http://t.cn/181338494%EF%BC%88%E5%85%B6%E4%B8%AD%EF%BC%8Chttp://t.cn">http://t.cn/181338494（其中，http://t.cn</a> 是短网址服务的域名）。</p>
<h3 id="1-如何让短网址更短？"><a href="#1-如何让短网址更短？" class="headerlink" title="1. 如何让短网址更短？"></a>1. 如何让短网址更短？</h3><p>不过，你可能已经看出来了，通过 MurmurHash 算法得到的短网址还是很长啊，而且跟我们开头那个网址的格式好像也不一样。别着急，我们只需要稍微改变一个哈希值的表示方法，就可以轻松把短网址变得更短些。</p>
<p>我们可以将 10 进制的哈希值，转化成更高进制的哈希值，这样哈希值就变短了。我们知道，16 进制中，我们用 A～F，来表示 10～15。在网址 URL 中，常用的合法字符有 0～9、a～z、A～Z 这样 62 个字符。为了让哈希值表示起来尽可能短，我们可以将 10 进制的哈希值转化成 62 进制。具体的计算过程，我写在这里了。最终用 62 进制表示的短网址就是<a target="_blank" rel="noopener" href="http://t.cn/cgSqq%E3%80%82">http://t.cn/cgSqq。</a></p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qffvjtj30vq0k43zx.jpg" alt="img"></p>
<h3 id="2-如何解决哈希冲突问题？"><a href="#2-如何解决哈希冲突问题？" class="headerlink" title="2. 如何解决哈希冲突问题？"></a>2. 如何解决哈希冲突问题？</h3><p>不过，我们前面讲过，哈希算法无法避免的一个问题，就是哈希冲突。尽管 MurmurHash 算法，冲突的概率非常低。但是，一旦冲突，就会导致两个原始网址被转化成同一个短网址。当用户访问短网址的时候，我们就无从判断，用户想要访问的是哪一个原始网址了。这个问题该如何解决呢？</p>
<p>一般情况下，我们会保存短网址跟原始网址之间的对应关系，以便后续用户在访问短网址的时候，可以根据对应关系，查找到原始网址。存储这种对应关系的方式有很多，比如我们自己设计存储系统或者利用现成的数据库。前面我们讲到的数据库有 MySQL、Redis。我们就拿 MySQL 来举例。假设短网址与原始网址之间的对应关系，就存储在 MySQL 数据库中。</p>
<p>当有一个新的原始网址需要生成短网址的时候，我们先利用 MurmurHash 算法，生成短网址。然后，我们拿这个新生成的短网址，在 MySQL 数据库中查找。</p>
<p>如果没有找到相同的短网址，这也就表明，这个新生成的短网址没有冲突。于是我们就将这个短网址返回给用户（请求生成短网址的用户），然后将这个短网址与原始网址之间的对应关系，存储到 MySQL 数据库中。</p>
<p>如果我们在数据库中，找到了相同的短网址，那也并不一定说明就冲突了。我们从数据库中，将这个短网址对应的原始网址也取出来。如果数据库中的原始网址，跟我们现在正在处理的原始网址是一样的，这就说明已经有人请求过这个原始网址的短网址了。我们就可以拿这个短网址直接用。如果数据库中记录的原始网址，跟我们正在处理的原始网址不一样，那就说明哈希算法发生了冲突。不同的原始网址，经过计算，得到的短网址重复了。这个时候，我们该怎么办呢？</p>
<p>我们可以给原始网址拼接一串特殊字符，比如“[DUPLICATED]”，然后再重新计算哈希值，两次哈希计算都冲突的概率，显然是非常低的。假设出现非常极端的情况，又发生冲突了，我们可以再换一个拼接字符串，比如“[OHMYGOD]”，再计算哈希值。然后把计算得到的哈希值，跟原始网址拼接了特殊字符串之后的文本，一并存储在 MySQL 数据库中。</p>
<p>当用户访问短网址的时候，短网址服务先通过短网址，在数据库中查找到对应的原始网址。如果原始网址有拼接特殊字符（这个很容易通过字符串匹配算法找到），我们就先将特殊字符去掉，然后再将不包含特殊字符的原始网址返回给浏览器。</p>
<h3 id="3-如何优化哈希算法生成短网址的性能？"><a href="#3-如何优化哈希算法生成短网址的性能？" class="headerlink" title="3. 如何优化哈希算法生成短网址的性能？"></a>3. 如何优化哈希算法生成短网址的性能？</h3><p>为了判断生成的短网址是否冲突，我们需要拿生成的短网址，在数据库中查找。如果数据库中存储的数据非常多，那查找起来就会非常慢，势必影响短网址服务的性能。那有没有什么优化的手段呢？</p>
<p>还记得我们之前讲的 MySQL 数据库索引吗？我们可以给短网址字段添加 B+ 树索引。这样通过短网址查询原始网址的速度就提高了很多。实际上，在真实的软件开发中，我们还可以通过一个小技巧，来进一步提高速度。</p>
<p>在短网址生成的过程中，我们会跟数据库打两次交道，也就是会执行两条 SQL 语句。第一个 SQL 语句是通过短网址查询短网址与原始网址的对应关系，第二个 SQL 语句是将新生成的短网址和原始网址之间的对应关系存储到数据库。</p>
<p>我们知道，一般情况下，数据库和应用服务（只做计算不存储数据的业务逻辑部分）会部署在两个独立的服务器或者虚拟服务器上。那两条 SQL 语句的执行就需要两次网络通信。这种 IO 通信耗时以及 SQL 语句的执行，才是整个短网址服务的性能瓶颈所在。所以，为了提高性能，我们需要尽量减少 SQL 语句。那又该如何减少 SQL 语句呢？</p>
<p>我们可以给数据库中的短网址字段，添加一个唯一索引（不只是索引，还要求表中不能有重复的数据）。当有新的原始网址需要生成短网址的时候，我们并不会先拿生成的短网址，在数据库中查找判重，而是直接将生成的短网址与对应的原始网址，尝试存储到数据库中。如果数据库能够将数据正常写入，那说明并没有违反唯一索引，也就是说，这个新生成的短网址并没有冲突。</p>
<p>当然，如果数据库反馈违反唯一性索引异常，那我们还得重新执行刚刚讲过的“查询、写入”过程，SQL 语句执行的次数不减反增。但是，在大部分情况下，我们把新生成的短网址和对应的原始网址，插入到数据库的时候，并不会出现冲突。所以，大部分情况下，我们只需要执行一条写入的 SQL 语句就可以了。所以，从整体上看，总的 SQL 语句执行次数会大大减少。</p>
<p>实际上，我们还有另外一个优化 SQL 语句次数的方法，那就是借助布隆过滤器。</p>
<p>我们把已经生成的短网址，构建成布隆过滤器。我们知道，布隆过滤器是比较节省内存的一种存储结构，长度是 10 亿的布隆过滤器，也只需要 125MB 左右的内存空间。</p>
<p>当有新的短网址生成的时候，我们先拿这个新生成的短网址，在布隆过滤器中查找。如果查找的结果是不存在，那就说明这个新生成的短网址并没有冲突。这个时候，我们只需要再执行写入短网址和对应原始网页的 SQL 语句就可以了。通过先查询布隆过滤器，总的 SQL 语句的执行次数减少了。</p>
<p>到此，利用哈希算法来生成短网址的思路，我就讲完了。实际上，这种解决思路已经完全满足需求了，我们已经可以直接用到真实的软件开发中。不过，我们还有另外一种短网址的生成算法，那就是利用自增的 ID 生成器来生成短网址。我们接下来就看一下，这种算法是如何工作的？对于哈希算法生成短网址来说，它又有什么优势和劣势？</p>
<h2 id="如何通过-ID-生成器生成短网址？"><a href="#如何通过-ID-生成器生成短网址？" class="headerlink" title="如何通过 ID 生成器生成短网址？"></a>如何通过 ID 生成器生成短网址？</h2><p>我们可以维护一个 ID 自增生成器。它可以生成 1、2、3…这样自增的整数 ID。当短网址服务接收到一个原始网址转化成短网址的请求之后，它先从 ID 生成器中取一个号码，然后将其转化成 62 进制表示法，拼接到短网址服务的域名（比如<a target="_blank" rel="noopener" href="http://t.cn/%EF%BC%89%E5%90%8E%E9%9D%A2%EF%BC%8C%E5%B0%B1%E5%BD%A2%E6%88%90%E4%BA%86%E6%9C%80%E7%BB%88%E7%9A%84%E7%9F%AD%E7%BD%91%E5%9D%80%E3%80%82%E6%9C%80%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E8%BF%98%E6%98%AF%E4%BC%9A%E6%8A%8A%E7%94%9F%E6%88%90%E7%9A%84%E7%9F%AD%E7%BD%91%E5%9D%80%E5%92%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%8E%9F%E5%A7%8B%E7%BD%91%E5%9D%80%E5%AD%98%E5%82%A8%E5%88%B0%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E3%80%82">http://t.cn/）后面，就形成了最终的短网址。最后，我们还是会把生成的短网址和对应的原始网址存储到数据库中。</a></p>
<p>理论非常简单好理解。不过，这里有几个细节问题需要处理。</p>
<h3 id="1-相同的原始网址可能会对应不同的短网址"><a href="#1-相同的原始网址可能会对应不同的短网址" class="headerlink" title="1. 相同的原始网址可能会对应不同的短网址"></a>1. 相同的原始网址可能会对应不同的短网址</h3><p>每次新来一个原始网址，我们就生成一个新的短网址，这种做法就会导致两个相同的原始网址生成了不同的短网址。这个该如何处理呢？实际上，我们有两种处理思路。</p>
<p>第一种处理思路是<strong>不做处理</strong>。听起来有点无厘头，我稍微解释下你就明白了。实际上，相同的原始网址对应不同的短网址，这个用户是可以接受的。在大部分短网址的应用场景里，用户只关心短网址能否正确地跳转到原始网址。至于短网址长什么样子，他其实根本就不关心。所以，即便是同一个原始网址，两次生成的短网址不一样，也并不会影响到用户的使用。</p>
<p>第二种处理思路是<strong>借助哈希算法生成短网址的处理思想，</strong>当要给一个原始网址生成短网址的时候，我们要先拿原始网址在数据库中查找，看数据库中是否已经存在相同的原始网址了。如果数据库中存在，那我们就取出对应的短网址，直接返回给用户。</p>
<p>不过，这种处理思路有个问题，我们需要给数据库中的短网址和原始网址这两个字段，都添加索引。短网址上加索引是为了提高用户查询短网址对应的原始网页的速度，原始网址上加索引是为了加快刚刚讲的通过原始网址查询短网址的速度。这种解决思路虽然能满足“相同原始网址对应相同短网址”这样一个需求，但是是有代价的：一方面两个索引会占用更多的存储空间，另一方面索引还会导致插入、删除等操作性能的下降。</p>
<h3 id="2-如何实现高性能的-ID-生成器？"><a href="#2-如何实现高性能的-ID-生成器？" class="headerlink" title="2. 如何实现高性能的 ID 生成器？"></a>2. 如何实现高性能的 ID 生成器？</h3><p>实现 ID 生成器的方法有很多，比如利用数据库自增字段。当然我们也可以自己维护一个计数器，不停地加一加一。但是，一个计数器来应对频繁的短网址生成请求，显然是有点吃力的（因为计数器必须保证生成的 ID 不重复，笼统概念上讲，就是需要加锁）。如何提高 ID 生成器的性能呢？关于这个问题，实际上，有很多解决思路。我这里给出两种思路。</p>
<p>第一种思路是借助第 54 节中讲的方法。我们可以给 ID 生成器装多个前置发号器。我们批量地给每个前置发号器发送 ID 号码。当我们接受到短网址生成请求的时候，就选择一个前置发号器来取号码。这样通过多个前置发号器，明显提高了并发发号的能力。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83qiqvb0j30vq0mx0u5.jpg" alt="img"></p>
<p>第二种思路跟第一种差不多。不过，我们不再使用一个 ID 生成器和多个前置发号器这样的架构，而是，直接实现多个 ID 生成器同时服务。为了保证每个 ID 生成器生成的 ID 不重复。我们要求每个 ID 生成器按照一定的规则，来生成 ID 号码。比如，第一个 ID 生成器只能生成尾号为 0 的，第二个只能生成尾号为 1 的，以此类推。这样通过多个 ID 生成器同时工作，也提高了 ID 生成的效率。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp83q4g9haj30vq0grjsn.jpg" alt="img"></p>
<h2 id="总结引申-13"><a href="#总结引申-13" class="headerlink" title="总结引申"></a>总结引申</h2><p>今天，我们讲了短网址服务的两种实现方法。我现在来稍微总结一下。</p>
<p>第一种实现思路是通过哈希算法生成短网址。我们采用计算速度快、冲突概率小的 MurmurHash 算法，并将计算得到的 10 进制数，转化成 62 进制表示法，进一步缩短短网址的长度。对于哈希算法的哈希冲突问题，我们通过给原始网址添加特殊前缀字符，重新计算哈希值的方法来解决。</p>
<p>第二种实现思路是通过 ID 生成器来生成短网址。我们维护一个 ID 自增的 ID 生成器，给每个原始网址分配一个 ID 号码，并且同样转成 62 进制表示法，拼接到短网址服务的域名之后，形成最终的短网址。</p>
<h2 id="课后思考-13"><a href="#课后思考-13" class="headerlink" title="课后思考"></a>课后思考</h2><p>如果我们还要额外支持用户自定义短网址功能（http//t.cn/{用户自定部分}），我们又该如何改造刚刚的算法呢?</p>
<p>我们在讲通过 ID 生成器生成短网址这种实现思路的时候，讲到相同的原始网址可能会对应不同的短网址。针对这个问题，其中一个解决思路就是，不做处理。但是，如果每个请求都生成一个短网址，并且存储在数据库中，那这样会不会撑爆数据库呢？我们又该如何解决呢？</p>
<p>今天是农历的大年三十，我们专栏的正文到这里也就全部结束了。从明天开始，我会每天发布一篇练习题，内容针对专栏涉及的数据结构和算法。从初一到初七，帮你复习巩固所学知识，拿下数据结构和算法，打响新年进步的第一枪！明天见！</p>
<p>出自【1】<a name='toc1'></a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="">参考1</a><a href="#toc1">🔼</a></li>
<li></li>
</ol>
</div></article></div><div class="right-container"><div class="widget"><div id="arAnchorBar"></div></div></div></section></div><div class="right-menu"></div><div class="modal search-modal"><div class="input-field"><input type="text" id="search_input"><label for="search-input">搜索</label></div><div id="search_result" class="search-result"></div></div><div class="blog-overlay"></div><footer class="row"><div class="footer-con"><div class="paginator"><a href="/2021/04/02/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E%EF%BC%88%E5%9F%BA%E7%A1%80%E7%AF%87%EF%BC%89/" title="⭐️数据结构与算法之美（基础篇）" class="prev">PREV</a><a href="/2021/03/25/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/" title="⭐️并发编程【未看完】" class="next">NEXT</a></div><div class="copyright"><p>© 2016 - 2021 <a target="_blank">John Doe</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <br> and <a href="https://github.com/Bulandent/hexo-theme-bubuzou" target="_blank">hexo-theme-bubuzou</a></p><p> <span style="padding-right: 6px;">闽ICP备16007301号-2</span></p></div><div class="totop"><i></i></div></div></footer><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/jquery-1.8.2.min.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/articleCatalog.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/main.js"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>