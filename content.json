{"meta":{"title":"Note","subtitle":null,"description":null,"author":"Dufy","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"⭐️待研究问题","slug":"待研究问题","date":"2119-10-27T16:00:00.000Z","updated":"2020-08-17T13:58:39.610Z","comments":true,"path":"2119/10/28/待研究问题/","link":"","permalink":"http://yoursite.com/2119/10/28/待研究问题/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 各数据库的作用？2020-0817如ES, TIDB, MYSQL, KAFKA等… 网页展示模型效果2020-0713https://github.com/luoyangbiao/bert_flask 关于类 2020-0408什么时候应该使用类？ 众多函数如何构建文件进行分类存储？ 流程图参考 图片爬取 2020-03-08https://movie.douban.com/celebrity/1025155/photos/?type=C&amp;start=0&amp;sortby=like&amp;size=a&amp;subtype=a 对工作中fasttext源程序进行修改，提高易读性 2020-02-23参考：词向量的表示学习方法：word2vec和fasttext详解和代码示例 关于预先训练模型的使用 2020-02-17参考文章：一文看懂迁移学习：怎样用预训练模型搞定深度学习？ 于是，我转而去采用预训练模型，这样我不需要重新训练我的整个结构，只需要针对其中的几层进行训练即可。 因此，我采用了在ImageNet数据集上预先训练好的VGG16模型，这个模型可以在Keras库中找到。 模型的结构如下所示： 联想到目前的bom 分类，可以试着参考 目标检测论文（尤其针对一些小目标的可能改进方法）https://blog.csdn.net/u014236392/article/details/83993730 Hanlp训练测试过程有待进一步搞清楚？具体怎么样的？ 训练与不训练区别如何？ 流向图表绘制 2019-12-10在业务中，遇到不同标签的分类错误问题，我想讲错误的结果用图表给绘制出来，但是目前还没有方法 如，labelA误分为labelB，labelC误分为labelB，等等 那么可以用图表表示如下： 其中，箭头方向表示不同标签错分情况，粗细表示❌划分的数量 文件读、写2019-12-12需要总结项目经验，把经验写下来 如何换行写入？？？ 如何正确读？ readline()方法 如何操作？？ 华为面试 参考","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[],"author":"Dufy"},{"title":"⭐️网站推荐","slug":"网站推荐","date":"2100-01-28T16:00:00.000Z","updated":"2020-03-16T15:21:34.445Z","comments":true,"path":"2100/01/29/网站推荐/","link":"","permalink":"http://yoursite.com/2100/01/29/网站推荐/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… Top Websites Ranking 全球排名查询系统 阮一峰GitHubhttps://github.com/ruanyf 参考","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}],"tags":[{"name":"网站","slug":"网站","permalink":"http://yoursite.com/tags/网站/"}],"author":"Dufy"},{"title":"⭐️copy","slug":"copy","date":"2021-11-07T16:00:00.000Z","updated":"2021-02-28T14:28:50.987Z","comments":true,"path":"2021/11/08/copy/","link":"","permalink":"http://yoursite.com/2021/11/08/copy/","excerpt":"","text":"出自【1】 参考 参考1🔼","categories":[],"tags":[],"author":"Dufy"},{"title":"⭐️数据结构与算法之美（加餐）","slug":"数据结构与算法之美（加餐）","date":"2021-04-03T16:00:00.000Z","updated":"2021-04-06T14:51:13.806Z","comments":true,"path":"2021/04/04/数据结构与算法之美（加餐）/","link":"","permalink":"http://yoursite.com/2021/04/04/数据结构与算法之美（加餐）/","excerpt":"","text":"不定期福利第一期 | 数据结构与算法学习书单王争 2018-09-30 你好，我是王争。欢迎来到不定期更新的周末福利时间。 专栏已经上线两周了，看到这么多人在留言区写下自己的疑惑或者观点，我特别开心。在留言里，很多同学让我推荐一些学习数据结构与算法的书籍。因此我特意跟编辑商量了，给你一个周末福利。所以这一期呢，我们就来聊一聊数据结构和算法学习过程中有哪些必读书籍。 有的同学还在读大学，代码还没写过几行；有的同学已经工作数十年，这之间的差别还是挺大的。而不同基础的人，适宜看的书是完全不一样的。因此，针对不同层次、不同语言的同学，我分别推荐了不同的书。希望每个同学，都能找到适合自己的学习资料，都能在现有水平上有所提高。 针对入门的趣味书入门的同学，我建议你不要过度追求上去就看经典书。像《算法导论》《算法》这些书，虽然比较经典、比较权威，但是非常厚。初学就去啃这些书肯定会很费劲。而一旦啃不下来，挫败感就会很强。所以，入门的同学，我建议你找一些比较容易看的书来看，比如《大话数据结构》和《算法图解》。不要太在意书写得深浅，重要的是能不能坚持看完。 《大话数据结构》 这本书最大的特点是，它把理论讲得很有趣，不枯燥。而且每个数据结构和算法，作者都结合生活中的例子进行了讲解，能让你有非常直观的感受。虽然这本书有 400 多页，但是花两天时间读完，应该是没问题的。如果你之前完全不懂数据结构和算法，可以先从这本书看起。 《算法图解》 跟《大话数据结构》走的是同样的路线，就像这本书副标题写的那样，“像小说一样有趣的算法入门书”，主打“图解”，通俗易懂。它只有不到 200 页，所以内容比较少。作为入门，看看这本书，能让你对数据结构和算法有个大概的认识。 这些入门书共同的问题是，缺少细节，不够系统，也不够严谨。所以，如果你想要系统地学数据结构和算法，看这两本书肯定是不够的。 针对特定编程语言的教科书讲数据结构和算法，肯定会跟代码实现挂钩。所以，很多人就很关心，某某书籍是用什么语言实现的，是不是自己熟悉的语言。市面大部分数据结构和算法书籍都是用 C、C++、Java 语言实现的，还有些是用伪代码。而使用 Python、Go、PHP、JavaScript、Objective-C 这些编程语言实现的就更少了。 我这里推荐《数据结构和算法分析》。国内外很多大学都拿这本书当作教材。这本书非常系统、全面、严谨，而且又不是特别难，适合对数据结构和算法有些了解，并且掌握了至少一门编程语言的同学。而且，这个作者也很用心。他用了三种语言，写了三个版本，分别是：《数据结构与算法分析 ：C 语言描述》《数据结构与算法分析：C++ 描述》《数据结构与算法分析：Java 语言描述》。 如果你熟悉的是 Python 或者 JavaScript，可以参考《数据结构与算法 JavaScript 描述》《数据结构与算法：Python 语言描述》 。至于其他语言的算法书籍，确实比较少。如果你有推荐，可以在留言区补充一下。 面试必刷的宝典算法对面试很重要，很多人也很关心。我这里推荐几本有益于面试的书籍，分别是：《剑指 offer》《编程珠玑》《编程之美》。 从《剑指 offer》这本书的名字就可以看出，作者的写作目的非常明确，就是为了面试。这本书几乎包含了所有常见的、经典的面试题。如果能搞懂这本书里的内容，应付一般公司的面试应该不成问题。 《编程珠玑》这本书的豆瓣评分非常高，有 9 分。这本书最大的特色就是讲了很多针对海量数据的处理技巧。这个可能是其他算法书籍很少涉及的。面试的时候，海量数据处理的问题也是经常会问的，特别是校招面试。不管是开拓眼界，还是应付面试，这本书都很值得一看。 《编程之美》这本书有多位作者，其中绝大部分是微软的工程师，所以书的质量很有保证。不过，这里面的算法题目稍微有点难，也不是很系统，这也是我把它归到面试这一部分的原因。如果你有一定基础，也喜欢钻研些算法问题，或者要面试 Google、Facebook 这样的公司，可以拿这本书里的题，先来自测一下。 经典大部头很多人一提到算法书就会搬出《算法导论》和《算法》。这两本确实非常经典，但是都太厚了，看起来比较费劲，我估计很少有人能坚持全部看下来。如果你想更加深入地学一学数据结构和算法，我还是强烈建议你看看。 我个人觉得，《算法导论》这本书的章节安排不是循序渐进的，里面充斥着各种算法的正确性、复杂度的证明、推导，数学公式比较多，一般人看起来会比较吃力。所以，作为入门书籍，并不是很推荐。 《算法》这本书也是一本经典大部头，不过它比起《算法导论》来要友好很多，更容易看懂，更适合初学者入门。但是这本书的缺点也很明显，就是内容不够全面，比如动态规划这么重要的知识点，这本书就没有讲。对于数据结构的东西，它讲的也不多，基本就是偏重讲算法。 殿堂级经典说到殿堂级经典书，如果《计算机程序设计艺术》称第二，我想没人敢称第一。这本书包括很多卷。说实话，我也只看过比较简单的几卷，比如《基本算法》《排序和查找》。 这套书的深度、广度、系统性、全面性是其他所有数据结构和算法书籍都无法相比的。但是，如果你对算法和数据结构不是特别感兴趣，没有很好的数学、算法、计算机基础，想要把这套书读完、读懂是比较难的。你可以把它当作你算法学习的终极挑战。 闲暇阅读算法无处不在。我这里再推荐几本适合闲暇时间阅读的书：《算法帝国》《数学之美》《算法之美》。 这些书共同的特点是，都列举了大量的例子，非常通俗易懂。夸张点说，像《算法帝国》，文科生都能读懂。当你看这些书的时候，你常常会深深感受到算法的力量，被算法的优美之处折服。即便不是从事 IT 工作的，看完这几本书也可以开拓眼界。 书籍差不多就是这些。除此之外，留言区很多人问到算法的实现语言。我这里也解释一下。因为我现在比较常用的编程语言是 Java。所以，在专栏里，特别简单的、不涉及高级语法的，我会用 Java 或者 C、C++ 来实现。稍微复杂的，为了让你能看懂，我会用伪代码。所以你完全不用担心语言的问题。 每节课中有需要代码实现的数据结构和算法，我都另外用 Java 语言实现一遍，然后放到 Github 上，供你参考。Github 的地址我放在这里，你可以收藏一下：https://github.com/wangzheng0822/algo。 至于其他语言的同学，比如 C、C++、Python、Go、PHP、JavaScript、Objective-C 等，我想了一个 crowd sourcing 的方法。 我希望基础较好的同学，参照我的 Java 实现，用你熟悉的编程语言再实现一遍，并且将代码留言给我。如果你写得正确，我会将你的代码上传到 Github 上，分享给更多人。 还有人问，我学完这个专栏，就可以拿下数据结构和算法吗？我想说的是，每个人的基础、学习能力都不一样，掌握程度取决于你的努力程度。除了你之外，没有人能百分之百保证你能掌握什么知识。 有的同学只是把每一节课听下来、看下来，就束之高阁，也不求甚解，那效果肯定会很差。而有些同学除了听、看之外，遇到不懂的会自己去查资料、看参考书籍，还会把我讲的数据结构和算法都认真地实现一遍，这样的学习效果自然就比只听一遍、看一遍要好很多。即便我已经尽我所能把这些知识讲得深入浅出，通俗易懂，但是学习依然还是要靠你自己啊。 这种答疑的方式也会成为我们之后的固定动作，我会把留言里有价值的问题和反馈沉淀下来，希望对你的日常学习起到补充作用。如果你有什么看不懂、听不懂的地方，或者工作中有遇到算法问题、技术难题，欢迎写在留言区。（我发现留言区里卧虎藏龙啊，没事儿可以多扫扫留言区。） 这次的周末福利时间就到这啦，我们下次见！ 不定期福利第二期 | 王争：羁绊前行的，不是肆虐的狂风，而是内心的迷茫王争 2018-11-23 专栏更新过半，我发现有些小伙伴已经掉队，虽然有人掉队也挺正常，但是我还是想尽量拉一把。于是，周末的时间，我就在想，究竟是什么原因让有些小伙伴掉队了？是内容本身太难了吗？是我讲得不够清楚吗？还是小伙伴本身基础太差、不够努力、没有掌握学习方法？ 我觉得都不是，让你掉队的原因，从根儿上讲，是你内心的迷茫。如果我们不那么确信能不能看懂、能不能学会的时候，当面对困难的时候，很容易就会否定自己，也就很容易半途而废。 这就好比你迷失在沙漠中，对你来说，肆虐的狂风并不可怕，可怕的是，你不知道该努力多久才能走出沙漠，不知道到底能不能走出沙漠。这种对结果的未知、不确定，导致了你内心的恐惧，最后就差那么一点点就可以走出沙漠的时候，你放弃了。 学习也是同样的道理。所以，我今天不打算讲学习方法，也不打算给你灌输心灵鸡汤，我就讲讲，对这个专栏的学习，或者对于任何学习来说，我觉得你应该建立的一些正确认知。有了这些认知，希望你能在后面的专栏学习中，少一点迷茫，多一份坚持。 没有捷径，没有杀手锏，更没有一招致胜的“葵花宝典” 有小伙伴给我留言说：“看书五分钟，笔记两小时，急求学霸的学习方法”，还有人问，“数据结构和算法好难，到底该怎么学？是我的学习方法不对？还是我太笨？” 我想说，并没有什么杀手锏的学习方法，更没有一招致胜的“葵花宝典”。不知道这么说有没有让你失望。如果你真要“求”一个学习方法，那就再看看我在专栏开始写的“**如何抓住重点，系统高效地学习数据结构与算法**”那篇文章吧。 说实话，我也挺想知道学霸的学习方法的，所以，在求学路上，每当有学霸来分享学习方法，我都要去听一听。但是，听多了之后，我发现其实并没有太多用。因为那些所谓学霸的学习方法，其实都很简单，比如“认认真真听讲”“认认真真做每一道题”等等。 也不是他们说的不对，但是这种大实话，我总有一种领会不了的感觉，更别说真正指导我的学习了。而且，我觉得，很多时候，这些方法论的难点并不在于能不能听懂，而是在于能不能执行到位。比如很多人都听过“一万小时定律”，坚持一万个小时，你就能成为大牛，但有多少人能坚持一万个小时呢？ 所以，这里我要纠正一个认知，那就是，学习没有“杀手锏”似的方法论。不要怀疑是不是自己的学习方法不对，不要在开始就否定自己。因为否定得越多，你就越迷茫，越不能坚持。 不要浮躁，不要丧失思考能力，不要丧失学习能力 有小伙伴给我留言说：“老师，这个地方看不懂，你能不能再解释一下”，还有小伙伴留言说：“《红黑树（上）》里的图为什么跟你的定义不相符？” 对于留言的问题，我都挺重视的，但是当仔细看这些问题的时候，我发现，实际上文章里已经有答案了，他根本没有认真看、认真思考，更别说去自己搜搜资料，再研究下，就来提问了。 一般情况下，我都会回复“你自己再认真看一遍”或者“你自己先去网上搜一下，研究研究，如果还不懂再给我留言”。告诉你答案，并不会花费我太长时间，但是，这样会让你丢失最宝贵的东西，那就是，你自己的思考能力、学习能力，能自己沉下心来研究的能力。这个是很可怕的。 现在，互联网如此发达，我们每天都会面对各种各样的信息轰炸，人也变得越来越浮躁。很多人习惯看些不动脑子就能看懂的东西，看到稍微复杂的东西，就感觉脑子转不动了。 上学的时候还好，要考试，有老师督促，还能坚持学习。但是工作之后，没有人监督，很多人陷入各种手机 App 中不能自拔，学一会儿就想玩会儿手机，想静下心来学上半个小时都无比困难。无法自律，沉不下心来，那你就基本可以跟学习说拜拜了。 只有做好打硬仗的心理准备，遇到困难才能心态平和 还有小伙伴给我留言说：“看不懂，一个 4000 多字的文章、10 分钟的音频，反复看了、听了 2 个小时都没怎么看懂”。我给他的回复是：“如果之前没有基础或者基础不好的话，看 2 个小时还不懂，很正常，看一个礼拜试试。” “一个礼拜”的说法，我一点都不是夸张。虽然专栏的每篇文章都只有三四千字，10 分钟左右的音频，但是知识点的密度还是很高的。如果你潜意识里觉得应该一下子就能看懂，就会出现这样的情况：看了一遍不懂，又看了一遍还是不怎么懂，然后就放弃了。 数据结构和算法就是一个非常难啃的硬骨头，可以说是计算机学科中最难学的学科之一了。我当时学习也费了老大的劲，能做到讲给你听，我靠的也是十年如一的积累和坚持。如果没有基础、或者基础不好，你怎能期望看 2 个小时就能完全掌握呢？ 面对这种硬骨头，我觉得我们要有打硬仗、打持久战的心理准备。只有这样，在学习的过程中遇到困难的时候，心态才能更加平和，才能沉下心来有条不紊地去解决一个个的疑难问题。这样，碰到问题，你可能还会“窃喜”，我又遇到了一个之前不怎么懂的知识点了，看懂它我又进步了一点。甚至你还会“坏坏地”想，又多了一个拉开我跟其他人距离的地方了。跨过这些点，我就能比别人更厉害。 一口吃不成胖子，如果你基础不好，那就从长计议吧，给自己定一个长一点的“死磕”计划，比如一年。面对不懂的知识点，沉下心来逐个突破，这样你的信心慢慢也就建立了。 “放弃”的念头像是一个心魔，它会一直围绕着你 还有小伙伴给我留言说：“开始没怎么看懂，看了一下午，终于看懂了”。看到这样的留言，我其实挺为他感到庆幸的，庆幸他没有中途放弃。因为，放弃的念头就像一个心魔，在我们的学习过程中，它会一直围绕着我们，一旦被它打败一次，你就会被它打败很多次，掉队就不可避免了。 我分享一个我最近思考比较多的事情。前一段时间，我在研究多线程方面的东西，它涉及一块比较复杂的内容，“Java 内存模型”。虽然看懂并不难，但是要透彻、无盲点地理解并不容易。本来以为半天就能看懂的东西，结果我从周一一直看到周五下午，断断续续花了 5 天的时间才把它彻底搞懂。回忆起这 5 天，我有不下 10 次都想放弃，每次心里都在想：“算了，先放一放，以后再说吧”“太难了，啃不下来，算了。”“就这样吧，反正也用不到，没必要浪费时间”等等。这种放弃的念头就像一个邪恶的魔鬼一样，一直围绕着我这 5 天的研究中。 现在回想起来，我很庆幸我当时没有放弃，多坚持了几天。如果当时我放弃了，那之后再遇到技术难题时，“放弃”的心魔还会再来拜访我，潜意识里我还是会认输。 之所以没有放弃，我自己总结了两点原因。 第一，我对学习这件事情认识得比较清楚，我一直觉得，没有学不会的东西，没有攻克不了的技术难题，如果有，那就说明时间花得还不够多。 第二，我之前遇到卡壳的时候，几乎从来没有放弃过，即便短暂地停歇，我也会继续拎起来再死磕，而且每次都能搞定，正是这种正向的激励，给了我信心，让我再遇到困难的时候，都能坚信自己能搞定它。 入门是一个非常漫长和煎熬的过程，谁都逃不过 还有小伙伴留言说：“看到有小伙伴有很多疑问，我来帮作者说句话，文章写得很好，通俗易懂，如果有一定基础，看懂还是不成问题的。” 我觉得，有些小伙伴的觉悟还是挺高的：）。我文章写得再通俗易懂，对于之前没有任何基础的人来说，看起来还是挺费劲的。 第一，数据结构和算法这门课程本身的难度摆在那里，想要轻松看懂，本身就不太现实。第二，对于任何新知识的学习，入门都是一个非常漫长和煎熬的过程。但是这个过程都是要经历的，谁都逃不过。只要你挺过去，入了门，再学习更深的知识就简单多了。 我大学里的第一堂课是 C 语言，现在回想起来，当时对我来说，简直就是听天书。因为之前没有接触过计算机，更别说编程语言，对我来说，C 语言就像另一个世界的东西。从完全看不懂，到慢慢有点看懂，再到完全看懂，不夸张地讲，我花了好几年的时间，但是当掌握了之后，我发现这个东西其实也不难。但是如果没有度过漫长和煎熬的入门的过程，如果没有一点韧性，没有一点点信念，那可能也没有现在的我了。 其实我一直觉得情商比智商更重要。对于很多学科的学习，智商并不是瓶颈，最终能够决定你能达到的高度的，还是情商，而情商中最重要的，我觉得就是逆商（逆境商数，Adversity Quotient），也就是，当你遇到困难时，你会如何去面对，这将会决定你的人生最终能够走多远。 好了，今天我想分享的关于学习的几个认知就讲完了。现在，你有没有对学习这件事有更加清晰的认识呢？能不能让你少一点迷茫，多一份坚持呢？ 最后，我有一句送给你：吃得苦中苦，方为人上人。耐得住寂寞，才能守得住繁华。 不定期福利第三期 | 测一测你的算法阶段学习成果王争 2018-12-21 专栏最重要的基础篇马上就要讲完了，不知道你掌握了多少？我从前面的文章中挑选了一些案例，稍加修改，组成了一套测试题。 你先不要着急看答案，自己先想一想怎么解决，测一测自己对之前的知识掌握的程度。如果有哪里卡壳或者不怎么清楚的，可以回过头再复习一下。 正所谓温故知新，这种通过实际问题查缺补漏的学习方法，非常利于你巩固前面讲的知识点，你可要好好珍惜这次机会哦！ 实战测试题（一）假设猎聘网有 10 万名猎头顾问，每个猎头顾问都可以通过做任务（比如发布职位），来积累积分，然后通过积分来下载简历。假设你是猎聘网的一名工程师，如何在内存中存储这 10 万个猎头 ID 和积分信息，让它能够支持这样几个操作： 根据猎头的 ID 快速查找、删除、更新这个猎头的积分信息； 查找积分在某个区间的猎头 ID 列表； 查询积分从小到大排在第 x 位的猎头 ID 信息； 查找按照积分从小到大排名在第 x 位到第 y 位之间的猎头 ID 列表。 相关章节17 | 跳表：为什么 Redis 一定要用跳表来实现有序集合？ 20 | 散列表（下）：为什么散列表和链表经常会一起使用？ 25 | 红黑树：为什么工程中都用红黑树这种二叉树？ 题目解析这个问题既要通过 ID 来查询，又要通过积分来查询，所以，对于猎头这样一个对象，我们需要将其组织成两种数据结构，才能支持这两类操作。 我们按照 ID，将猎头信息组织成散列表。这样，就可以根据 ID 信息快速地查找、删除、更新猎头的信息。我们按照积分，将猎头信息组织成跳表这种数据结构，按照积分来查找猎头信息，就非常高效，时间复杂度是 O(logn)。 我刚刚讲的是针对第一个、第二个操作的解决方案。第三个、第四个操作是类似的，按照排名来查询，这两个操作该如何实现呢？ 我们可以对刚刚的跳表进行改造，每个索引结点中加入一个 span 字段，记录这个索引结点到下一个索引结点的包含的链表结点的个数。这样就可以利用跳表索引，快速计算出排名在某一位的猎头或者排名在某个区间的猎头列表。 实际上，这些就是 Redis 中有序集合这种数据类型的实现原理。在开发中，我们并不需要从零开始代码实现一个散列表和跳表，我们可以直接利用 Redis 的有序集合来完成。 实战测试题（二）电商交易系统中，订单数据一般都会很大，我们一般都分库分表来存储。假设我们分了 10 个库并存储在不同的机器上，在不引入复杂的分库分表中间件的情况下，我们希望开发一个小的功能，能够快速地查询金额最大的前 K 个订单（K 是输入参数，可能是 1、10、1000、10000，假设最大不会超过 10 万）。如果你是这个功能的设计开发负责人，你会如何设计一个比较详细的、可以落地执行的设计方案呢？ 为了方便你设计，我先交代一些必要的背景，在设计过程中，如果有其他需要明确的背景，你可以自行假设。 数据库中，订单表的金额字段上建有索引，我们可以通过 select order by limit 语句来获取数据库中的数据； 我们的机器的可用内存有限，比如只有几百 M 剩余可用内存。希望你的设计尽量节省内存，不要发生 Out of Memory Error。 相关章节12 | 排序（下）：如何用快排思想在 O(n) 内查找第 K 大元素？ 28 | 堆和堆排序：为什么说堆排序没有快速排序快？ 29 | 堆的应用：如何快速获取到 Top 10 最热门的搜索关键词？ 题目解析解决这个题目的基本思路我想你应该能想到，就是借助归并排序中的合并函数，这个我们在排序（下）以及堆的应用那一节中讲过。 我们从每个数据库中，通过 select order by limit 语句，各取局部金额最大的订单，把取出来的 10 个订单放到优先级队列中，取出最大值（也就是大顶堆堆顶数据），就是全局金额最大的订单。然后再从这个全局金额最大订单对应的数据库中，取出下一条订单（按照订单金额从大到小排列的），然后放到优先级队列中。一直重复上面的过程，直到找到金额前 K（K 是用户输入的）大订单。 从算法的角度看起来，这个方案非常完美，但是，从实战的角度来说，这个方案并不高效，甚至很低效。因为我们忽略了，数据库读取数据的性能才是这个问题的性能瓶颈。所以，我们要尽量减少 SQL 请求，每次多取一些数据出来，那一次性取出多少才合适呢？这就比较灵活、比较有技巧了。一次性取太多，会导致数据量太大，SQL 执行很慢，还有可能触发超时，而且，我们题目中也说了，内存有限，太多的数据加载到内存中，还有可能导致 Out of Memory Error。 所以，一次性不能取太多数据，也不能取太少数据，到底是多少，还要根据实际的硬件环境做 benchmark 测试去找最合适的。 实战测试题（三）我们知道，CPU 资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致 CPU 频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。 当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？ 相关章节09 | 队列：队列在线程池等有限资源池中的应用 题目解析这个问题的答案涉及队列这种数据结构。队列可以应用在任何有限资源池中，用于排队请求，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。 这个问题的具体答案，在队列那一节我已经讲得非常详细了，你可以回去看看，这里我就不赘述了。 实战测试题（四）通过 IP 地址来查找 IP 归属地的功能，不知道你有没有用过？没用过也没关系，你现在可以打开百度，在搜索框里随便输一个 IP 地址，就会看到它的归属地。 这个功能并不复杂，它是通过维护一个很大的 IP 地址库来实现的。地址库中包括 IP 地址范围和归属地的对应关系。比如，当我们想要查询 202.102.133.13 这个 IP 地址的归属地时，我们就在地址库中搜索，发现这个 IP 地址落在[202.102.133.0, 202.102.133.255]这个地址范围内，那我们就可以将这个 IP 地址范围对应的归属地“山东东营市”显示给用户了。 [202.102.133.0, 202.102.133.255] 山东东营市 [202.102.135.0, 202.102.136.255] 山东烟台 [202.102.156.34, 202.102.157.255] 山东青岛 [202.102.48.0, 202.102.48.255] 江苏宿迁 [202.102.49.15, 202.102.51.251] 江苏泰州 [202.102.56.0, 202.102.56.255] 江苏连云港 在庞大的地址库中逐一比对 IP 地址所在的区间，是非常耗时的。假设在内存中有 12 万条这样的 IP 区间与归属地的对应关系，如何快速定位出一个 IP 地址的归属地呢？ 相关章节15 | 二分查找（上）：如何用最省内存的方式实现快速查找功能？ 16 | 二分查找（下）：如何快速定位 IP 对应的省份地址？ 题目解析这个问题可以用二分查找来解决，不过，普通的二分查找是不行的，我们需要用到二分查找的变形算法，查找最后一个小于等于某个给定值的数据。不过，二分查找最难的不是原理，而是实现。要实现一个二分查找的变形算法，并且实现的代码没有 bug，可不是一件容易的事情，不信你自己写写试试。 关于这个问题的解答以及写出 bug free 的二分查找代码的技巧，我们在二分查找（下）那一节有非常详细的讲解，你可以回去看看，我这里就不赘述了。 实战测试题（五）假设我们现在希望设计一个简单的海量图片存储系统，最大预期能够存储 1 亿张图片，并且希望这个海量图片存储系统具有下面这样几个功能： 存储一张图片及其它的元信息，主要的元信息有：图片名称以及一组 tag 信息。比如图片名称叫玫瑰花，tag 信息是{红色，花，情人节}； 根据关键词搜索一张图片，比如关键词是“情人节 花”“玫瑰花”； 避免重复插入相同的图片。这里，我们不能单纯地用图片的元信息，来比对是否是同一张图片，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。 我们希望自主开发一个简单的系统，不希望借助和维护过于复杂的三方系统，比如数据库（MySQL、Redis 等）、分布式存储系统（GFS、Bigtable 等），并且我们单台机器的性能有限，比如硬盘只有 1TB，内存只有 2GB，如何设计一个符合我们上面要求，操作高效，且使用机器资源最少的存储系统呢？ 相关章节21 | 哈希算法（上）：如何防止数据库中的用户信息被脱库？ 22 | 哈希算法（下）：哈希算法在分布式系统中有哪些应用？ 题目解析这个问题可以分成两部分，第一部分是根据元信息的搜索功能，第二部分是图片判重。 第一部分，我们可以借助搜索引擎中的倒排索引结构。关于倒排索引我会在实战篇详细讲解，我这里先简要说下。 如题目中所说，一个图片会对应一组元信息，比如玫瑰花对应{红色，花，情人节}，牡丹花对应{白色，花}，我们可以将这种图片与元信息之间的关系，倒置过来建立索引。“花”这个关键词对应{玫瑰花，牡丹花}，“红色”对应{玫瑰花}，“白色”对应{牡丹花}，“情人节”对应{玫瑰花}。 当我们搜索“情人节 花”的时候，我们拿两个搜索关键词分别在倒排索引中查找，“花”查找到了{玫瑰花，牡丹花}，“情人节”查找到了{玫瑰花}，两个关键词对应的结果取交集，就是最终的结果了。 第二部分关于图片判重，我们要基于图片本身来判重，所以可以用哈希算法，对图片内容取哈希值。我们对哈希值建立散列表，这样就可以通过哈希值以及散列表，快速判断图片是否存在。 我这里只说说我的思路，这个问题中还有详细的内存和硬盘的限制。要想给出更加详细的设计思路，还需要根据这些限制，给出一个估算。详细的解答，我都放在哈希算法（下）那一节里了，你可以自己回去看。 实战测试题（六）我们知道，散列表的查询效率并不能笼统地说成是 O(1)。它跟散列函数、装载因子、散列冲突等都有关系。如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降。 在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果我们使用的是基于链表的冲突解决方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从 O(1) 急剧退化为 O(n)。 如果散列表中有 10 万个数据，退化后的散列表查询的效率就下降了 10 万倍。更直观点说，如果之前运行 100 次查询只需要 0.1 秒，那现在就需要 1 万秒。这样就有可能因为查询操作消耗大量 CPU 或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。这也就是散列表碰撞攻击的基本原理。 如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？ 相关章节18 | 散列表（上）：Word 文档中的单词拼写检查功能是如何实现的？ 19 | 散列表（中）：如何打造一个工业级水平的散列表？ 题目解析我经常把这道题拿来作为面试题考察候选人。散列表可以说是我们最常用的一种数据结构了，编程语言中很多数据类型，都是用散列表来实现的。尽管很多人能对散列表都知道一二，知道有几种散列表冲突解决方案，知道散列表操作的时间复杂度，但是理论跟实践还是有一定距离的。光知道这些基础的理论并不足以开发一个工业级的散列表。 所以，我在散列表（中）那一节中详细给你展示了一个工业级的散列表要处理哪些问题，以及如何处理的，也就是这个问题的详细答案。 这六道题你回答得怎么样呢？或许你还无法 100% 回答正确，没关系。其实只要你看了解析之后，有比较深的印象，能立马想到哪节课里讲过，这已经说明你掌握得不错了。毕竟想要完全掌握我讲的全部内容还是需要时间沉淀的。对于这门课的学习，你一定不要心急，慢慢来。只要方向对了就都对了，剩下就交给时间和努力吧！ 通过这套题，你对自己的学习状况应该有了一个了解。从专栏开始到现在，三个月过去了，我们的内容也更新了大半。你在专栏开始的时候设定的目标是什么？现在实施得如何了？**你可以在留言区给这三个月的学习做个**阶段性学习复盘。重新整理，继续出发！ 不定期福利第四期 | 刘超：我是怎么学习《数据结构与算法之美》的？刘超 2018-12-28 你好，我是刘超，是隔壁《趣谈网络协议》专栏的作者。今天来“串个门儿”，讲讲我学习《数据结构与算法之美》这个专栏的一些体会和感受。 《数据结构与算法之美》是目前“极客时间”订阅量最多的专栏，我也是其中最早购买的一员。我之所以一看就心动了，源于王争老师在开篇词里面说的那段话： 基础知识就像是一座大楼的地基，它决定了我们的技术高度。那技术人究竟都需要修炼哪些“内功”呢？我觉得，无外乎就是大学里的那些基础课程，操作系统、计算机网络、编译原理等等，当然还有数据结构和算法。 这个也是我写《趣谈网络协议》的时候，在开篇词里反复强调的观点。我为什么这么说呢？因为，我们作为面试官，在招人的时候，往往发现，使用框架速成的人很多，基础知识扎实的人少见，而基础不扎实会影响你以后学习新技术的速度和职业发展的广度。 和“极客时间”编辑聊的时候，我也多次表达，希望我们讲的东西和一般的培训机构有所区别，希望“极客时间”能做真正对程序员的技能提升和职业发展有价值的内容，希望“极客时间”能够成为真正帮助程序员成长的助手。 所以，当“极客时间”相继推出《Java 核心技术 36 讲》《零基础学 Python》《从 0 开始学架构》《MySQL 实战 45 讲》这些课程的时候，我非常开心。我希望将来能够继续覆盖到编译原理、操作系统、计算机组成原理等等。在这些课程里，算法是基础的基础，也是我本人很想精进的部分。 当然，除了长远的职业发展需要，搞定算法还有一个看得见、摸得着的好处，面试。 我经常讲，越是薪资低的企业，面试的时候，它们往往越注重你会不会做网站，甚至会要求你现场做出个东西来。你要注意了，这其实是在找代码熟练工。相反，越是薪资高的企业，越是重视考察基础知识。基础好，说明可塑性强，培养起来也比较快。而最牛的公司，考的往往是算法和思路。 相信很多购买《数据结构与算法之美》专栏的同学，下单的时候，已经想象自己面试的时候，在白板上挥洒代码，面试官频频点头的场景，想着自己马上就能“进驻牛公司，迎娶白富美”了。 然而，事实却是，武功套路容易学，扎马步基本功难练，编程也是一样。框架容易学，基本功难。你没办法讨巧，你要像郭靖学习降龙十八掌那样，一掌一掌劈下去才行。 于是，咱们这个专栏就开始了，你见到的仍然是困难的复杂度计算，指针指来指去，烧脑的逻辑，小心翼翼的边界条件判断。你发现，数据结构和算法好像并不是你上下班时间顺便听一听就能攻克的问题。你需要静下心来仔细想，拿个笔画一画，甚至要写一写代码，Debug 一下，才能够理解。是的，的确不轻松，那你坚持下来了吗？ 我在这里分享一下我的学习思路，我将这个看起来困难的过程分成了几部分来完成。 第一部分，数据结构和算法的基础知识部分。如果在大学学过这门课，在专栏里，你会看到很多熟悉的描述。有些基础比较好的同学会质疑写这些知识的必要性。这大可不必，因为每个人的基础不一样，为了专栏内容的系统性和完整性，老师肯定要把这些基础知识重新讲述一遍的。对于这一部分内容，如果你的基础比较好，可以像学其他课程一样，在上下班或者午休的时候进行学习，主要是起到温习的作用。 第二部分，需要代码练习的部分。由于王争老师面试过很多人，所以在专栏里，他会列举一些他在面试中常常会问的题目。很多情况下，这些题目需要当场就能在白板上写出来。这些问题对于想要提升自己面试能力的同学来说，应该是很有帮助的。 我这里列举几个，你可以看看，是不是都能回答出来呢？ 在链表这一节：单链表反转，链表中环的检测，两个有序的链表合并，删除链表倒数第 n 个结点，求链表的中间结点等。 在栈这一节，在函数调用中的应用，在表达式求值中的应用，在括号匹配中的应用。 在排序这一节，如何在 O(n) 的时间复杂度内查找一个无序数组中的第 K 大元素？ 在二分查找这一节，二分查找的四个变体。 这些问题你都应该上手写写代码，或者在面试之前拿来练练手，而且，不仅仅只是实现主要功能。大公司的面试很多情况下都会考虑边界条件。只要被面试官抓住漏洞，就会被扣分，所以你最好事先写写。 第三部分，对于海量数据的处理思路问题。现在排名靠前的大公司，大都存在海量数据的处理问题。对于这一类问题，在面试的时候，也是经常会问到的。由于这类问题复杂度比较高，很少让当场就写代码，但是基本上会让你说一个思路，或者写写伪代码。想要解决海量数据的问题，你会的就不能只是基础的数据结构和算法了，你需要综合应用。如果平时没有想过这部分问题，临时被问，肯定会懵。 在专栏里，王争老师列举了大量这类问题，你要重点思考这类问题背后的思路，然后平时自己处理问题的时候，也多想想，如果这个问题数据量大的话，应该怎么办。这样多思考，面试的时候，思路很容易就来了。 比如，我这里随便列了几个，都是很经典的问题。你要是想不起来，就赶紧去复习吧！ 比如说，我们有 10GB 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。这个时候该怎么办呢？ 如果你所在的省有 50 万考生，如何通过成绩快速排序得出名次呢？ 假设我们有 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，你有什么比较快速的排序方法呢？ 假设我们有 1000 万个整型数据，每个数据占 8 个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这 1000 万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过 100MB，你会怎么做呢？ 第四部分，工业实践部分。在每种数据结构的讲解中，老师会重点分析一些这些数据结构在工业上的实践，封装在库里面的，一般人不注意的。 我看王争老师也是个代码分析控。一般同学可能遇到问题，查一查有没有开源软件或者现成的库，可以用就完了。而王争老师会研究底层代码的实现，解析为什么这些在工业中大量使用的库，应该这样实现。这部分不但对于面试有帮助，对于实际开发也有很大的帮助。普通程序员和高手的差距，就是一个用完了就完了，一个用完了要看看为啥这样用。 例如，老师解析了 Glibc 中的 qsort() 函数，Java 中的 HashMap 如何实现工业级的散列表，Redis 中的有序集合（Sorted Set）的实现，工程上使用的红黑树等等。 尤其是对于哈希算法，老师解析了安全加密、数据校验、唯一标识、散列函数，负载均衡、数据分片、分布式存储等应用。如果你同时订阅了架构、微服务的课程，你会发现这些算法在目前最火的架构设计中，都有使用。 师傅领进门，修行在个人。尽管老师只是解析了其中一部分，但是咱们在平时使用开源软件和库的时候，也要多问个为什么。写完了程序，看看官方文档，看看原理解析的书，看看源代码，然后映射到算法与数据结构中，你会发现，这些知识和思路到处都在使用。 最后，我还想说一句，坚持，别放弃，啃下来。基础越扎实，路走得越远，走得越宽。加油！ 总结课 | 在实际开发中，如何权衡选择使用哪种数据结构和算法？王争 2019-02-18  00:00 1.25x 讲述：冯永吉 大小：9.47M 时长：10:20 你好，我是王争，今天是一篇总结课。我们学了这么多数据结构和算法，在实际开发中，究竟该如何权衡选择使用哪种数据结构和算法呢？今天我们就来聊一聊这个问题，希望能帮你把学习带回实践中。 我一直强调，学习数据结构和算法，不要停留在学院派的思维中，只把算法当作应付面试、考试或者竞赛的花拳绣腿。作为软件开发工程师，我们要把数据结构和算法，应用到软件开发中，解决实际的开发问题。 不过，要想在实际的开发中，灵活、恰到好处地应用数据结构和算法，需要非常深厚的实战经验积累。尽管我在课程中，一直都结合实际的开发场景来讲解，希望带你真枪实弹地演练算法如何解决实际的问题。但是，在今后的软件开发中，你要面对的问题远比我讲的场景要复杂、多变、不确定。 要想游刃有余地解决今后你要面对的问题，光是熟知每种数据结构和算法的功能、特点、时间空间复杂度，还是不够的。毕竟工程上的问题不是算法题。算法题的背景、条件、限制都非常明确，我们只需要在规定的输入、输出下，找最优解就可以了。 而工程上的问题往往都比较开放，在选择数据结构和算法的时候，我们往往需要综合各种因素，比如编码难度、维护成本、数据特征、数据规模等，最终选择一个工程的最合适解，而非理论上的最优解。 为了让你能做到活学活用，在实际的软件开发中，不生搬硬套数据结构和算法，今天，我们就聊一聊，在实际的软件开发中，如何权衡各种因素，合理地选择使用哪种数据结构和算法？关于这个问题，我总结了六条经验。 1. 时间、空间复杂度不能跟性能划等号我们在学习每种数据结构和算法的时候，都详细分析了算法的时间复杂度、空间复杂度，但是，在实际的软件开发中，复杂度不能与性能简单划等号，不能表示执行时间和内存消耗的确切数据量。为什么这么说呢？原因有下面几点。 复杂度不是执行时间和内存消耗的精确值 在用大 O 表示法表示复杂度的时候，我们会忽略掉低阶、常数、系数，只保留高阶，并且它的度量单位是语句的执行频度。每条语句的执行时间，并非是相同、确定的。所以，复杂度给出的只能是一个非精确量值的趋势。 代码的执行时间有时不跟时间复杂度成正比 我们常说，时间复杂度是 O(nlogn) 的算法，比时间复杂度是 O(n^2) 的算法，执行效率要高。这样说的一个前提是，算法处理的是大规模数据的情况。对于小规模数据的处理，算法的执行效率并不一定跟时间复杂度成正比，有时还会跟复杂度成反比。 对于处理不同问题的不同算法，其复杂度大小没有可比性 复杂度只能用来表征不同算法，在处理同样的问题，以及同样数据类型的情况下的性能表现。但是，对于不同的问题、不同的数据类型，不同算法之间的复杂度大小并没有可比性。 2. 抛开数据规模谈数据结构和算法都是“耍流氓”在平时的开发中，在数据规模很小的情况下，普通算法和高级算法之间的性能差距会非常小。如果代码执行频率不高、又不是核心代码，这个时候，我们选择数据结构和算法的主要依据是，其是否简单、容易维护、容易实现。大部分情况下，我们直接用最简单的存储结构和最暴力的算法就可以了。 比如，对于长度在一百以内的字符串匹配，我们直接使用朴素的字符串匹配算法就够了。如果用 KMP、BM 这些更加高效的字符串匹配算法，实际上就大材小用了。因为这对于处理时间是毫秒量级敏感的系统来说，性能的提升并不大。相反，这些高级算法会徒增编码的难度，还容易产生 bug。 3. 结合数据特征和访问方式来选择数据结构面对实际的软件开发场景，当我们掌握了基础数据结构和算法之后，最考验能力的并不是数据结构和算法本身，而是对问题需求的挖掘、抽象、建模。如何将一个背景复杂、开放的问题，通过细致的观察、调研、假设，理清楚要处理数据的特征与访问方式，这才是解决问题的重点。只有理清楚了这些东西，我们才能将问题转化成合理的数据结构模型，进而找到满足需求的算法。 比如我们前面讲过，Trie 树这种数据结构是一种非常高效的字符串匹配算法。但是，如果你要处理的数据，并没有太多的前缀重合，并且字符集很大，显然就不适合利用 Trie 树了。所以，在用 Trie 树之前，我们需要详细地分析数据的特点，甚至还要写些分析代码、测试代码，明确要处理的数据是否适合使用 Trie 树这种数据结构。 再比如，图的表示方式有很多种，邻接矩阵、邻接表、逆邻接表、二元组等等。你面对的场景应该用哪种方式来表示，具体还要看你的数据特征和访问方式。如果每个数据之间联系很少，对应到图中，就是一个稀疏图，就比较适合用邻接表来存储。相反，如果是稠密图，那就比较适合采用邻接矩阵来存储。 4. 区别对待 IO 密集、内存密集和计算密集如果你要处理的数据存储在磁盘，比如数据库中。那代码的性能瓶颈有可能在磁盘 IO，而并非算法本身。这个时候，你需要合理地选择数据存储格式和存取方式，减少磁盘 IO 的次数。 比如我们在递归那一节讲过最终推荐人的例子。你应该注意到了，当时我给出的代码尽管正确，但其实并不高效。如果某个用户是经过层层推荐才来注册的，那我们获取他的最终推荐人的时候，就需要多次访问数据库，性能显然就不高了。 不过，这个问题解决起来不难。我们知道，某个用户的最终推荐人一旦确定，就不会变动。所以，我们可以离线计算每个用户的最终推荐人，并且保存在表中的某个字段里。当我们要查看某个用户的最终推荐人的时候，访问一次数据库就可以获取到。 刚刚我们讲了数据存储在磁盘的情况，现在我们再来看下，数据存储在内存中的情况。如果你的数据是存储在内存中，那我们还需要考虑，代码是内存密集型的还是 CPU 密集型的。 所谓 CPU 密集型，简单点理解就是，代码执行效率的瓶颈主要在 CPU 执行的效率。我们从内存中读取一次数据，到 CPU 缓存或者寄存器之后，会进行多次频繁的 CPU 计算（比如加减乘除），CPU 计算耗时占大部分。所以，在选择数据结构和算法的时候，要尽量减少逻辑计算的复杂度。比如，用位运算代替加减乘除运算等。 所谓内存密集型，简单点理解就是，代码执行效率的瓶颈在内存数据的存取。对于内存密集型的代码，计算操作都比较简单，比如，字符串比较操作，实际上就是内存密集型的。每次从内存中读取数据之后，我们只需要进行一次简单的比较操作。所以，内存数据的读取速度，是字符串比较操作的瓶颈。因此，在选择数据结构和算法的时候，需要考虑是否能减少数据的读取量，数据是否在内存中连续存储，是否能利用 CPU 缓存预读。 5. 善用语言提供的类，避免重复造轮子实际上，对于大部分常用的数据结构和算法，编程语言都提供了现成的类和函数实现。比如，Java 中的 HashMap 就是散列表的实现，TreeMap 就是红黑树的实现等。在实际的软件开发中，除非有特殊的要求，我们都可以直接使用编程语言中提供的这些类或函数。 这些编程语言提供的类和函数，都是经过无数验证过的，不管是正确性、鲁棒性，都要超过你自己造的轮子。而且，你要知道，重复造轮子，并没有那么简单。你需要写大量的测试用例，并且考虑各种异常情况，还要团队能看懂、能维护。这显然是一个出力不讨好的事情。这也是很多高级的数据结构和算法，比如 Trie 树、跳表等，在工程中，并不经常被应用的原因。 但这并不代表，学习数据结构和算法是没用的。深入理解原理，有助于你能更好地应用这些编程语言提供的类和函数。能否深入理解所用工具、类的原理，这也是普通程序员跟技术专家的区别。 6. 千万不要漫无目的地过度优化掌握了数据结构和算法这把锤子，不要看哪里都是钉子。比如，一段代码执行只需要 0.01 秒，你非得用一个非常复杂的算法或者数据结构，将其优化成 0.005 秒。即便你的算法再优秀，这种微小优化的意义也并不大。相反，对应的代码维护成本可能要高很多。 不过度优化并不代表，我们在软件开发的时候，可以不加思考地随意选择数据结构和算法。我们要学会估算。估算能力实际上也是一个非常重要的能力。我们不仅要对普通情况下的数据规模和性能压力做估算，还需要对异常以及将来一段时间内，可能达到的数据规模和性能压力做估算。这样，我们才能做到未雨绸缪，写出来的代码才能经久可用。 还有，当你真的要优化代码的时候，一定要先做 Benchmark 基准测试。这样才能避免你想当然地换了一个更高效的算法，但真实情况下，性能反倒下降了。 总结工程上的问题，远比课本上的要复杂。所以，我今天总结了六条经验，希望你能把数据结构和算法用在刀刃上，恰当地解决实际问题。 我们在利用数据结构和算法解决问题的时候，一定要先分析清楚问题的需求、限制、隐藏的特点等。只有搞清楚了这些，才能有针对性地选择恰当的数据结构和算法。这种灵活应用的实战能力，需要长期的刻意锻炼和积累。这是一个有经验的工程师和一个学院派的工程师的区别。 好了，今天的内容就到这里了。最后，我想听你谈一谈，你在实际开发选择数据结构和算法时，有什么感受和方法呢？ 欢迎在留言区写下你的想法，也欢迎你把今天的文章分享给你的朋友，帮助他在数据结构和算法的实际运用中走得更远。 《数据结构与算法之美》学习指导手册王争 2019-04-22 你好，我是王争。 在设计专栏内容的时候，为了兼顾不同基础的同学，我在内容上做到了难易结合，既有简单的数组、链表、栈、队列这些基础内容，也有红黑树、BM、KMP 这些难度较大的算法。但是，对于初学者来说，一下子面对这么多知识，可能还是比较懵。 我觉得，对于初学者来说，先把最简单、最基础、最重要的知识点掌握好，再去研究难度较高、更加高级的知识点，这样由易到难、循序渐进的学习路径，无疑是最合理的。 基于这个路径，我对专栏内容，重新做了一次梳理，希望给你一份具体、明确、有效的学习指导。我会写清楚每个知识点的难易程度、需要你掌握到什么程度、具体如何来学习。 如果你是数据结构和算法的初学者，或者你觉得自己的基础比较薄弱，希望这份学习指导，能够让你学起来能更加有的放矢，能把精力、时间花在刀刃上，获得更好的学习效果。 下面，我先给出一个大致的学习路线。 （建议保存后查看大图） 现在，针对每个知识点，我再给你逐一解释一下。我这里先说明一下，下面标记的难易程度、是否重点、掌握程度，都只是针对初学者来说的，如果你已经有一定基础，可以根据自己的情况，安排自己的学习。 1. 复杂度分析尽管在专栏中，我只用了两节课的内容，来讲复杂度分析这个知识点。但是，我想说的是，它真的非常重要。你必须要牢牢掌握这两节，基本上要做到，简单代码能很快分析出时间、空间复杂度；对于复杂点的代码，比如递归代码，你也要掌握专栏中讲到的两种分析方法：递推公式和递归树。 对于初学者来说，光看入门篇的两节复杂度分析文章，可能还不足以完全掌握复杂度分析。不过，在后续讲解每种数据结构和算法的时候，我都有详细分析它们的时间、空间复杂度。所以，你可以在学习专栏中其他章节的时候，再不停地、有意识地去训练自己的复杂度分析能力。 难易程度：Medium 是否重点：10 分 掌握程度：在不看我的分析的情况下，能自行分析专栏中大部分数据结构和算法的时间、空间复杂度 2. 数组、栈、队列这一部分内容非常简单，初学者学起来也不会很难。但是，作为基础的数据结构，数组、栈、队列，是后续很多复杂数据结构和算法的基础，所以，这些内容你一定要掌握。 难易程度：Easy 是否重点：8 分 掌握程度：能自己实现动态数组、栈、队列 3. 链表链表非常重要！虽然理论内容不多，但链表上的操作却很复杂。所以，面试中经常会考察，你一定要掌握。而且，我这里说“掌握”不只是能看懂专栏中的内容，还能将专栏中提到的经典链表题目，比如链表反转、求中间结点等，轻松无 bug 地实现出来。 难易程度：Medium 是否重点：9 分 掌握程度：能轻松写出经典链表题目代码 4. 递归对于初学者来说，递归代码非常难掌握，不管是读起来，还是写起来。但是，这道坎你必须要跨过，跨不过就不能算是入门数据结构和算法。我们后面讲到的很多数据结构和算法的代码实现，都要用到递归。 递归相关的理论知识也不多，所以还是要多练。你可以先在网上找些简单的题目练手，比如斐波那契数列、求阶乘等，然后再慢慢过渡到更加有难度的，比如归并排序、快速排序、二叉树的遍历、求高度，最后是回溯八皇后、背包问题等。 难易程度：Hard 是否重点：10 分 掌握程度：轻松写出二叉树遍历、八皇后、背包问题、DFS 的递归代码 5. 排序、二分查找这一部分并不难，你只需要能看懂我专栏里的内容即可。 难易程度：Easy 是否重点：7 分 掌握程度：能自己把各种排序算法、二分查找及其变体代码写一遍就可以了 6. 跳表对于初学者来说，并不需要非得掌握跳表，所以，如果没有精力，这一章节可以先跳过。 难易程度：Medium 是否重点：6 分 掌握程度：初学者可以先跳过。如果感兴趣，看懂专栏内容即可，不需要掌握代码实现 7. 散列表尽管散列表的内容我讲了很多，有三节课。但是，总体上来讲，这块内容理解起来并不难。但是，作为一种应用非常广泛的数据结构，你还是要掌握牢固散列表。 难易程度：Medium 是否重点：8 分 掌握程度：对于初学者来说，自己能代码实现一个拉链法解决冲突的散列表即可 8. 哈希算法这部分纯粹是为了开拓思路，初学者可以略过。 难易程度：Easy 是否重点：3 分 掌握程度：可以暂时不看 9. 二叉树这一部分非常重要！二叉树在面试中经常会被考到，所以要重点掌握。但是我这里说的二叉树，并不包含专栏中红黑树的内容。红黑树我们待会再讲。 难易程度：Medium 是否重点：9 分 掌握程度：能代码实现二叉树的三种遍历算法、按层遍历、求高度等经典二叉树题目 10. 红黑树对于初学者来说，这一节课完全可以不看。 难易程度：Hard 是否重点：3 分 掌握程度：初学者不用把时间浪费在上面 11. B+ 树虽然 B+ 树也算是比较高级的一种数据结构了，但是对初学者来说，也不是重点。有时候面试的时候还是会问的，所以这一部分内容，你能看懂专栏里的讲解就可以了。 难易程度：Medium 是否重点：5 分 掌握程度：可看可不看 12. 堆与堆排序这一部分内容不是很难，初学者也是要掌握的。 难易程度：Medium 是否重点：8 分 掌握程度：能代码实现堆、堆排序，并且掌握堆的三种应用（优先级队列、Top k、中位数） 13. 图的表示图的内容很多，但是初学者不需要掌握那么多。一般 BAT 等大厂面试，不怎么会面试有关图的内容，因为面试官可能也对这块不会很熟悉哈：）。但是，最基本图的概念、表示方法还是要掌握的。 难易程度：Easy 是否重点：8 分 掌握程度：理解图的三种表示方法（邻接矩阵、邻接表、逆邻接表），能自己代码实现 14. 深度广度优先搜索这算是图上最基础的遍历或者说是搜索算法了，所以还是要掌握一下。这两种算法的原理都不难哈，但是代码实现并不简单，一个用到了队列，另一个用到了递归。对于初学者来说，看懂这两个代码实现就是一个挑战！可以等到其他更重要的内容都掌握之后，再来挑战，也是可以的。 难易程度：Hard 是否重点：8 分 掌握程度：能代码实现广度优先、深度优先搜索算法 15. 拓扑排序、最短路径、A* 算法这几个算法稍微高级点。如果你能轻松实现深度、广度优先搜索，那看懂这三个算法不成问题。不过，这三种算法不是重点。面试不会考的。 难易程度：Hard 是否重点：5 分 掌握程度：有时间再看，暂时可以不看 16. 字符串匹配（BF、RK）BF 非常简单，RK 稍微复杂点，但都不难。这个最好还是掌握下。 难易程度：Easy 是否重点：7 分 掌握程度：能实践 BF 算法，能看懂 RK 算法 17. 字符串匹配（BM、KMP、AC 自动机）这三个算法都挺难的，对于算法有一定基础的人来说，看懂也不容易。所以，对于初学者来说，千万别浪费时间在这上面。即便有余力，看懂就好了，不用非得能自己实现。 难易程度：Hard 是否重点：3 分 掌握程度：初学者不用把时间浪费在上面 18. 字符串匹配（Trie）这个还是要能看懂，不过不需要能代码实现。有些面试官喜欢考这个东西，主要是结合应用场景来考察，只是看你知不知道要用 Trie 树这个东西。 难易程度：Medium 是否重点：7 分 掌握程度：能看懂，知道特点、应用场景即可，不要求代码实现 19. 位图位图不是重点，如果有余力最好掌握一下。 难易程度：Easy 是否重点：6 分 掌握程度：看懂即可，能自己实现一个位图结构最好 20. 四种算法思想这个是重点，也是难点。贪心、分治、回溯、动态规划，每一个都不简单，其中动态规划又是最难、最烧脑的。要应付 FLAG 这样公司的面试，必须拿下这块内容。但是呢，学习要循序渐进，这块内容的学习可以放到最后，做个长时间的学习计划来攻克。 这块内容理论的东西不多，要想真的掌握，还是要大量刷题。 难易程度：Hard 是否重点：10 分 掌握程度：可以放到最后，但是一定要掌握！做到能实现 Leetcode 上 Medium 难度的题目 学而时习之，专栏虽然已经结束，但是学习的同学和留言依旧源源不断。希望这份学习指导手册对你有帮助，也欢迎你继续给我留言，和大家一起交流、学习、进步。 春节7天练 | Day 1：数组和链表王争 2019-02-04 讲述：冯永吉 大小：882.20K 时长：00:55 你好，我是王争。首先祝你新年快乐！ 专栏的正文部分已经结束，相信这半年的时间，你学到了很多，究竟学习成果怎样呢？ 我整理了数据结构和算法中必知必会的 30 个代码实现，从今天开始，分 7 天发布出来，供你复习巩固所用。你可以每天花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。 除此之外，@Smallfly 同学还整理了一份配套的 LeetCode 练习题，你也可以一起练习一下。在此，我谨代表我本人对 @Smallfly 表示感谢！ 另外，我还为假期坚持学习的同学准备了丰厚的春节加油礼包。 2 月 5 日 -2 月 14 日，只要在专栏文章下的留言区写下你的答案，参与答题，并且留言被精选，即可获得极客时间 10 元无门槛优惠券。 7 篇中的所有题目，只要回答正确 3 道及以上，即可获得极客时间 99 元专栏通用阅码。 如果 7 天连续参与答题，并且每天的留言均被精选，还可额外获得极客时间价值 365 元的每日一课年度会员。 关于数组和链表的几个必知必会的代码实现 数组 实现一个支持动态扩容的数组 实现一个大小固定的有序数组，支持动态增删改操作 实现两个有序数组合并为一个有序数组 链表 实现单链表、循环链表、双向链表，支持增删操作 实现单链表反转 实现两个有序的链表合并为一个有序链表 实现求链表的中间结点 对应的 LeetCode 练习题（@Smallfly 整理） 数组 Three Sum（求三数之和） 英文版：https://leetcode.com/problems/3sum/ 中文版：https://leetcode-cn.com/problems/3sum/ Majority Element（求众数） 英文版：https://leetcode.com/problems/majority-element/ 中文版：https://leetcode-cn.com/problems/majority-element/ Missing Positive（求缺失的第一个正数） 英文版：https://leetcode.com/problems/first-missing-positive/ 中文版：https://leetcode-cn.com/problems/first-missing-positive/ 链表 Linked List Cycle I（环形链表） 英文版：https://leetcode.com/problems/linked-list-cycle/ 中文版：https://leetcode-cn.com/problems/linked-list-cycle/ Merge k Sorted Lists（合并 k 个排序链表） 英文版：https://leetcode.com/problems/merge-k-sorted-lists/ 中文版：https://leetcode-cn.com/problems/merge-k-sorted-lists/ 做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友，说不定就帮他解决了一个难题。 祝你取得好成绩！明天见！ 春节7天练 | Day 2：栈、队列和递归王争 2019-02-05 你好，我是王争。初二好！ 为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的 30 个代码实现，分 7 天发布出来，供你复习巩固所用。今天是第二篇。 和昨天一样，你可以花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。 关于栈、队列和递归的几个必知必会的代码实现栈 用数组实现一个顺序栈 用链表实现一个链式栈 编程模拟实现一个浏览器的前进、后退功能 队列 用数组实现一个顺序队列 用链表实现一个链式队列 实现一个循环队列 递归 编程实现斐波那契数列求值 f(n)=f(n-1)+f(n-2) 编程实现求阶乘 n! 编程实现一组数据集合的全排列 对应的 LeetCode 练习题（@Smallfly 整理） 栈 Valid Parentheses（有效的括号） 英文版：https://leetcode.com/problems/valid-parentheses/ 中文版：https://leetcode-cn.com/problems/valid-parentheses/ Longest Valid Parentheses（最长有效的括号） 英文版：https://leetcode.com/problems/longest-valid-parentheses/ 中文版：https://leetcode-cn.com/problems/longest-valid-parentheses/ Evaluate Reverse Polish Notatio（逆波兰表达式求值） 英文版：https://leetcode.com/problems/evaluate-reverse-polish-notation/ 中文版：https://leetcode-cn.com/problems/evaluate-reverse-polish-notation/ 队列 Design Circular Deque（设计一个双端队列） 英文版：https://leetcode.com/problems/design-circular-deque/ 中文版：https://leetcode-cn.com/problems/design-circular-deque/ Sliding Window Maximum（滑动窗口最大值） 英文版：https://leetcode.com/problems/sliding-window-maximum/ 中文版：https://leetcode-cn.com/problems/sliding-window-maximum/ 递归 Climbing Stairs（爬楼梯） 英文版：https://leetcode.com/problems/climbing-stairs/ 中文版：https://leetcode-cn.com/problems/climbing-stairs/ 昨天的第一篇，是关于数组和链表的，如果你错过了，点击文末的“上一篇”，即可进入测试。 祝你取得好成绩！明天见！ 春节7天练 | Day 3：排序和二分查找王争 2019-02-06 你好，我是王争。初三好！ 为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的 30 个代码实现，分 7 天发布出来，供你复习巩固所用。今天是第三篇。 和昨天一样，你可以花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。 前两天的内容，是关于数组和链表、排序和二分查找的。如果你错过了，点击文末的“上一篇”，即可进入测试。 关于排序和二分查找的几个必知必会的代码实现排序 实现归并排序、快速排序、插入排序、冒泡排序、选择排序 编程实现 O(\bn) 时间复杂度内找到一组数据的第 K 大元素 二分查找 实现一个有序数组的二分查找算法 实现模糊二分查找算法（比如大于等于给定值的第一个元素） 对应的 LeetCode 练习题（@Smallfly 整理）Sqrt(x) （x 的平方根） 英文版：https://leetcode.com/problems/sqrtx/ 中文版：https://leetcode-cn.com/problems/sqrtx/ 做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友，说不定就帮他解决了一个难题。 祝你取得好成绩！明天见！ 春节7天练 | Day 4：散列表和字符串王争 2019-02-08 你好，我是王争。初四好！ 为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的 30 个代码实现，分 7 天发布出来，供你复习巩固所用。今天是第四篇。 和昨天一样，你可以花一点时间，来完成测验。测验完成后，你可以根据结果，回到相应章节，有针对性地进行复习。 前几天的内容。如果你错过了，点击文末的“上一篇”，即可进入测试。 关于散列表和字符串的 4 个必知必会的代码实现散列表 实现一个基于链表法解决冲突问题的散列表 实现一个 LRU 缓存淘汰算法 字符串 实现一个字符集，只包含 a～z 这 26 个英文字母的 Trie 树 实现朴素的字符串匹配算法 对应的 LeetCode 练习题（@Smallfly 整理）字符串 Reverse String （反转字符串） 英文版：https://leetcode.com/problems/reverse-string/ 中文版：https://leetcode-cn.com/problems/reverse-string/ Reverse Words in a String（翻转字符串里的单词） 英文版：https://leetcode.com/problems/reverse-words-in-a-string/ 中文版：https://leetcode-cn.com/problems/reverse-words-in-a-string/ String to Integer (atoi)（字符串转换整数 (atoi)） 英文版：https://leetcode.com/problems/string-to-integer-atoi/ 中文版：https://leetcode-cn.com/problems/string-to-integer-atoi/ 做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友，说不定就帮他解决了一个难题。 祝你取得好成绩！明天见！ 春节7天练 | Day 5：二叉树和堆王争 2019-02-09 你好，我是王争。春节假期进入尾声了。你现在是否已经准备返回工作岗位了呢？今天更新的是测试题的第五篇，我们继续来复习。 关于二叉树和堆的 7 个必知必会的代码实现二叉树 实现一个二叉查找树，并且支持插入、删除、查找操作 实现查找二叉查找树中某个节点的后继、前驱节点 实现二叉树前、中、后序以及按层遍历 堆 实现一个小顶堆、大顶堆、优先级队列 实现堆排序 利用优先级队列合并 K 个有序数组 求一组动态数据集合的最大 Top K 对应的 LeetCode 练习题（@Smallfly 整理）Invert Binary Tree（翻转二叉树） 英文版：https://leetcode.com/problems/invert-binary-tree/ 中文版：https://leetcode-cn.com/problems/invert-binary-tree/ Maximum Depth of Binary Tree（二叉树的最大深度） 英文版：https://leetcode.com/problems/maximum-depth-of-binary-tree/ 中文版：https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/ Validate Binary Search Tree（验证二叉查找树） 英文版：https://leetcode.com/problems/validate-binary-search-tree/ 中文版：https://leetcode-cn.com/problems/validate-binary-search-tree/ Path Sum（路径总和） 英文版：https://leetcode.com/problems/path-sum/ 中文版：https://leetcode-cn.com/problems/path-sum/ 做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友。 祝你取得好成绩！明天见！ 春节7天练 | Day 6：图王争 2019-02-10 你好，我是王争。初六好！ 为了帮你巩固所学，真正掌握数据结构和算法，我整理了数据结构和算法中，必知必会的 30 个代码实现，分 7 天发布出来，供你复习巩固所用。今天是第六篇。 和之前一样，你可以花一点时间，来手写这些必知必会的代码。写完之后，你可以根据结果，回到相应章节，有针对性地进行复习。做到这些，相信你会有不一样的收获。 关于图的几个必知必会的代码实现图 实现有向图、无向图、有权图、无权图的邻接矩阵和邻接表表示方法 实现图的深度优先搜索、广度优先搜索 实现 Dijkstra 算法、A* 算法 实现拓扑排序的 Kahn 算法、DFS 算法 对应的 LeetCode 练习题（@Smallfly 整理）Number of Islands（岛屿的个数） 英文版：https://leetcode.com/problems/number-of-islands/description/ 中文版：https://leetcode-cn.com/problems/number-of-islands/description/ Valid Sudoku（有效的数独） 英文版：https://leetcode.com/problems/valid-sudoku/ 中文版：https://leetcode-cn.com/problems/valid-sudoku/ 做完题目之后，你可以点击“请朋友读”，把测试题分享给你的朋友，说不定就帮他解决了一个难题。 祝你取得好成绩！明天见！ 春节7天练 | Day 7：贪心、分治、回溯和动态规划王争 2019-02-10 你好，我是王争。今天是节后的第一个工作日，也是我们“春节七天练”的最后一篇。 几种算法思想必知必会的代码实现回溯 利用回溯算法求解八皇后问题 利用回溯算法求解 0-1 背包问题 分治 利用分治算法求一组数据的逆序对个数 动态规划 0-1 背包问题 最小路径和（详细可看 @Smallfly 整理的 Minimum Path Sum） 编程实现莱文斯坦最短编辑距离 编程实现查找两个字符串的最长公共子序列 编程实现一个数据序列的最长递增子序列 对应的 LeetCode 练习题（@Smallfly 整理）Regular Expression Matching（正则表达式匹配） 英文版：https://leetcode.com/problems/regular-expression-matching/ 中文版：https://leetcode-cn.com/problems/regular-expression-matching/ Minimum Path Sum（最小路径和） 英文版：https://leetcode.com/problems/minimum-path-sum/ 中文版：https://leetcode-cn.com/problems/minimum-path-sum/ Coin Change （零钱兑换） 英文版：https://leetcode.com/problems/coin-change/ 中文版：https://leetcode-cn.com/problems/coin-change/ Best Time to Buy and Sell Stock（买卖股票的最佳时机） 英文版：https://leetcode.com/problems/best-time-to-buy-and-sell-stock/ 中文版：https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/ Maximum Product Subarray（乘积最大子序列） 英文版：https://leetcode.com/problems/maximum-product-subarray/ 中文版：https://leetcode-cn.com/problems/maximum-product-subarray/ Triangle（三角形最小路径和） 英文版：https://leetcode.com/problems/triangle/ 中文版：https://leetcode-cn.com/problems/triangle/ 到此为止，七天的练习就结束了。这些题目都是我精选出来的，是基础数据结构和算法中最核心的内容。建议你一定要全部手写练习。如果一遍搞不定，你可以结合前面的章节，多看几遍，反复练习，直到能够全部搞定为止。 学习数据结构和算法，最好的方法就是练习和实践。我相信这在任何知识的学习过程中都适用。 最后，祝你工作顺利！学业进步！ 【用户故事】 | Jerry银银：这一年我的脑海里只有算法Jerry银银 2019-02-13 比尔·盖茨曾说过：“如果你自以为是一个很好的程序员，请去读读 Donald E. Knuth 的《计算机程序设计艺术》吧……要是你真把它读下来了，就毫无疑问可以给我递简历了。”虽然比尔·盖茨推荐的是《计算机程序设计艺术》这本书，但是本质却折射出了算法的重要性。 大家好，我是 Jerry 银银，购买过算法专栏的同学应该时不时会看到我的留言！目前我是一名 Android 应用开发工程师，主要从事移动互联网教育软件的研发，坐标上海。 我为何要学算法？细细想来，从毕业到现在，7 年多的时间，我的脑海里一直没有停止过思考这样一个问题：技术人究竟能够走多远，技术人的路究竟该如何走下去？相信很多技术人应该有同样的感受，因为技术的更新迭代实在是太快了，但是我心里明白：我得为长远做打算，否则，就算换公司、换工作，可能本质也不会有什么改变。 但是，我其实不太清楚自己到底应该往什么地方努力。于是，我翻阅了好多书籍，搜寻 IT 领域各种牛人的观点。多方比较之后，我终于决定，从基础开始，从计算机领域最基础、最重要的一门课开始。毫无疑问，这门课就是数据结构和算法。 我是如何遇见极客时间的？既然找到了方向，那就开始吧。可是问题来了，从哪儿开始呢？大方向虽然有了，可是具体的实现细节还是得慢慢摸索。大学没怎么学，工作这么多年也没有刻意练习，起初我还真不知道从哪儿开始，只是买了本书，慢慢地啃，也找了一些简单的题目开始做。有过自学经历的同学，应该有同感吧？刚开始连单链表翻转这样简单的题都要折腾半天，真心觉得“痛苦”。 之前我在极客时间上订阅过“Java 核心技术 36 讲”，体会到了专栏和书本的不同。极客时间的专栏作者都是有着丰富的一线开发经验，能很好地把知识和实战结合在一起的大牛。这些课听起来非常爽。估计你应该经常跟我一样感叹：“哦！原来这些知识还可以这么使用！”当时我就在想，极客时间啥时候有一门算法课就好了。 说来真是巧，没多久，极客时间就推出了“数据结构与算法之美”。我试读了《为什么要学习数据结构和算法》和《数组：为什么很多编程语言中数组都从 0 开始编号？》这两篇之后，立即购买了。 到现在，专栏学完了，但是我依然记得，王争老师在《为什么要学习数据结构和算法》这篇文章里面提到的三句话，因为这每一句话都刺痛了我的小心脏！ 第一句：业务开发工程师，你真的愿意做一辈子 CRUD Boy 吗？ 第二句：基础架构研发工程师，写出达到开源水平的框架才是你的目标！ 第三句：对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！ 我每天是怎么学专栏的？于是，每天早上醒来，我的第一件事就是听专栏！专栏在每周的一、三、五更新，每周的这三天早上，我会听更新的文章。其它时间，我就听老的文章，当作复习。 听的过程，我一般会分这么几种情况。 第一种情况，更新的内容是我之前就已经学过的，基本已经掌握了的。这种情况下，听起来相对轻松点，基本上听一遍就够了。起床之后，再做一下老师给的思考题。这种情况在专栏的基础部分出现得比较多，像数组、链表、栈、队列、哈希表这些章节，我基本上都是这么过来的。 第二种情况，更新的内容是我学过的，但是还不太精通的。这种情况下，王争老师讲的内容都会将我的认知往前“推进”一步。顺利的话，我会在上班之前就搞懂今天更新的内容。这种情况是曾经没有接触过的内容，但是整体来说不难的理解的，比如跳表、递归等。 还有一种情况，就是听一遍不够，听完再看一遍也不行，上午上班之前也搞不定的。不过，我也不会急躁。我心里知道，我可能需要换换脑子，说不定，在上午工作期间，灵感会突然冒出来。这种情况一般出现在红黑树、字符串查找算法、动态规划这些章节。 到了中午休息时间，我会一个人在公司楼下转一圈，同样，还是听专栏、看专栏。 如果今天的文章，早上已经搞定了，我会重新看下其他同学的留言，看看其他同学是如何思考文章的课后思考题的，还有就是，我会看看其他同学学习过程中，会有哪些疑问，这些疑问自己曾经是否遇到过，现在是否已经完全解决了。 如果今天的文章，早上没有彻底搞懂，这种情况下，我会极力利用中午的时间去思考。 晚上的时间通常无法确定，我有时候会加班到很晚，回到家，再去啃算法，效率也不高。所以，我一般会在晚上“看”算法。为什么我会用双引号呢？是因为我真得只是“看”，目的就是加深印象。 以上基本是我工作日学专栏的“套路”。 等到了周末或者其它节假日，就是“打攻坚战”的时候了。估计很多上班族和我一样，只有周末才有大量集中思考的时间。这时候，我一般会通过做题来反向推动自己的算法学习。 像红黑树、Trie 树、递归、动态规划这些内容，我都是在周末和节假日搞懂的。虽然到现在对其中一些知识还不能达到游刃有余的地步，但是对一般的问题，大体上我都知道该如何抽象、如何拆解了。 我在学习算法时记的笔记 通过学习专栏，我有什么不一样的收获？首先，专栏学习拓宽了我的知识面。例如，很多书本不讲的跳表，王争老师用了一篇文章来讲解。犹记得当我看完跳表时，心想，这么简单、易懂、高效的数据结构，为什么很多书籍都没有呢？这个专栏真的买值了！ 其次，专栏的理论和实践结合很强。书籍是通用性很强的教材，一般很少会涉及软件系统是如何使用具体的数据结构和算法的。在专栏中，老师把对应的知识和实践相互结合，听起来特别过瘾！比如堆这种数据结构，理解起来不难，但是要用好它，还得下点功夫，经过老师一讲解，搭配音频，我的理解也变得更加深入了。 最后，专栏留言这个功能真的太好了，为自学带来了诸多便利，也让我获得了很多正向反馈。很多时候，经过相当长的一段时间思考，还是不能打通任督二脉，其实后来回想，当时就差那一层窗户纸了。于是，我在文末留下了自己的疑问，结果王争老师轻描淡写一句话我就明白了。 留言功能还有个非常大的好处。如果你用心学习，用心思考，用心留言，你的留言很大概率会被同伴点赞，很多时候还能被置顶。这本身就是一种正向反馈，也会更加促进自己的学习动力。还有一种更爽的体验，突然有一天早上，我照例醒来听专栏，突然听到了自己的名字。这个专栏 4 万多人订阅，老师居然记得我！可见王争老师真的认真看了每一条留言。 最后，我总结下自己学这个专栏的收获。尽管很多，但是我想用三句话来概括。 第一，写代码的时候，我会很自然地从时间和空间角度去衡量代码的优劣，时间、空间意识被加强了很多。 第二，学习算法的过程，有很多的“痛苦”，也正是因为这些“痛苦”，我学到了很多知识以外的东西。 第三，过程可能比知识更重要。要从过程中体会成长和精进的乐趣，而知识是附加产品！ 专栏虽然结束，但是学习并没有结束。同学们，我们开头见！ 【用户故事 】| zixuan：站在思维的高处，才有足够的视野和能力欣赏“美”zixuan 2019-02-15  00:12 1.25x 讲述：修阳 大小：11.53M 时长：12:35 大家好，我是 zixuan，在一家国内大型互联网公司做后端开发，坐标深圳，工作 5 年多了。今天和大家分享一下，我学习专栏的一些心得体会。 随着年龄的增长，我经历了不少业务、技术平台、中间件等多种环境和编程工具的迭代变更。与此同时，我越来越意识到，要做一名优秀的程序员，或者说，能够抵御年龄增长并且增值的程序员，有两样内功是必须持续积累的，那就是软件工程经验方法和算法应用能力。 通俗地讲，就是不论在什么系统或业务环境下、用什么编程工具，都能写出高质量、可维护、接口化代码的能力，以及分解并给出一个实际问题有效解决方案的能力。 我为什么会订阅这个专栏？这也是为什么我在极客时间上看到王争老师的“数据结构与算法之美”的开篇词之后，果断地加入学习行列。同时，我也抱有以下两点期望。 第一，这个专栏是从工程应用，也就是解决实际问题的角度出发来讲算法的，原理和实践相辅相成，现学现用，并且重视思考过程。从我个人经验来看，这的确是比较科学的学习方法。我相信很多人和我一样，以前在学校里都学过算法，不过一旦不碰书了，又没有了应用场景后，很快就把学过的东西丢了，重新拾起来非常困难。 第二，从专栏的标题看出，王争老师试图带我们感受算法的“美”，那必将要先引导我们站在思维的高处，这样才有足够的视野和能力去欣赏这种“美”。我很好奇他会怎么做，也好奇我能否真正地改变以前的认知，切身地感受到“美”。 我是如何学习这个专栏的？就这样，同时带着笃定和疑问，我上路了。经过几个月的认真学习，“数据结构与算法之美”成了我在极客时间打开次数最多，花费时间最多，完成度也最高的一门课。尽管如此，我觉得今后我很可能还会再二刷、多刷**这门课**，把它作为一个深入学习的索引入口。 接下来，我就从几个方面，跟你分享下，这半年我学习这个专栏的一些感受和收获。 1. 原理和实用并重：从实践中总结，应用到实践中去学习的最终目的是为了解决实际问题，专栏里讲的很多方法甚至代码，都能够直接应用到大型项目中去，而不仅仅是简单的原理示例。 比如王争老师在讲散列表的时候，讲了实现一个工业级强度的散列表有哪些需要注意的点。基本上面面俱到，我在很多标准库里都找到了印证。再比如，老师讲的 LRU Cache、Bloom Filter、范围索引上的二分查找等等，也基本和我之前阅读 LevelDB 源代码时，看到的实现细节如出一辙，无非是编程语言的差别。 所以，看这几部分的时候，我觉得十分惊喜，因为我经历过相关的实际应用场景。反过来，专栏这种原理和实用并重的风格，也能帮助我今后在阅读开源代码时提升效率、增进理解。 另外，我觉察到，文章的组织结构，应该也是老师试图传达给我们的“他自己的学习方法”：从开篇介绍一个经典的实际问题开始（需求），到一步步思考引导（分析），再到正式引出相关的数据结构和算法（有效解决方案），再将其应用于开篇问题的解决（实现、测试），最后提出一个课后思考题（泛化、抽象、交流、提升）。 这个形式其实和解决实际工程问题的过程非常类似。我想，大部分工程师就是在一个个这样的过程中不断积累和提升自己的，所以我觉得这个专栏，不论是内容还是形式真的都很赞。 2. 学习新知识的角度：体系、全面、严谨、精炼，可视化配图易于理解“全面”并不是指所有细节面面俱到。事实上，由于算法这门学科本身庞大的体量，这类专栏一般只能看作一个丰富的综述目录，或者深入学习的入口。尽管如此，王争老师依然用简洁精炼的语言 Cover 到了几乎所有最主要的数据结构和算法，以及它们背后的本质思想、原理和应用场景，知识体系结构全面完整并自成一体。 我发现只要能紧跟老师的思路，把每一节的内容理解透彻，到了语言实现部分，往往变成了一种自然的总结描述，所以代码本身并不是重点，重点是背后的思路。 例如，KMP 单模式串匹配和 AC 自动机多模式串匹配算法是我的知识盲区。以前读过几次 KMP 的代码，都没完全搞懂，于是就放弃了。至于 AC 自动机，惭愧地说，我压根儿就没怎么听说过。 但是，在专栏里，王争老师从 BruteForce 方法讲起，经过系统的优化思路铺垫，通俗的举例，再结合恰到好处的配图，最后给出精简的代码。我跟随着老师一路坚持下来，当我看到第二遍时突然就豁然开朗了。而当我真正理解了 AC 自动机的构建和工作原理之后，在某一瞬间，我的内心的确生出了一种美的感觉（或者更多的是“妙”吧？）。 AC 自动机构建的代码，让我不自觉地想到“编织”这个词。之前还觉得凌乱的、四处喷洒的指针，在这里一下子变成了一张有意义的网，编织的过程和成品都体现出了算法的巧妙。这类联想无疑加深了我对这类算法的理解，也许这也意味着，我可以把它正式加入到自己的算法工具箱里了。 另外一个例子是动态规划。以前应用 DP 的时候，我常常比较盲目，不知道怎么确定状态的表示，甚至需要几维的状态都不清楚，可以说是在瞎猜碰运气。经过老师从原理到实例的系统讲解后，我现在明白，原来 DP 本质上就是在压缩重复子问题，状态的定义可以通过最直接的回溯搜索来启发确定。明白这些之后，动态规划也被我轻松拿下了。 3. 已有知识加深的角度：促进思考，连点成线之前看目录的时候，我发现专栏里包含了不少我已经知道的知识。但真正学习了之后，我发现，以前头脑中的不少概念知识点，是相对独立存在的，基本上一个点就对应固定的那几个场景，而在专栏里，王争老师比较注重概念之间的相互关联。对于这些知识，经过王争老师的讲解，基本可以达到交叉强化理解，甚至温故知新的效果。 比如老师会问你，在链表上怎么做二分查找？哈希和链表为什么经常在一起出现？这些问题我之前很少会考虑到，但是当我看到的时候，却启发出很多新的要点和场景（比如 SkipList、LRUCache）。 更重要的是，跟着专栏学习一段时间之后，我脑中原本的一些旧概念，也开始自发地建立起新的连接，连点成线，最后产生了一些我之前从未注意到的想法。 举个感触最深的例子。在跟随专栏做了大量递归状态跟进推演，以及递归树分析后，我现在深刻地认识到，递归这种编程技巧背后，其实是树和堆栈这两种看似关联不大的数据结构。为什么这么说呢？ 堆栈和树在某个层面上，其实有着强烈的对应关系。我刚接触递归的时候，和大多数初学者一样，脑子很容易跟着机器执行的顺序往深里绕，就像 Debug 一个很深的函数调用链一样，每遇到一个函数就 step into，也就是递归函数展开 -&gt; 下一层 -&gt; 递归函数展开 -&gt; 下一层 -&gt;…，结果就是只有“递”，没有“归”，大脑连一次完整调用的一半都跑不完（或者跑完一次很辛苦），自然就会觉得无法分析。如下图，每个圈代表在某一层执行的递归函数，向下的箭头代表调用并进入下一层。 我初学递归时遇到的问题：有去无回，陷得太深 随着我处理了越来越多的递归，我慢慢意识到，为什么人的思考一定要 follow 机器的执行呢？在递归函数体中，我完全可以不用每遇到递归调用都展开并进入下一层（step into），而是可以直接假定下一层调用能够正确返回，然后我该干嘛就继续干嘛（step over），这样的话，我只需要保证最深一层的逻辑，也就是递归的终止条件正确即可。 原因也很简单，不管在哪一层，都是在执行递归函数这同一份代码，不同的层只有一些状态数据不同而已，所以我只需要保证递归函数代码逻辑的正确性，就确保了运行时任意一层的结果正确性。像这样说服自己可以随时 step over 后，我的大脑终于有“递”也有“归”了，后续事务也就能够推动了。 有一定经验后我如何思考递归：有去有回，自由把握 最近在学习这门课程的过程中，我进一步认识到，其实上面两个理解递归的方式，分别对应递归树的深度遍历和广度遍历。尽管机器只能按照深度优先的方式执行递归代码，但人写递归代码的时候更适合用广度的思考方式。当我在实现一个递归函数的时候，其实就是在确定这棵树的整体形状：什么时候终止，什么条件下生出子树，也就是说我实际上是在编程实现一棵树。 那递归树和堆栈又有什么关系呢？递归树中从根节点到树中任意节点的路径，都对应着某个时刻的函数调用链组成的堆栈。递归越深的节点越靠近栈顶，也越早返回。因而我们可以说，递归的背后是一棵树，递归的执行过程，就是在这棵树上做深度遍历的过程，每次进入下一层（“递”）就是压栈，每次退出当前层（“归”）就是出栈。所有的入栈、出栈形成的脉络就组成了递归树的形态。递归树是静态逻辑背景，而当前活跃堆栈是当前动态运行前景。 学完专栏后我怎么看待递归：胸有成“树”，化动为静 这样理解之后，编写或阅读递归代码的时候，我真的能够站得更高，看得更全面，也更不容易掉入一些细节陷阱里去了。 说到这里，我想起之前在不同时间做过的两道题，一道是计算某个长度为 n 的入栈序列可以有多少种出栈序列，另一道是计算包含 n 个节点的二叉树有多少种形状。我惊讶地发现，这两个量竟然是相等的（其实就是卡特兰数）。当时我并不理解为什么栈和树会存在这种关联，现在通过类似递归树的思路我觉得我能够理解了，那就是每种二叉树形状的中序遍历都能够对应上一种出栈顺序。 类似这样“旧知识新理解”还有很多，尽管专栏里并没有直接提到，但是这都是我跟随专栏，坚持边学边思考，逐步感受和收获的。 总结基于以上谈的几点收获和感受，我再总结下我认为比较有用的、学习这个专栏的方法。 1. 紧跟老师思路走，尽量理解每一句话、每一幅配图，亲手推演每一个例子。 王争老师语言精炼。有些文字段落虽短，但背后的信息量却很大。为了方便我们理解，老师用了大量的例子和配图来讲解。即便是非常复杂、枯燥的理论知识，我们理解起来也不会太吃力。 当然有些地方确实有点儿难，这时我们可以退而求其次，“先保接口，再求实现”。例如，红黑树保持平衡的具体策略实现，我跟不下来，就暂时跳过去了，但是我只要知道，它是一种动态数据的高效平衡树，就不妨碍我先使用这个工具，之后再慢慢理解。 2. 在学的过程中回顾和刷新老知识点，并往工程实践上靠。学以致用是最高效的方法。 3. 多思考，思考比结果重要；多交流，亲身感受和其他同学一起交流帮助很大。 最后，感谢王争老师和极客时间，让我在这个专栏里有了不少新收获。祝王争老师事业蒸蒸日上，继续开创新品，也希望极客时间能够联合更多的大牛老师，开发出更多严谨又实用的精品课程！ 结束语 | 送君千里，终须一别王争 2019-02-20 专栏到今天真的要结束了。在写这篇结束语的时候，我的心情还是蛮复杂的，既有点如释重负，又有点不舍。如释重负，是因为我自己对专栏的整体质量非常满意；不舍，是因为我还想分享更多“压箱底”的东西给你。 专栏是在 2018 年 9 月发布的。在发布后的两三天时间里，就有 2 万多人订阅，同时也引来了很多争议。有人说，我就是随便拿个目录就来“割韭菜”。也有人说，数据结构和算法的书籍那么多，国外还有那么多动画、视频教程，为什么要来学我的专栏？ 这些质疑我都非常理解，毕竟大部分基础学科的教材，的确是国外的更全面。实际上，在专栏构思初期，我就意识到了这一点。不夸张地讲，我几乎读过市面上所有有关数据结构和算法的书籍，所以，我也深知市面上的数据结构和算法书籍存在的问题。 尽管有很多书籍讲得通俗易懂，也有很多书籍全面、经典，但是大部分都偏理论，书中的例子也大多脱离真实的软件开发。这些书籍毫无疑问是有用的，但是看完书之后，很多人只是死记硬背了一些知识点而已。这样填鸭式的学习，对于锻炼思维、开拓眼界并没有太多作用。而且，从基础理论到应用实践，有一个非常大的鸿沟要跨越，这是大学教育的普遍不足之处，这也是为什么我们常常觉得大学里学过的很多知识都没用。 我本人是一个追求完美、极致的人，凡事都想做到最好，都想争第一。所以，就我个人而言，我也不允许自己写一个“太普通”“烂大街”的专栏。那时我就给自己立了一个 flag：我一定要写一个跟所有国内、国外经典书籍都不一样的专栏，写出一个可以长期影响一些人的专栏。 所以，在这个专栏写作过程中，我力争并非只是单纯地把某个知识点讲清楚，而是结合自己的理解、实践和经验来讲解。我写每篇文章的时候，几乎都是从由来讲起，做到让你知其然、知其所以然，并且列举大量的实际软件开发中的场景，给你展示如何利用数据结构和算法解决真实的问题。 除此之外，课后思考题我也不拿一些现成的 LeetCode 的题目来应付。这些题目都是我精心设计的、贴合具体实践、非常考验逻辑思维的问题。毫不夸张地讲，只把这些课后思考题做个解答，就可以写成一个有价值、有干货的专栏！ 专栏到今天就要结束了。尽管有些内容稍有瑕疵，但我觉得我实现了最初给自己立下的 flag。那你又学得怎么样呢？ 如果这是你第一次接触数据结构和算法，只是跟着学一遍，你可能不会完全理解所有的内容。关于这个专栏，我从来也不想标榜，我的专栏是易懂到地铁里听听就可以的。因为你要知道，没有难度的学习，也就没有收获。所以，作为初学者，你要想真的拿下数据结构和算法，时间允许的话，建议你再二刷、三刷。 如果你是有一定基础的小伙伴，希望你能够真的做到学以致用。在开发项目、阅读开源代码、理解中间件架构设计方面，多结合数据结构和算法，从本质上理解原理，掌握创新的源头。 如果你是数据结构和算法高手，那我的专栏应该也没有让你失望吧？我个人觉得，专栏里还是有很多可以给你惊喜的地方。对于你来说，哪怕只学到了一个之前没有接触的知识点，我觉得其实已经值得了。 送君千里终须一别。数据结构和算法的学习，我暂时只能陪你到这里了。感谢你订阅我的专栏，感谢这 5 个月的同行，真心希望我的专栏能对你有所帮助。 我知道，很多小伙伴都是“潜水党”，喜欢默默地学习，在专栏要结束的今天，我希望能听到你的声音，希望听听你学习这个专栏的感受和收获。最后，再次感谢！ 第2季回归 | 这一次，我们一起拿下设计模式！王争 2019-11-04 你好，我是王争。“数据结构与算法之美”在今年 2 月底全部更新完毕。时隔 8 个月，我又为你带来了一个新的专栏“设计模式之美”。如果说“数据结构与算法之美”是教你如何写出高效的代码，那“设计模式之美”就是教你如何写出高质量的代码。 在设计“设计模式之美”专栏的时候，我仍然延续“数据结构与算法之美”的讲述方式。在专栏的整体设计上，我希望尽量还原一对一、手把手 code review 的场景，通过 100 篇正文和 10 篇不定期加餐，200 多个真实的项目实战代码案例剖析，100 多个有深度的课堂讨论、头脑风暴，来为你交付这个“设计模式之美”专栏。 我希望通过这个专栏，一次性把跟编写高质量代码相关的所有知识，都系统、全面地讲清楚，一次性给你讲透彻。让你看完这个专栏，就能搞清楚所有跟写高质量代码相关的知识点。 专栏共 100 期正文和 10 期不定期加餐，分为 5 个模块。下面是专栏的目录： 为了感谢老同学，我为你准备了一个专属福利： 11 月 4 日，专栏上新时，我会送你一张 30 元专属优惠券，可与限时优惠同享，有效期 48 小时，建议尽早使用。点击下方图片，立即免费试读新专栏。 一段新的征程，期待与你一起见证成长！ 打卡召集令 | 60 天攻克数据结构与算法王争 2019-11-22 今年 4 月，专栏更新结束之后，我在专栏发布了一篇《数据结构与算法之美》学习指导手册》，在这篇文章里，我对专栏内容重新做了一次梳理，将整个专栏拆分成四个阶段，列出了每个阶段的核心知识点、标注了每个知识点的难易程度（E-Easy，M-Medium，H-Hard），并用 1-10 分说明其重要性。 但是，我发现，很多同学还是没能坚持下来，久而久之对学习算法失去了信心。 回想起来，写专栏之初，我就立下 Flag，要做一个跟国内外经典书籍不一样、可以长期影响一些人的专栏。所以，在专栏完结 9 个月后，我想再做一些事情。 为了带你彻底拿下“数据结构与算法”这座大山，我发起了“60 天攻克数据结构与算法”打卡行动，一起登顶！ 下面是我为你精心规划的学习计划表： 活动时间：2019.11.25-2020.1.19 你将获得： \\1. 坚持 60 天，与 2000 位优秀的工程师一起，彼此激励，相互学习； \\2. 整个学习周期内，我会进行 2 次高质量的社群分享； \\3. 我会精心整理 4 张知识脑图，为你梳理每个阶段的学习重点，发布在专栏里； \\4. 我和极客时间准备了 20 万奖学金，给坚持下来的同学。 活动规则： 在下方申请进入活动打卡群，根据课表打卡，完成学习。 打卡要求： \\1. 每个阶段持续 2 周，每周仅需打卡 3 次，即视为完成该阶段的学习。 2.4 个阶段（8 周）的学习，打卡总数仅需 30 次，即视为完成“60 天攻克数据结构与算法行动”。 \\3. 为了让大家养成习惯，每日只计 1 次打卡，单日内多次打卡视为 1 次。 进入打卡群后，完成学习还有如下奖励： 第一阶段（第 1-2 周）：¥15 奖励金 第二阶段（第 3-4 周）：¥25 奖励金 第三阶段（第 5-6 周）：¥35 奖励金 四个阶段（第 7-8 周）：¥50 奖励金 （注：奖励金会以无门槛优惠券形式、分阶段进行发放，发放时间为每阶段结束后的 7 个工作日内。） 当然，优惠券只是对你的小小奖励。坚持 60 天，与 2000 位优秀的工程师一起，互相学习，彼此激励，彻底拿下数据结构与算法，我奉陪到底。 打卡召集令 | 第一阶段知识总结王争、阿锦 2019-12-09 打卡召集令 | 第二阶段知识总结王争、阿锦 2019-12-23 打卡召集令 | 第三阶段知识总结王争、阿锦 2020-01-06 打卡召集令 | 第四阶段知识总结王争、阿锦 2020-01-20 测试 答案：C 题目解析基础概念，单向循环链表且带有头节点，判断为空，即循环链表只有头节点，自己指向自己，head-&gt;next = head，因此选 C。 答案：D 题目解析递归次数，取决于递归树，而递归树取决于轴枢的选择。树越平衡，递归次数越少。 而对分区的长短处理顺序，影响的是递归时对栈的使用内存，而不是递归次数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051答案：A题目解析尾递归是指，在函数返回的时候，调用自身本身，并且 return 语句不能包含表达式。这样，编译器或者解释器就可以把尾递归做优化，使递归本身无论调用多少次，都只占用一个栈帧，不会出现栈溢出的情况。 尾递归调用时，如果做了优化，栈不会增长，因此，无论多少次调用也不会导致栈溢出。以斐波那契数列为例子，普通的递归版本如下：`int fab(int n)&#123;if(n&lt;3)return 1;elsereturn fab(n-1)+fab(n-2);&#125;具有 &quot; 线性迭代过程 &quot; 特性的递归—尾递归过程int fab(int n,int b1=1,int b2=1,int c=3)&#123;if(n&lt;3)return 1;else &#123;if(n==c)return b1+b2;elsereturn fab1(n,b2,b1+b2,c+1);&#125;&#125; `以 fab(4) 为例子普通递归 fab(4)=fab(3)+fab(2)=fab(2)+fab(1)+fab(2)=3 6 次调用尾递归 fab(4,1,1,3)=fab(4,1,2,4)=1+2=32 次调用 答案：A 题目解析BFS 是广度优先遍历，DFS 是深度优先遍历。 对于一些特殊的图，比如只有一个顶点的图，其 BFS 生成树的树高和 DFS 生成树的树高相等。 一般的图，根据图的 BFS 生成树和 DFS 树的算法思想，BFS 生成树的树高比 DFS 生成树的树高小。 菜鸟面试菜鸟4.6一面面经 \\1. 自我介绍 \\2. 介绍一下项目中的作用以及学到了什么 3. 数据库 \\1. 数据库中的锁 \\2. 具体场景题，一个update 语句跟一个select语句之间的可读性问题 3. 幻读问题 \\4. 有用过哪些数据结构 \\1. ArrayList \\2. LinkedList \\3. 红黑树的查找效率，为什么数据库索引不使用红黑树 4. 为什么HashMap不使用B+树 \\5. HashMap查找时间复杂度 \\5. 计算机网络 \\1. 有三十个http请求能不能一次连接发完 \\2. 四次挥手的Close_wait要怎么解决过多的问题 \\3. 好多不记得了 \\6. 算法 \\1. 找出100个数中的前三个，用哪个方法比较次数最少，比较多少次 \\2. 最大堆要比较多少次 \\3. 快排多少次 \\4. 分治能不能保证第二名一定是排第二的 \\5. 二叉树的树高怎么求，公式是怎么推导的 \\7. java语言 \\1. String s = “中国人”，char数组存，byte存，分别占几个字节 2. String怎么转换成Byte \\3. 这三者1.8种怎么转换的 出自【1】 参考 参考1🔼","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"底层","slug":"底层","permalink":"http://yoursite.com/tags/底层/"},{"name":"网课","slug":"网课","permalink":"http://yoursite.com/tags/网课/"}],"author":"Dufy"},{"title":"⭐️数据结构与算法之美（基础篇）","slug":"数据结构与算法之美（基础篇）","date":"2021-04-01T16:00:00.000Z","updated":"2021-05-16T11:31:53.993Z","comments":true,"path":"2021/04/02/数据结构与算法之美（基础篇）/","link":"","permalink":"http://yoursite.com/2021/04/02/数据结构与算法之美（基础篇）/","excerpt":"","text":"[TOC] 学习路线 第一阶段 复杂度分析 数组 栈 队列 链表 递归 排序 二分查找 第二阶段 散列表 二叉树 堆 堆排序 字符串匹配 Trie🌲 图的表示 第三阶段 四种算法思想 跳表 第四阶段 红黑树 哈希算法 BM/KMP/AC自动机 数据结构、算法概念什么是数据结构、算法？从广义上讲，数据结构是指一组数据的存储结构，算法就是操作数据 的一组方法 从狭义上讲，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。 二者关系数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。 因此，我们无法孤立数据结构来讲算法，也无法孤立算法来讲数据结构。 比如，因为数组因为具有随机访问的特点，二分查找就需要用数组来存储数据。如果选择链表，则二分查找算法就无法工作了 数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构就是没用的。 数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系 学习原则要学习它的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景” 很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。 学习数据结构最难的不是理解和掌握原理，而是能灵活地将各种场景和问题抽象成对应的数据结构和算法 学习数据结构和算法，并不是为了死记硬背几个知识点。我们的目的是建立时间复杂度、空间复杂 度意识，写出高质􏰁的代码，能够设计基础架构，提升编程技能，训练逻辑思维，积攒人生经验，以此 获得工作回报，实现你的价值，完善你的人生。 掌握了数据结构与算法，你看待问题的深度，解决问题的⻆度就会完全不一样。因为这样的你，就像是 站在巨人的肩膀上，拿着生存利器行走世界。数据结构与算法，会为你的编程之路，甚至人生之路打开 一扇通往新世界的大⻔。 复杂度分析 注意递推公式和递归树求递归复杂度 重要性 复杂度分析究竟有多􏰀要呢?可以这么说，它几乎占了数据结构和算法这⻔课的半壁江山，是数据结构和算法学习的精髓。 数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考􏰁效率和 资源消耗的方法，这就是复杂度分析方法。所以，如果你只掌握了数据结构和算法的特点、用法，但是 没有学会复杂度分析，那就相当于只知道操作口诀，而没掌握心法。只有把心法了然于胸，才能做到无 招胜有招! O() 复杂度 要理解O() 复杂度 的含义，O()并不表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以大O时间复杂度，也叫渐进时间复杂度，简称时间复杂度。 更详细的参考：Know Thy Complexities!： 时间复杂度量级 分为： 1）多项式量级 2）非多项式量级（NP, non-deterministic polynomial） 空间复杂度 我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。 最好、最坏和平均情况时间复杂度 e.g. 123456789101112// n表示数组array的长度int find(int[] array, int n, int x) &#123; int i = 0; int pos = -1; for (; i &lt; n; ++i) &#123; if (array[i] == x) &#123; pos = i; break; &#125; &#125; return pos;&#125; 本例子代码用于在数组中查找变量 x 的位置 对于最好情况，x 刚好在数组的第一个位置，则复杂度为O(1)； 最差情况，不在数组中，则复杂度为O(n)； 当然，这两种情况都比较极端，发生概率不大。考虑平均时间复杂度，引入概率分析。（这里算的是期望） 我们知道，要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)。 因此，前面的推导过程中存在的最大问题就是，没有将各种情况发生的概率考虑进去。如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了这样： 则最后的平均时间复杂度仍为O(n) 均摊时间复杂度 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。 数组它可以说是最基础、最简单的数据结构了。数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特点就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为 O(n)。 定义数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。 这里需要注意以下几点： 第一是线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。【注意这里的‘前后’】 与之对应，有’非线性表‘，数据之间并不是简单的前后关系，如树，图等 第二个是连续的内存空间和相同类型的数据—–&gt;’随机访问‘ 随机访问的公式如下： 1a[i]_address = base_address + i * data_type_size 如下，内存块的首地址为 base_address =1000, data_type_size=4B 插入对于插入，如果数组是有序的， 插入平均复杂度为O(n)【依据概率计算】：（1+2+…+n）/n=O(n) 但是，如果数组存储的数据并没有任何规律，数组只是当做一个存储数据的集合，复杂度可以降为o(1)，如下： 利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。 这个处理思想在快排中也会用到 删除和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。 链表 缓存，Cache 缓存是一种提高数据读取性能的技术，比如CPU缓存、数据库缓存、浏览器缓存等 缓存容量有限，当容量用完时，哪些数据清除和保留就需要一定的策略来决定 缓存淘汰策略一般有三种： 1）先进先出策略 FIFO（First In，First Out） 2）最少使用策略 LFU（Least Frequently Used） 3）最近最少使用策略 LRU（Least Recently Used） 底层物理结构（与数组对比）链表，不需要一块连续的内存空间，通过’指针‘将零散的内存块串联起来使用 单链表 插入、删除 复杂度均为 O(1) 访问复杂度：O(n) 代码实现 Python - 链表 123456789101112131415161718class Node: def __init__(self, dataval=None): self.dataval = dataval self.nextval = Noneclass SLinkedList: def __init__(self): self.headval = None # 初始化，创建头指针，效果见下图👇list1 = SLinkedList()list1.headval = Node(\"Mon\")e2 = Node(\"Tue\")e3 = Node(\"Wed\")# Link first Node to second nodelist1.headval.nextval = e2# Link second Node to third nodee2.nextval = e3 循环链表跟单链表相比，区别是尾结点指针指向链表的头结点 循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽 双向链表 空间换时间与时间换空间对于执行较慢的程序，可以通过消耗更多的内存(空间换时间)来进行优化; 而消耗 过多内存的程序，可以通过消耗更多的时间(时间换空间)来降低内存的消耗。、 数组 VS 链表正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。 不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂 度分析就决定使用哪个数据结构来存储数据。 数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。 链表本身没有大小 的限制，天然地支持动态扩容。 除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消 耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的 插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是 Java 语言，就有可能会 导致频繁的 GC(Garbage Collection，垃圾回收)。 所以，在我们实际的开发中，针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。 栈从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。 应用 1）函数调用 图中 显示的是，在执行到 add() 函数时，函数调用栈的情况。 2）表达式求值 队列队列跟栈一样，也是一种操作受限的线性表数据结构。 数组实现 链表实现 循环队列 用数组实现的循环队列： 阻塞队列 阻塞 队列其实是在队列的基础上增加了阻塞操作。就是在队列为空的时候，从队头取数据会 被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回;如果队列已经满了，那么插入数据 的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。 利用阻塞队列可以轻松实现一个”生产者-消费者模型“ 基于阻塞队列，我们还可以通过协调“生产者”和“消费者”的个数，来提高数据的处理效率 并发队列 在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢? 线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加 锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队 列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原 因。 递归掌握程度:轻松写出二叉树遍历、八皇后、背包问题、DFS 的递归代码 递归的理解 递归包括两个过程 ，去的过程叫’递‘，回来的过程叫’归‘，自顶向下。 需要满足的三个条件 一个问题的解可以分解为几个子问题的解 子问题就是数据规模更小的问题。 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 存在递归终止条件 把问题分解为子问题，把子问题再分解为子子问题，一层一层分解下去，不能存在无限循环，这就需要 有终止条件。 如何写出递归代码 写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公 式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。 写递归代码的关键，就是看能不能写出递推公式，而写递推公式的关键就是，如果要解决问题 A，就假 设子问题 B、C 已经解决，然后再来看如何利用 B、C 来解决 A。 编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用 关系，不要试图用人脑去分解递归的每个步骤。 应用举例：找最终推荐人 一般来说，我们会通过数据库来记录这种推荐关系。在数据库表中，我们可以记录两行数据，其中 actor_id 表示用户 id，referrer_id 表示推荐人 id。 123456long findRootReferrerId(long actorId) &#123; Long referrerId = select referrer_id from [table] where actor_id = actorId; # 通过这个缩小规模 if (referrerId == null) return actorId; # 终止条件 return findRootReferrerId(referrerId);&#125; 用户 C 的“最终推荐人”为用户 A 排序按照复杂度分为： 原地排序 指空间复杂度O(1)的排序算法 排序稳定性 指的是待排序序列中有值相等的元素，经过排序后，相等元素之间原有的先后顺序不变，则称稳定的排序算法，否则就叫作不稳定的排序算法 冒泡排序 思想： 通过比较相邻元素，交换位置，每一轮将最值元素通过’冒泡‘浮到边界位置 具体参考：consult/jupyter/排序实现.ipynb 代码： 1234567891011121314# 封装函数def bubble_sort(li): n = len(li) for i in range(1,n): # 比较趟数 tag = 1 for j in range(n-i): if li[j]&lt;li[j+1]: li[j], li[j+1] = li[j+1], li[j] # 交换元素 tag = 0 if tag: breakli = [random.randint(0,100) for i in range(10)]print(li)bubble_sort(li)print(li) 复杂度分析： 属于原地排序； 由于元素相等时候不进行交换，故为稳定的； 2个for循环，复杂度O(n^2） 插入排序插入排序是将列表分为两个区：已排序区间和未排序区间。 排序的过程则是不断的从未排序区间取元素，插入到已排序区间。 具体的插入过程，涉及到元素的移动和最后的插入操作 1234567891011121314151617181920212223\"\"\"插入排序，分为两个区域：左侧为已排序，右侧为未排序区间原始：4 1 6 5 3 2第一趟： 1 4| 6 5 3 2 选【1】第二趟： 1 4 6| 5 3 2 选【2】....第五趟： 1 2 3 4 5 6| 选【5】\"\"\"li = [4, 1, 6, 5, 3, 2]n = len(li)for i in range(1,n): # 比较的趟数：1，2，3，4，5 value = li[i] # 待比较元素 for j in range(i-1,-1,-1): print(j, end='*') if li[j]&gt;value: li[j+1] = li[j] # 移动元素 else: break li[j] = value #插入数据 print(li)li 复杂度分析： 属于原地排序； 由于元素相等时候不进行移动，故为稳定的； 2个for循环，复杂度O(n^2） 冒泡与插入的比较冒泡排序和插入排序的时间复杂度都是 O(n2)，都是原地 排序算法，为什么插入排序要比冒泡排序更受欢迎呢? 这是以为具体在操作时候，冒泡涉及到元素的互换，操作次数要多于元素的移动所需的操作次数，具体运行参见下： 12345678910111213141516171819202122# 插入与冒泡时间比较# 随机生成1000个数组，每个 包含100个数据import timedef generate_data(): test = [] for _ in range(1000): li = [random.randint(0,20000) for i in range(100)] test.append(li) return test test = generate_data()time0 = time.time()for li in test: bubble_sort(li)print(f'冒泡花费时间：&#123;time.time()-time0&#125;')test = generate_data()time0 = time.time()for li in test: insert_sort(li)print(f'插入花费时间：&#123;time.time()-time0&#125;') 12冒泡花费时间：0.7605938911437988插入花费时间：0.429502010345459 所以，插入更优 综合比较 归并排序归并和快排背后 使用的都是分治思想，而分治算法一般都是用递归实现的。分治是一种解决问题的处理思想， 递归是一种编程技巧，这二者并不冲突。 代码： 1234567891011121314151617181920212223242526272829303132333435363738\"\"\"递归实现：递推公式： merge_sort(p,r) = merge(merge_sort(p,q),merge_sort(q+1, r))终止条件： p&gt;=r\"\"\"li = [3,11,5,7,9,3]def merge_sort(li, p, r): pass if p&gt;=r: return mid = (p+r)//2 # 中间位置 merge_sort(li, p, mid) merge_sort(li, mid+1, r) merge(li, p, mid, r) def merge(li, p, mid, r): tmp = [] # 申请临时数组，存放merge 结果 i= p j = mid+1 while i&lt;=mid and j&lt;=r: if li[i]&lt;=li[j]: # ‘=’保证是稳定的 tmp.append(li[i]) i +=1 else: tmp.append(li[j]) j +=1 # while 执行完肯定有一部分没数了 while i&lt;=mid: # 说明这部分还没取完 tmp.append(li[i]) i +=1 while j&lt;=r: # 说明这部分还没取完 tmp.append(li[j]) j +=1 li[p:r+1] = tmp merge_sort(li, 0, len(li)-1)li 复杂度分析： 空间复杂度O(n), 不属于原地排序； 稳定的， 可由if li[i]&lt;=li[j]: tmp.append(li[i]) 保证 复杂度O(nlg(n)） 分析： 假设对 n 个元素进行归并排序所需时间为T(n)，则根据归并排序代码 def merge_sort(li, p, r): pass if p&gt;=r: return mid = (p+r)//2 # 中间位置 merge_sort(li, p, mid) ——– T(n/2) merge_sort(li, mid+1, r) ——– T(n/2) merge(li, p, mid, r) ——– (n) 可知，T(n) = T(n/2)+T(n/2)+n; n&gt;1 T(n) = C; n=1 T(n) = 2T(n/2)+n=4T(n/4)+2n=8T(n/8)+3n=….=O(nlgn) 快排快排，也是采用分治思想 主要是要分区点的获取，已经分区点的选择水平 代码： 123456789101112131415161718192021222324252627# 递推公式：# quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)# 终止条件：# p &gt;= rdef quick_sort(li, p, r): if p&gt;=r: return q = partition(li, p, r) # 获分区点 quick_sort(li, p, q-1) quick_sort(li, q+1, r) def partition(li, p, r): pivot = li[p] # 选队首为比较点 while p&lt;r: while li[r]&gt;=pivot and p&lt;r: r -= 1 li[p] = li[r] while li[p] &lt;=pivot and p&lt;r: p += 1 li[r] = li[p] li[p] = pivot # p/r相遇,得到 return p li = [13,11,25,7,19,3] quick_sort(li, 0, len(li)-1)li 关于def partition(li, p, r) # 获分区点函数的解释： 快排时间复杂度： 12T(1) = C； n=1时，只需要常量级的执行时间，所以表示为C。T(n) = 2*T(n/2) + n； n&gt;1 T(n) = O(nlogn) 快排与归并 的比较 可以发现，归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处 理过程是由上到下的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的 排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是合并 函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占 用太多内存的问题。 复杂度O(n)排序 桶排序 计数排序 基数排序 它们均不涉及元素之间的比较操作 二分查找 1234567891011121314151617# 二分查找# [1,2,3,5,6,7], tar=1def b_search(li, tar): l = 0 h = len(li)-1 while l&lt;=h: mid= (h+l)//2 pivot = li[mid] if tar==pivot: return mid elif tar&lt;pivot: # 缩小h h=mid-1 else: l=mid+1 return Noneli = [1,2,3,5,6,7]print(b_search(li,6)) 递归和非递归代码参见：二分查找.ipynb 适用场景 二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用。那针对动态数据集合，如何在其中快速查找某个数据呢?别急，等到二叉树 那一节我会详细讲。 二分查找变形 1234变体一:查找第一个值等于给定值的元素变体二:查找最后一个值等于给定值的元素变体三:查找第一个大于等于给定值的元素变体四:查找最后一个小于等于给定值的元素 参见：二分查找.ipynb 跳表应用场景：Redis用跳表实现有序集合 跳表是一种各方面性能都比较优秀的动态数据结构，可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树(Red-black tree)。 链表+索引 = 跳表： 查询时间复杂度 假设索引 h 级，最高索引有2个结点。那么整个跳表高度为：log(n) 单层索引最多需要遍历3个结点，所以整体的复杂度为O(log(n)) 此外，插入、删除的复杂度也为O(log(n)) 空间复杂度 将所有索引的节点数相加得到： n/2+n/4+n/8…+8+4+2=n-2。所以，跳表的空间复杂度是 O(n)。 索引的动态更新 如果不停地往跳表中插入数据，而不更新索引，极端情况下会退化成单链表，如下： 作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链 表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。 而跳表是通过随机函数来维护前面提到的“平衡性”： 我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就 将这个结点添加到第一级到第 K 级这 K 级索引中。 散列表散列表英文名叫’Hash Table‘，又称’哈希表‘ 散列表用的是数组支持下标随机访问数据的特性，所以散列表其实是数组的一种扩展，由数组演化而来。可以说，没有数组，就没有散列表。 原理 注意，哈希函数哈希的是key ,因此，在dict中查找某个key是否存在操作的时间复杂度为o（1）；查找某个value是否存在操作的时间复杂度为o（n） 散列冲突 1）开放寻址法 开放寻址法的核心思想是，如果出现了散列冲突，我们就􏰂新探测一个空闲位置，将其插入 Python 数据结构入门 - 哈希表（Hash Table） 2）链表法 代码实现: 链式哈希表在Python中的实现,链接,法散,列表,python 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101class Node(object): def __init__(self, key): self.key = key self.next = None self.pre = None def __repr__(self): value = '&#123;%d&#125;' % (self.key) return valueclass HashTable(object): ''' 散列表的实现（链接法） ''' def __init__(self, T = []): self.T = T #散列表 self.m = len(self.T) #散列表的容量（槽的数目） self.size = 0 #散列表中所有元素的数量 def _hash(self, key): #定义散列函数 return abs(key) % self.m def insert(self, node): #插入一个节点 k = self._hash(node.key) #散列值 if self.T[k] == None: #此链表内没有值的情况 self.T[k] = node node.next = None node.pre = None else: node.next = self.T[k] #此链表内有值的情况 self.T[k].pre = node self.T[k] = node self.T[k].pre = None self.size += 1 def search(self, key): #按关键字搜索 k = self._hash(key) current = self.T[k] while current != None and current.key != key: current = current.next if current == None : #没有找到时 return \"keyerror\" return True def delete(self, node): #删除散列表的指定节点 k = self._hash(node.key) if node == self.T[k]: self.T[k] == None elif node.next == None: node.pre.next = None else: node.next.pre = node.pre node.pre.next = node.next self.size -= 1 return node def print(self): #可视化散列表 res = [None for _ in range(self.m)] for i in range(self.m): k = self.T[i] line = '' while k: line += '%s' %k k = k.next if k: line += '&lt;=&gt;' res[i] = line print(res)if __name__=='__main__': T = HashTable([None for _ in range(10)]) nodes = [] for i in range(31): node = Node(i) nodes.append(node) T.print() T.insert(nodes[0]) T.print() T.insert(nodes[5]) T.print() T.insert(nodes[9]) T.print() T.insert(nodes[10]) T.print() T.insert(nodes[20]) T.print() T.insert(nodes[30]) T.print() T.delete(nodes[20]) T.print()输出结果：['', '', '', '', '', '', '', '', '', '']['&#123;0&#125;', '', '', '', '', '', '', '', '', '']['&#123;0&#125;', '', '', '', '', '&#123;5&#125;', '', '', '', '']['&#123;0&#125;', '', '', '', '', '&#123;5&#125;', '', '', '', '&#123;9&#125;']['&#123;10&#125;&lt;=&gt;&#123;0&#125;', '', '', '', '', '&#123;5&#125;', '', '', '', '&#123;9&#125;']['&#123;20&#125;&lt;=&gt;&#123;10&#125;&lt;=&gt;&#123;0&#125;', '', '', '', '', '&#123;5&#125;', '', '', '', '&#123;9&#125;']['&#123;30&#125;&lt;=&gt;&#123;20&#125;&lt;=&gt;&#123;10&#125;&lt;=&gt;&#123;0&#125;', '', '', '', '', '&#123;5&#125;', '', '', '', '&#123;9&#125;']['&#123;30&#125;&lt;=&gt;&#123;10&#125;&lt;=&gt;&#123;0&#125;', '', '', '', '', '&#123;5&#125;', '', '', '', '&#123;9&#125;'] 课后题 \\1. 假设我们有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？ 遍历 10 万条数据，以 URL 为 key，访问次数为 value，存入散列表，同时记录下访问次数的最大值 K，时间复杂度 O(N)。 如果 K 不是很大，可以使用桶排序，时间复杂度 O(N)。如果 K 非常大（比如大于 10 万），就使用快速排序，复杂度 O(NlogN)。 12345&gt; a = &#123;'sdfg':1, 'zzjk':9, 'ooo':4, 'ww.s':3&#125;&gt; sorted(a.items(), key = lambda item:item[1])&gt; -----------------------------------&gt; [('sdfg', 1), ('ww.s', 3), ('ooo', 4), ('zzjk', 9)]&gt; \\2. 有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？ 以第一个字符串数组构建散列表，key 为字符串，value 为出现次数。再遍历第二个字符串数组，以字符串为 key 在散列表中查找，如果 value 大于零，说明存在相同字符串。时间复杂度 O(N)。 扩容 如果散列表的装载因子过大，散列冲突就会变得不可接受，这时可以进行动态扩容，来降低装载因子的大小 如何设计工业级的散列表 主要考虑：如何设计散列函数，如何根据装载因子动态扩容，以及如何选择散列冲突解决方法： 关于散列函数的设计，我们要尽可能让散列后的值随机且均匀分布，这样会尽可能地减少散列冲突，即便冲突之后，分配到每个槽内的数据也比较均匀。除此之外，散列函数的设计也不能太复杂，太复杂就 会太耗时间，也会影响散列表的性能。 关于散列冲突解决方法的选择，我对比了开放寻址法和链表法两种方法的优劣和适应的场景。大部分情 况下，链表法更加普适。而且，我们还可以通过将链表法中的链表改造成其他动态查找数据结构，比如 红黑树，来避免散列表时间复杂度退化成 O(n)，抵御散列碰撞攻击。但是，对于小规模数据、装载因子 不高的散列表，比较适合用开放寻址法。 对于动态散列表来说，不管我们如何设计散列函数，选择什么样的散列冲突解决方法。随着数据的不断 增加，散列表总会出现装载因子过高的情况。这个时候，我们就需要启动动态扩容。 散列表和链表的组合使用 案例：LRU缓存淘汰算法 单链表实现的复杂度为O(n) 借助散列表，实现复杂度O(1): 二叉树 基本概念 完全二叉树与满二叉树 二叉树存储 链式存储 基于数组的顺序存储 如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单􏰁拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。 遍历 时间复杂度O(n)，参考递归时间复杂度分析：f(n)=2*f(n/2)+1,且f(1)=1—-&gt;O(n) 层序遍历？？？【结合队列】 二叉查找树 二叉查找树要求，在树中的任意一个节点，其左子树中的每个节 点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。 中序遍历二叉查找树，可以输出有 序的数据序列，时间复杂度是 O(n)，非常高效。因此，二叉查找树也叫作二叉排序树。 堆需要满足下面两个条件： 1.堆是一个完全二叉树; 2.堆中每一个节点的值都必须大于等于(或小于等于)其子树中每个节点的值。 堆排序 堆排序包括建堆和排 序两个操作，建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 O(nlogn)，所以，堆排序整体的 时间复杂度是 O(nlogn)。 堆 的应用 优先级队列 队列是先进先出，不过优先级队列则是优先级最高的，最先出队 Trie 树Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。 Trie 树只是不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。Trie 树比较适 合的是查找前缀匹配的字符串，也就是类似开篇问题的那种场景。 关于 Trie 的应用场景，希望你能记住 8 个字：一次建树，多次查询。 哈希算法哈希算法的定义和原理非常简单，基本上一句话就可以概括了。将任意⻓度的二进制值串映射为固定⻓ 度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。 但是，要想设计一个优秀的哈希算法并不容易，需要满足的几点要求: 从哈希值不能反向推导出原始数据(所以哈希算法也叫单向哈希算法); 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同; 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小; 哈希算法的执行效率要尽􏰃高效，针对较⻓的文本，也能快速地计算出哈希值。 应用： 安全加密 MD5(MD5 Message-Digest Algorithm，MD5 消息摘要算法) SHA(Secure Hash Algorithm，安全散列算法) 除了这两个之外，当然还有很多其他加密算法，比如 DES(Data Encryption Standard，数据加密标 准)、AES(Advanced Encryption Standard，高级加密标准)。 根据’鸽巢原理‘，哈希算法无法做到零冲突。 我们知道，哈希算法产生的哈希值的⻓度是固定且有限的。比如前面举的 MD5 的例子，哈希值是固定 的 128 位二进制串，能表示的数据是有限的，最多能表示 2^128 个数据，而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对 2^128+1 个数据求哈希值，就必然会存在哈希值相同的情况 字符串匹配BF算法Brute Force, 也即暴力匹配算法，复杂度为O（mn），其中m为模式串长度，n 为主串长度，n&gt;m RK 算法RK 算法的全称叫 Rabin-Karp 算法，是由它的两位发明者 Rabin 和 Karp 的名字来命名的。 是BF算法的升级版 RK 算法的思路是这样的：我们通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了(这 里先不考虑哈希冲突的问题，后面我们会讲到)。因为哈希值是一个数字，数字之间比较是否相等是非 常快速的，所以模式串和子串比较的效率就提高了。 整体复杂度：O(n) BM 算法图度：每个用户有多少个好友， 对应到图中，就叫做顶点的度(degree)，就是跟顶点相连接的边的条数。 有向、无向图 存储 邻接矩阵和邻接表 邻接表： BFS 关于BFS和DFS，一个用到了队列，一个用到了递归 注意体会之 广度优先搜索(Breadth-First-Search) 借助 ’队列‘ 实现 DFS回溯思想 四种算法思想贪心 算法、分治算法、回溯算法、动态规划，更加确切地说，它们应该是算法思想，并不是具体的算法，常用来指导我们设计具体的算法和编码等。 贪心算法贪心算法有很多经典的应用，比如霍夫曼编码(Huffman Coding)、Prim 和 Kruskal 最小生成树算法、还有 Dijkstra 单源最短路径算法 霍夫曼编码 贪心思想：出现频率多的字符，用稍微短一些的字符编码；出现频率少的字符，用长一些的编码。霍夫曼编码要求各个字符的编码之间，不会出现某个编码是 另一个编码前缀的情况。 具体实现： 所以，最终的编码为 分治分治算法核心思想：分而治之 ，也就是将原问题划分成 n 个 规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问 题的解。 分治算法子问题之间没有相关性，这一点是跟动态规划的明显区别 MapReduce MapReduce 本质就是分治思想 一台机器过于低效，那我们就把任务拆分到多台机器上来处理。如果拆分之后的小任务之间互不干扰，独立计算，最后再将结果合并。这不就是分治思想吗? 尽管 MapReduce 的模型非常简单，但是在 Google 内部应用非常广泛。它除了可以用来处理这种数据 与数据之间存在关系的任务，比如 MapReduce 的经典例子，统计文件中单词出现的频率。除此之外， 它还可以用来处理数据与数据之间没有关系的任务，比如对网⻚分析、分词等，每个网⻚可以􏰂立的分 析、分词，而这两个网⻚之间并没有关系。网⻚几十亿、上百亿，如果单机处理，效率低下，我们就可 以利用 MapReduce 提供的高可靠、高性能、高容错的并行计算框架，并行地处理这几十亿、上百亿的 网⻚。 我们也时常感叹 Google 的创新能力如此之强，总是在引领技术的发展。实际上，创新并非离我们很 远，创新的源泉来自对事物本质的认识。无数优秀架构设计的思想来源都是基础的数据结构和算法，这 本身就是算法的一个魅力所在。 回溯深度优先搜索算法利用的就是回溯算法思想 笼统地讲，回溯算法很多时候都应用在“搜索”这类问题上。不过这里说的搜索，并不是􏰃义的指我们前 面讲过的图的搜索算法，而是在一组可能的解中，搜索满足期望的解。 回溯有点类似枚举，非常适合用递归实现。某次尝试出现错误后回到之前的时刻，重新尝试，类似于《蝴蝶效应》 N皇后问题 参考：合格程序员必会，回溯法解八皇后，看了这个动画，你就全明白了，C++实现 动态规划 一个模型三个特征 首先，我们来看，什么是“一个模型”?它指的是动态规划适合解决的问题的模型。我把这个模型定义为 “多阶段决策最优解模型”。下面我具体来给你讲讲。 我们一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都 对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。 什么是“三个特征”?它们分别是最优子结构、无后效性和重复子问题。 两种方法： 状态转移表法 问题：假设我们有一个 n 乘以 n 的矩阵 w[n][n]。矩阵存储的都是正整数。棋子起始位置在左上⻆，终止位置 在右下⻆。我们将棋子从左上⻆移动到右下⻆。每次只能向右或者向下移动一位。从左上⻆到右下⻆， 会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的⻓度。那从左上⻆移动到右 下⻆的最短路径⻓度是多少呢? 状态转移方程法 参考 别人的课程索引：http://173.249.58.195/","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"底层","slug":"底层","permalink":"http://yoursite.com/tags/底层/"},{"name":"网课","slug":"网课","permalink":"http://yoursite.com/tags/网课/"}],"author":"Dufy"},{"title":"⭐️数据结构与算法之美（高级篇 & 实战篇）","slug":"数据结构与算法之美（高级篇）","date":"2021-04-01T16:00:00.000Z","updated":"2021-04-04T15:52:37.722Z","comments":true,"path":"2021/04/02/数据结构与算法之美（高级篇）/","link":"","permalink":"http://yoursite.com/2021/04/02/数据结构与算法之美（高级篇）/","excerpt":"[TOC]","text":"[TOC] =======================================高级篇 43-51（9讲）43 | 拓扑排序：如何确定代码源文件的编译依赖关系？王争 2019-01-04  07:12 1.25x 讲述：冯永吉 大小：8.80M 时长：09:36 从今天开始，我们就进入了专栏的高级篇。相对基础篇，高级篇涉及的知识点，都比较零散，不是太系统。所以，我会围绕一个实际软件开发的问题，在阐述具体解决方法的过程中，将涉及的知识点给你详细讲解出来。 所以，相较于基础篇“开篇问题 - 知识讲解 - 回答开篇 - 总结 - 课后思考”这样的文章结构，高级篇我稍作了些改变，大致分为这样几个部分：“问题阐述 - 算法解析 - 总结引申 - 课后思考”。 好了，现在，我们就进入高级篇的第一节，如何确定代码源文件的编译依赖关系？ 我们知道，一个完整的项目往往会包含很多代码源文件。编译器在编译整个项目的时候，需要按照依赖关系，依次编译每个源文件。比如，A.cpp 依赖 B.cpp，那在编译的时候，编译器需要先编译 B.cpp，才能编译 A.cpp。 编译器通过分析源文件或者程序员事先写好的编译配置文件（比如 Makefile 文件），来获取这种局部的依赖关系。那编译器又该如何通过源文件两两之间的局部依赖关系，确定一个全局的编译顺序呢？ 算法解析这个问题的解决思路与“图”这种数据结构的一个经典算法“拓扑排序算法”有关。那什么是拓扑排序呢？这个概念很好理解，我们先来看一个生活中的拓扑排序的例子。 我们在穿衣服的时候都有一定的顺序，我们可以把这种顺序想成，衣服与衣服之间有一定的依赖关系。比如说，你必须先穿袜子才能穿鞋，先穿内裤才能穿秋裤。假设我们现在有八件衣服要穿，它们之间的两两依赖关系我们已经很清楚了，那如何安排一个穿衣序列，能够满足所有的两两之间的依赖关系？ 这就是个拓扑排序问题。从这个例子中，你应该能想到，在很多时候，拓扑排序的序列并不是唯一的。你可以看我画的这幅图，我找到了好几种满足这些局部先后关系的穿衣序列。 弄懂了这个生活中的例子，开篇的关于编译顺序的问题，你应该也有思路了。开篇问题跟这个问题的模型是一样的，也可以抽象成一个拓扑排序问题。 拓扑排序的原理非常简单，我们的重点应该放到拓扑排序的实现上面。 我前面多次讲过，算法是构建在具体的数据结构之上的。针对这个问题，我们先来看下，如何将问题背景抽象成具体的数据结构？ 我们可以把源文件与源文件之间的依赖关系，抽象成一个有向图。每个源文件对应图中的一个顶点，源文件之间的依赖关系就是顶点之间的边。 如果 a 先于 b 执行，也就是说 b 依赖于 a，那么就在顶点 a 和顶点 b 之间，构建一条从 a 指向 b 的边。而且，这个图不仅要是有向图，还要是一个有向无环图，也就是不能存在像 a-&gt;b-&gt;c-&gt;a 这样的循环依赖关系。因为图中一旦出现环，拓扑排序就无法工作了。实际上，拓扑排序本身就是基于有向无环图的一个算法。 public class Graph { private int v; // 顶点的个数 private LinkedList adj[]; // 邻接表 public Graph(int v) { ​ this.v = v; ​ adj = new LinkedList[v]; ​ for (int i=0; i&lt;v; ++i) { ​ adj[i] = new LinkedList&lt;&gt;(); ​ } } public void addEdge(int s, int t) { // s先于t，边s-&gt;t ​ adj[s].add(t); } } 数据结构定义好了，现在，我们来看，如何在这个有向无环图上，实现拓扑排序？ 拓扑排序有两种实现方法，都不难理解。它们分别是 Kahn 算法和 DFS 深度优先搜索算法。我们依次来看下它们都是怎么工作的。 1.Kahn 算法Kahn 算法实际上用的是贪心算法思想，思路非常简单、好懂。 定义数据结构的时候，如果 s 需要先于 t 执行，那就添加一条 s 指向 t 的边。所以，如果某个顶点入度为 0， 也就表示，没有任何顶点必须先于这个顶点执行，那么这个顶点就可以执行了。 我们先从图中，找出一个入度为 0 的顶点，将其输出到拓扑排序的结果序列中（对应代码中就是把它打印出来），并且把这个顶点从图中删除（也就是把这个顶点可达的顶点的入度都减 1）。我们循环执行上面的过程，直到所有的顶点都被输出。最后输出的序列，就是满足局部依赖关系的拓扑排序。 我把 Kahn 算法用代码实现了一下，你可以结合着文字描述一块看下。不过，你应该能发现，这段代码实现更有技巧一些，并没有真正删除顶点的操作。代码中有详细的注释，你自己来看，我就不多解释了。 public void topoSortByKahn() { int[] inDegree = new int[v]; // 统计每个顶点的入度 for (int i = 0; i &lt; v; ++i) { ​ for (int j = 0; j &lt; adj[i].size(); ++j) { ​ int w = adj[i].get(j); // i-&gt;w ​ inDegree[w]++; ​ } } LinkedList queue = new LinkedList&lt;&gt;(); for (int i = 0; i &lt; v; ++i) { ​ if (inDegree[i] == 0) queue.add(i); } while (!queue.isEmpty()) { ​ int i = queue.remove(); ​ System.out.print(“-&gt;” + i); ​ for (int j = 0; j &lt; adj[i].size(); ++j) { ​ int k = adj[i].get(j); ​ inDegree[k]–; ​ if (inDegree[k] == 0) queue.add(k); ​ } } } 2.DFS 算法图上的深度优先搜索我们前面已经讲过了，实际上，拓扑排序也可以用深度优先搜索来实现。不过这里的名字要稍微改下，更加确切的说法应该是深度优先遍历，遍历图中的所有顶点，而非只是搜索一个顶点到另一个顶点的路径。 关于这个算法的实现原理，我先把代码贴在下面，下面给你具体解释。 public void topoSortByDFS() { // 先构建逆邻接表，边s-&gt;t表示，s依赖于t，t先于s LinkedList inverseAdj[] = new LinkedList[v]; for (int i = 0; i &lt; v; ++i) { // 申请空间 ​ inverseAdj[i] = new LinkedList&lt;&gt;(); } for (int i = 0; i &lt; v; ++i) { // 通过邻接表生成逆邻接表 ​ for (int j = 0; j &lt; adj[i].size(); ++j) { ​ int w = adj[i].get(j); // i-&gt;w ​ inverseAdj[w].add(i); // w-&gt;i ​ } } boolean[] visited = new boolean[v]; for (int i = 0; i &lt; v; ++i) { // 深度优先遍历图 ​ if (visited[i] == false) { ​ visited[i] = true; ​ dfs(i, inverseAdj, visited); ​ } } } private void dfs( ​ int vertex, LinkedList inverseAdj[], boolean[] visited) { for (int i = 0; i &lt; inverseAdj[vertex].size(); ++i) { ​ int w = inverseAdj[vertex].get(i); ​ if (visited[w] == true) continue; ​ visited[w] = true; ​ dfs(w, inverseAdj, visited); } // 先把vertex这个顶点可达的所有顶点都打印出来之后，再打印它自己 System.out.print(“-&gt;” + vertex); } 这个算法包含两个关键部分。 第一部分是通过邻接表构造逆邻接表。邻接表中，边 s-&gt;t 表示 s 先于 t 执行，也就是 t 要依赖 s。在逆邻接表中，边 s-&gt;t 表示 s 依赖于 t，s 后于 t 执行。为什么这么转化呢？这个跟我们这个算法的实现思想有关。 第二部分是这个算法的核心，也就是递归处理每个顶点。对于顶点 vertex 来说，我们先输出它可达的所有顶点，也就是说，先把它依赖的所有的顶点输出了，然后再输出自己。 到这里，用 Kahn 算法和 DFS 算法求拓扑排序的原理和代码实现都讲完了。我们来看下，这两个算法的时间复杂度分别是多少呢？ 从 Kahn 代码中可以看出来，每个顶点被访问了一次，每个边也都被访问了一次，所以，Kahn 算法的时间复杂度就是 O(V+E)（V 表示顶点个数，E 表示边的个数）。 DFS 算法的时间复杂度我们之前分析过。每个顶点被访问两次，每条边都被访问一次，所以时间复杂度也是 O(V+E)。 注意，这里的图可能不是连通的，有可能是有好几个不连通的子图构成，所以，E 并不一定大于 V，两者的大小关系不确定。所以，在表示时间复杂度的时候，V、E 都要考虑在内。 总结引申在基础篇中，关于“图”，我们讲了图的定义和存储、图的广度和深度优先搜索。今天，我们又讲了一个关于图的算法，拓扑排序。 拓扑排序应用非常广泛，解决的问题的模型也非常一致。凡是需要通过局部顺序来推导全局顺序的，一般都能用拓扑排序来解决。除此之外，拓扑排序还能检测图中环的存在。对于 Kahn 算法来说，如果最后输出出来的顶点个数，少于图中顶点个数，图中还有入度不是 0 的顶点，那就说明，图中存在环。 关于图中环的检测，我们在递归那一节讲过一个例子，在查找最终推荐人的时候，可能会因为脏数据，造成存在循环推荐，比如，用户 A 推荐了用户 B，用户 B 推荐了用户 C，用户 C 又推荐了用户 A。如何避免这种脏数据导致的无限递归？这个问题，我当时留给你思考了，现在是时候解答了。 实际上，这就是环的检测问题。因为我们每次都只是查找一个用户的最终推荐人，所以，我们并不需要动用复杂的拓扑排序算法，而只需要记录已经访问过的用户 ID，当用户 ID 第二次被访问的时候，就说明存在环，也就说明存在脏数据。 HashSet hashTable = new HashSet&lt;&gt;(); // 保存已经访问过的actorId long findRootReferrerId(long actorId) { if (hashTable.contains(actorId)) { // 存在环 ​ return; } hashTable.add(actorId); Long referrerId = ​ select referrer_id from [table] where actor_id = actorId; if (referrerId == null) return actorId; return findRootReferrerId(referrerId); } 如果把这个问题改一下，我们想要知道，数据库中的所有用户之间的推荐关系了，有没有存在环的情况。这个问题，就需要用到拓扑排序算法了。我们把用户之间的推荐关系，从数据库中加载到内存中，然后构建成今天讲的这种有向图数据结构，再利用拓扑排序，就可以快速检测出是否存在环了。 课后思考在今天的讲解中，我们用图表示依赖关系的时候，如果 a 先于 b 执行，我们就画一条从 a 到 b 的有向边；反过来，如果 a 先于 b，我们画一条从 b 到 a 的有向边，表示 b 依赖 a，那今天讲的 Kahn 算法和 DFS 算法还能否正确工作呢？如果不能，应该如何改造一下呢？ 我们今天讲了两种拓扑排序算法的实现思路，Kahn 算法和 DFS 深度优先搜索算法，如果换做 BFS 广度优先搜索算法，还可以实现吗？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 44 | 最短路径：地图软件是如何计算出最优出行路径的？王争 2019-01-07  00:13 1.25x 讲述：冯永吉 大小：13.30M 时长：14:31 基础篇的时候，我们学习了图的两种搜索算法，深度优先搜索和广度优先搜索。这两种算法主要是针对无权图的搜索算法。针对有权图，也就是图中的每条边都有一个权重，我们该如何计算两点之间的最短路径（经过的边的权重和最小）呢？今天，我就从地图软件的路线规划问题讲起，带你看看常用的最短路径算法（Shortest Path Algorithm）。 像 Google 地图、百度地图、高德地图这样的地图软件，我想你应该经常使用吧？如果想从家开车到公司，你只需要输入起始、结束地址，地图就会给你规划一条最优出行路线。这里的最优，有很多种定义，比如最短路线、最少用时路线、最少红绿灯路线等等。作为一名软件开发工程师，你是否思考过，地图软件的最优路线是如何计算出来的吗？底层依赖了什么算法呢？ 算法解析我们刚提到的最优问题包含三个：最短路线、最少用时和最少红绿灯。我们先解决最简单的，最短路线。 解决软件开发中的实际问题，最重要的一点就是建模，也就是将复杂的场景抽象成具体的数据结构。针对这个问题，我们该如何抽象成数据结构呢？ 我们之前也提到过，图这种数据结构的表达能力很强，显然，把地图抽象成图最合适不过了。我们把每个岔路口看作一个顶点，岔路口与岔路口之间的路看作一条边，路的长度就是边的权重。如果路是单行道，我们就在两个顶点之间画一条有向边；如果路是双行道，我们就在两个顶点之间画两条方向不同的边。这样，整个地图就被抽象成一个有向有权图。 具体的代码实现，我放在下面了。于是，我们要求解的问题就转化为，在一个有向有权图中，求两个顶点间的最短路径。 public class Graph { // 有向有权图的邻接表表示 private LinkedList adj[]; // 邻接表 private int v; // 顶点个数 public Graph(int v) { ​ this.v = v; ​ this.adj = new LinkedList[v]; ​ for (int i = 0; i &lt; v; ++i) { ​ this.adj[i] = new LinkedList&lt;&gt;(); ​ } } public void addEdge(int s, int t, int w) { // 添加一条边 ​ this.adj[s].add(new Edge(s, t, w)); } private class Edge { ​ public int sid; // 边的起始顶点编号 ​ public int tid; // 边的终止顶点编号 ​ public int w; // 权重 ​ public Edge(int sid, int tid, int w) { ​ this.sid = sid; ​ this.tid = tid; ​ this.w = w; ​ } } // 下面这个类是为了dijkstra实现用的 private class Vertex { ​ public int id; // 顶点编号ID ​ public int dist; // 从起始顶点到这个顶点的距离 ​ public Vertex(int id, int dist) { ​ this.id = id; ​ this.dist = dist; ​ } } } 想要解决这个问题，有一个非常经典的算法，最短路径算法，更加准确地说，是单源最短路径算法（一个顶点到一个顶点）。提到最短路径算法，最出名的莫过于 Dijkstra 算法了。所以，我们现在来看，Dijkstra 算法是怎么工作的。 这个算法的原理稍微有点儿复杂，单纯的文字描述，不是很好懂。所以，我还是结合代码来讲解。 // 因为Java提供的优先级队列，没有暴露更新数据的接口，所以我们需要重新实现一个 private class PriorityQueue { // 根据vertex.dist构建小顶堆 private Vertex[] nodes; private int count; public PriorityQueue(int v) { ​ this.nodes = new Vertex[v+1]; ​ this.count = v; } public Vertex poll() { // TODO: 留给读者实现… } public void add(Vertex vertex) { // TODO: 留给读者实现…} // 更新结点的值，并且从下往上堆化，重新符合堆的定义。时间复杂度O(logn)。 public void update(Vertex vertex) { // TODO: 留给读者实现…} public boolean isEmpty() { // TODO: 留给读者实现…} } public void dijkstra(int s, int t) { // 从顶点s到顶点t的最短路径 int[] predecessor = new int[this.v]; // 用来还原最短路径 Vertex[] vertexes = new Vertex[this.v]; for (int i = 0; i &lt; this.v; ++i) { ​ vertexes[i] = new Vertex(i, Integer.MAX_VALUE); } PriorityQueue queue = new PriorityQueue(this.v);// 小顶堆 boolean[] inqueue = new boolean[this.v]; // 标记是否进入过队列 vertexes[s].dist = 0; queue.add(vertexes[s]); inqueue[s] = true; while (!queue.isEmpty()) { ​ Vertex minVertex= queue.poll(); // 取堆顶元素并删除 ​ if (minVertex.id == t) break; // 最短路径产生了 ​ for (int i = 0; i &lt; adj[minVertex.id].size(); ++i) { ​ Edge e = adj[minVertex.id].get(i); // 取出一条minVetex相连的边 ​ Vertex nextVertex = vertexes[e.tid]; // minVertex–&gt;nextVertex ​ if (minVertex.dist + e.w &lt; nextVertex.dist) { // 更新next的dist ​ nextVertex.dist = minVertex.dist + e.w; ​ predecessor[nextVertex.id] = minVertex.id; ​ if (inqueue[nextVertex.id] == true) { ​ queue.update(nextVertex); // 更新队列中的dist值 ​ } else { ​ queue.add(nextVertex); ​ inqueue[nextVertex.id] = true; ​ } ​ } ​ } } // 输出最短路径 System.out.print(s); print(s, t, predecessor); } private void print(int s, int t, int[] predecessor) { if (s == t) return; print(s, predecessor[t], predecessor); System.out.print(“-&gt;” + t); } 我们用 vertexes 数组，记录从起始顶点到每个顶点的距离（dist）。起初，我们把所有顶点的 dist 都初始化为无穷大（也就是代码中的 Integer.MAX_VALUE）。我们把起始顶点的 dist 值初始化为 0，然后将其放到优先级队列中。 我们从优先级队列中取出 dist 最小的顶点 minVertex，然后考察这个顶点可达的所有顶点（代码中的 nextVertex）。如果 minVertex 的 dist 值加上 minVertex 与 nextVertex 之间边的权重 w 小于 nextVertex 当前的 dist 值，也就是说，存在另一条更短的路径，它经过 minVertex 到达 nextVertex。那我们就把 nextVertex 的 dist 更新为 minVertex 的 dist 值加上 w。然后，我们把 nextVertex 加入到优先级队列中。重复这个过程，直到找到终止顶点 t 或者队列为空。 以上就是 Dijkstra 算法的核心逻辑。除此之外，代码中还有两个额外的变量，predecessor 数组和 inqueue 数组。 predecessor 数组的作用是为了还原最短路径，它记录每个顶点的前驱顶点。最后，我们通过递归的方式，将这个路径打印出来。打印路径的 print 递归代码我就不详细讲了，这个跟我们在图的搜索中讲的打印路径方法一样。如果不理解的话，你可以回过头去看下那一节。 inqueue 数组是为了避免将一个顶点多次添加到优先级队列中。我们更新了某个顶点的 dist 值之后，如果这个顶点已经在优先级队列中了，就不要再将它重复添加进去了。 看完了代码和文字解释，你可能还是有点懵，那我就举个例子，再给你解释一下。 理解了 Dijkstra 的原理和代码实现，我们来看下，Dijkstra 算法的时间复杂度是多少？ 在刚刚的代码实现中，最复杂就是 while 循环嵌套 for 循环那部分代码了。while 循环最多会执行 V 次（V 表示顶点的个数），而内部的 for 循环的执行次数不确定，跟每个顶点的相邻边的个数有关，我们分别记作 E0，E1，E2，……，E(V-1)。如果我们把这 V 个顶点的边都加起来，最大也不会超过图中所有边的个数 E（E 表示边的个数）。 for 循环内部的代码涉及从优先级队列取数据、往优先级队列中添加数据、更新优先级队列中的数据，这样三个主要的操作。我们知道，优先级队列是用堆来实现的，堆中的这几个操作，时间复杂度都是 O(logV)（堆中的元素个数不会超过顶点的个数 V）。 所以，综合这两部分，再利用乘法原则，整个代码的时间复杂度就是 O(E*logV)。 弄懂了 Dijkstra 算法，我们再来回答之前的问题，如何计算最优出行路线？ 从理论上讲，用 Dijkstra 算法可以计算出两点之间的最短路径。但是，你有没有想过，对于一个超级大地图来说，岔路口、道路都非常多，对应到图这种数据结构上来说，就有非常多的顶点和边。如果为了计算两点之间的最短路径，在一个超级大图上动用 Dijkstra 算法，遍历所有的顶点和边，显然会非常耗时。那我们有没有什么优化的方法呢？ 做工程不像做理论，一定要给出个最优解。理论上算法再好，如果执行效率太低，也无法应用到实际的工程中。对于软件开发工程师来说，我们经常要根据问题的实际背景，对解决方案权衡取舍。类似出行路线这种工程上的问题，我们没有必要非得求出个绝对最优解。很多时候，为了兼顾执行效率，我们只需要计算出一个可行的次优解就可以了。 有了这个原则，你能想出刚刚那个问题的优化方案吗？ 虽然地图很大，但是两点之间的最短路径或者说较好的出行路径，并不会很“发散”，只会出现在两点之间和两点附近的区块内。所以我们可以在整个大地图上，划出一个小的区块，这个小区块恰好可以覆盖住两个点，但又不会很大。我们只需要在这个小区块内部运行 Dijkstra 算法，这样就可以避免遍历整个大图，也就大大提高了执行效率。 不过你可能会说了，如果两点距离比较远，从北京海淀区某个地点，到上海黄浦区某个地点，那上面的这种处理方法，显然就不工作了，毕竟覆盖北京和上海的区块并不小。 我给你点提示，你可以现在打开地图 App，缩小放大一下地图，看下地图上的路线有什么变化，然后再思考，这个问题该怎么解决。 对于这样两点之间距离较远的路线规划，我们可以把北京海淀区或者北京看作一个顶点，把上海黄浦区或者上海看作一个顶点，先规划大的出行路线。比如，如何从北京到上海，必须要经过某几个顶点，或者某几条干道，然后再细化每个阶段的小路线。 这样，最短路径问题就解决了。我们再来看另外两个问题，最少时间和最少红绿灯。 前面讲最短路径的时候，每条边的权重是路的长度。在计算最少时间的时候，算法还是不变，我们只需要把边的权重，从路的长度变成经过这段路所需要的时间。不过，这个时间会根据拥堵情况时刻变化。如何计算车通过一段路的时间呢？这是一个蛮有意思的问题，你可以自己思考下。 每经过一条边，就要经过一个红绿灯。关于最少红绿灯的出行方案，实际上，我们只需要把每条边的权值改为 1 即可，算法还是不变，可以继续使用前面讲的 Dijkstra 算法。不过，边的权值为 1，也就相当于无权图了，我们还可以使用之前讲过的广度优先搜索算法。因为我们前面讲过，广度优先搜索算法计算出来的两点之间的路径，就是两点的最短路径。 不过，这里给出的所有方案都非常粗糙，只是为了给你展示，如何结合实际的场景，灵活地应用算法，让算法为我们所用，真实的地图软件的路径规划，要比这个复杂很多。而且，比起 Dijkstra 算法，地图软件用的更多的是类似 A* 的启发式搜索算法，不过也是在 Dijkstra 算法上的优化罢了，我们后面会讲到，这里暂且不展开。 总结引申今天，我们学习了一种非常重要的图算法，Dijkstra 最短路径算法。实际上，最短路径算法还有很多，比如 Bellford 算法、Floyd 算法等等。如果感兴趣，你可以自己去研究。 关于 Dijkstra 算法，我只讲了原理和代码实现。对于正确性，我没有去证明。之所以这么做，是因为证明过程会涉及比较复杂的数学推导。这个并不是我们的重点，你只要掌握这个算法的思路就可以了。 这些算法实现思路非常经典，掌握了这些思路，我们可以拿来指导、解决其他问题。比如 Dijkstra 这个算法的核心思想，就可以拿来解决下面这个看似完全不相关的问题。这个问题是我之前工作中遇到的真实的问题，为了在较短的篇幅里把问题介绍清楚，我对背景做了一些简化。 我们有一个翻译系统，只能针对单个词来做翻译。如果要翻译一整个句子，我们需要将句子拆成一个一个的单词，再丢给翻译系统。针对每个单词，翻译系统会返回一组可选的翻译列表，并且针对每个翻译打一个分，表示这个翻译的可信程度。 针对每个单词，我们从可选列表中，选择其中一个翻译，组合起来就是整个句子的翻译。每个单词的翻译的得分之和，就是整个句子的翻译得分。随意搭配单词的翻译，会得到一个句子的不同翻译。针对整个句子，我们希望计算出得分最高的前 k 个翻译结果，你会怎么编程来实现呢？ 当然，最简单的办法还是借助回溯算法，穷举所有的排列组合情况，然后选出得分最高的前 k 个翻译结果。但是，这样做的时间复杂度会比较高，是 O(m^n)，其中，m 表示平均每个单词的可选翻译个数，n 表示一个句子中包含多少个单词。这个解决方案，你可以当作回溯算法的练习题，自己编程实现一下，我就不多说了。 实际上，这个问题可以借助 Dijkstra 算法的核心思想，非常高效地解决。每个单词的可选翻译是按照分数从大到小排列的，所以 a0b0c0 肯定是得分最高组合结果。我们把 a0b0c0 及得分作为一个对象，放入到优先级队列中。 我们每次从优先级队列中取出一个得分最高的组合，并基于这个组合进行扩展。扩展的策略是每个单词的翻译分别替换成下一个单词的翻译。比如 a0b0c0 扩展后，会得到三个组合，a1b0c0、a0b1c0、a0b0c1。我们把扩展之后的组合，加到优先级队列中。重复这个过程，直到获取到 k 个翻译组合或者队列为空。 我们来看，这种实现思路的时间复杂度是多少？ 假设句子包含 n 个单词，每个单词平均有 m 个可选的翻译，我们求得分最高的前 k 个组合结果。每次一个组合出队列，就对应着一个组合结果，我们希望得到 k 个，那就对应着 k 次出队操作。每次有一个组合出队列，就有 n 个组合入队列。优先级队列中出队和入队操作的时间复杂度都是 O(logX)，X 表示队列中的组合个数。所以，总的时间复杂度就是 O(knlogX)。那 X 到底是多少呢？ k 次出入队列，队列中的总数据不会超过 kn，也就是说，出队、入队操作的时间复杂度是 O(log(kn))。所以，总的时间复杂度就是 O(knlog(k*n))，比之前的指数级时间复杂度降低了很多。 课后思考在计算最短时间的出行路线中，如何获得通过某条路的时间呢？这个题目很有意思，我之前面试的时候也被问到过，你可以思考看看。 今天讲的出行路线问题，我假设的是开车出行，那如果是公交出行呢？如果混合地铁、公交、步行，又该如何规划路线呢？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 45 | 位图：如何实现网页爬虫中的URL去重功能？王争 2019-01-09  00:09 1.25x 讲述：冯永吉 大小：13.84M 时长：15:06 网页爬虫是搜索引擎中的非常重要的系统，负责爬取几十亿、上百亿的网页。爬虫的工作原理是，通过解析已经爬取页面中的网页链接，然后再爬取这些链接对应的网页。而同一个网页链接有可能被包含在多个页面中，这就会导致爬虫在爬取的过程中，重复爬取相同的网页。如果你是一名负责爬虫的工程师，你会如何避免这些重复的爬取呢？ 最容易想到的方法就是，我们记录已经爬取的网页链接（也就是 URL），在爬取一个新的网页之前，我们拿它的链接，在已经爬取的网页链接列表中搜索。如果存在，那就说明这个网页已经被爬取过了；如果不存在，那就说明这个网页还没有被爬取过，可以继续去爬取。等爬取到这个网页之后，我们将这个网页的链接添加到已经爬取的网页链接列表了。 思路非常简单，我想你应该很容易就能想到。不过，我们该如何记录已经爬取的网页链接呢？需要用什么样的数据结构呢？ 算法解析关于这个问题，我们可以先回想下，是否可以用我们之前学过的数据结构来解决呢？ 这个问题要处理的对象是网页链接，也就是 URL，需要支持的操作有两个，添加一个 URL 和查询一个 URL。除了这两个功能性的要求之外，在非功能性方面，我们还要求这两个操作的执行效率要尽可能高。除此之外，因为我们处理的是上亿的网页链接，内存消耗会非常大，所以在存储效率上，我们要尽可能地高效。 我们回想一下，满足这些条件的数据结构有哪些呢？显然，散列表、红黑树、跳表这些动态数据结构，都能支持快速地插入、查找数据，但是在内存消耗方面，是否可以接受呢？ 我们拿散列表来举例。假设我们要爬取 10 亿个网页（像 Google、百度这样的通用搜索引擎，爬取的网页可能会更多），为了判重，我们把这 10 亿网页链接存储在散列表中。你来估算下，大约需要多少内存？ 假设一个 URL 的平均长度是 64 字节，那单纯存储这 10 亿个 URL，需要大约 60GB 的内存空间。因为散列表必须维持较小的装载因子，才能保证不会出现过多的散列冲突，导致操作的性能下降。而且，用链表法解决冲突的散列表，还会存储链表指针。所以，如果将这 10 亿个 URL 构建成散列表，那需要的内存空间会远大于 60GB，有可能会超过 100GB。 当然，对于一个大型的搜索引擎来说，即便是 100GB 的内存要求，其实也不算太高，我们可以采用分治的思想，用多台机器（比如 20 台内存是 8GB 的机器）来存储这 10 亿网页链接。这种分治的处理思路，我们讲过很多次了，这里就不详细说了。 对于爬虫的 URL 去重这个问题，刚刚讲到的分治加散列表的思路，已经是可以实实在在工作的了。不过，作为一个有追求的工程师，我们应该考虑，在添加、查询数据的效率以及内存消耗方面，是否还有进一步的优化空间呢？ 你可能会说，散列表中添加、查找数据的时间复杂度已经是 O(1)，还能有进一步优化的空间吗？实际上，我们前面也讲过，时间复杂度并不能完全代表代码的执行时间。大 O 时间复杂度表示法，会忽略掉常数、系数和低阶，并且统计的对象是语句的频度。不同的语句，执行时间也是不同的。时间复杂度只是表示执行时间随数据规模的变化趋势，并不能度量在特定的数据规模下，代码执行时间的多少。 如果时间复杂度中原来的系数是 10，我们现在能够通过优化，将系数降为 1，那在时间复杂度没有变化的情况下，执行效率就提高了 10 倍。对于实际的软件开发来说，10 倍效率的提升，显然是一个非常值得的优化。 如果我们用基于链表的方法解决冲突问题，散列表中存储的是 URL，那当查询的时候，通过哈希函数定位到某个链表之后，我们还需要依次比对每个链表中的 URL。这个操作是比较耗时的，主要有两点原因。 一方面，链表中的结点在内存中不是连续存储的，所以不能一下子加载到 CPU 缓存中，没法很好地利用到 CPU 高速缓存，所以数据访问性能方面会打折扣。 另一方面，链表中的每个数据都是 URL，而 URL 不是简单的数字，是平均长度为 64 字节的字符串。也就是说，我们要让待判重的 URL，跟链表中的每个 URL，做字符串匹配。显然，这样一个字符串匹配操作，比起单纯的数字比对，要慢很多。所以，基于这两点，执行效率方面肯定是有优化空间的。 对于内存消耗方面的优化，除了刚刚这种基于散列表的解决方案，貌似没有更好的法子了。实际上，如果要想内存方面有明显的节省，那就得换一种解决方案，也就是我们今天要着重讲的这种存储结构，布隆过滤器（Bloom Filter）。 在讲布隆过滤器前，我要先讲一下另一种存储结构，位图（BitMap）。因为，布隆过滤器本身就是基于位图的，是对位图的一种改进。 我们先来看一个跟开篇问题非常类似、但比那个稍微简单的问题。我们有 1 千万个整数，整数的范围在 1 到 1 亿之间。如何快速查找某个整数是否在这 1 千万个整数中呢？ 当然，这个问题还是可以用散列表来解决。不过，我们可以使用一种比较“特殊”的散列表，那就是位图。我们申请一个大小为 1 亿、数据类型为布尔类型（true 或者 false）的数组。我们将这 1 千万个整数作为数组下标，将对应的数组值设置成 true。比如，整数 5 对应下标为 5 的数组值设置为 true，也就是 array[5]=true。 当我们查询某个整数 K 是否在这 1 千万个整数中的时候，我们只需要将对应的数组值 array[K]取出来，看是否等于 true。如果等于 true，那说明 1 千万整数中包含这个整数 K；相反，就表示不包含这个整数 K。 不过，很多语言中提供的布尔类型，大小是 1 个字节的，并不能节省太多内存空间。实际上，表示 true 和 false 两个值，我们只需要用一个二进制位（bit）就可以了。那如何通过编程语言，来表示一个二进制位呢？ 这里就要用到位运算了。我们可以借助编程语言中提供的数据类型，比如 int、long、char 等类型，通过位运算，用其中的某个位表示某个数字。文字描述起来有点儿不好理解，我把位图的代码实现写了出来，你可以对照着代码看下，应该就能看懂了。 public class BitMap { // Java中char类型占16bit，也即是2个字节 private char[] bytes; private int nbits; public BitMap(int nbits) { ​ this.nbits = nbits; ​ this.bytes = new char[nbits/16+1]; } public void set(int k) { ​ if (k &gt; nbits) return; ​ int byteIndex = k / 16; ​ int bitIndex = k % 16; ​ bytes[byteIndex] |= (1 &lt;&lt; bitIndex); } public boolean get(int k) { ​ if (k &gt; nbits) return false; ​ int byteIndex = k / 16; ​ int bitIndex = k % 16; ​ return (bytes[byteIndex] &amp; (1 &lt;&lt; bitIndex)) != 0; } } 从刚刚位图结构的讲解中，你应该可以发现，位图通过数组下标来定位数据，所以，访问效率非常高。而且，每个数字用一个二进制位来表示，在数字范围不大的情况下，所需要的内存空间非常节省。 比如刚刚那个例子，如果用散列表存储这 1 千万的数据，数据是 32 位的整型数，也就是需要 4 个字节的存储空间，那总共至少需要 40MB 的存储空间。如果我们通过位图的话，数字范围在 1 到 1 亿之间，只需要 1 亿个二进制位，也就是 12MB 左右的存储空间就够了。 关于位图，我们就讲完了，是不是挺简单的？不过，这里我们有个假设，就是数字所在的范围不是很大。如果数字的范围很大，比如刚刚那个问题，数字范围不是 1 到 1 亿，而是 1 到 10 亿，那位图的大小就是 10 亿个二进制位，也就是 120MB 的大小，消耗的内存空间，不降反增。 这个时候，布隆过滤器就要出场了。布隆过滤器就是为了解决刚刚这个问题，对位图这种数据结构的一种改进。 还是刚刚那个例子，数据个数是 1 千万，数据的范围是 1 到 10 亿。布隆过滤器的做法是，我们仍然使用一个 1 亿个二进制大小的位图，然后通过哈希函数，对数字进行处理，让它落在这 1 到 1 亿范围内。比如我们把哈希函数设计成 f(x)=x%n。其中，x 表示数字，n 表示位图的大小（1 亿），也就是，对数字跟位图的大小进行取模求余。 不过，你肯定会说，哈希函数会存在冲突的问题啊，一亿零一和 1 两个数字，经过你刚刚那个取模求余的哈希函数处理之后，最后的结果都是 1。这样我就无法区分，位图存储的是 1 还是一亿零一了。 为了降低这种冲突概率，当然我们可以设计一个复杂点、随机点的哈希函数。除此之外，还有其他方法吗？我们来看布隆过滤器的处理方法。既然一个哈希函数可能会存在冲突，那用多个哈希函数一块儿定位一个数据，是否能降低冲突的概率呢？我来具体解释一下，布隆过滤器是怎么做的。 我们使用 K 个哈希函数，对同一个数字进行求哈希值，那会得到 K 个不同的哈希值，我们分别记作 X1，X2，X3，…，X*K。我们把这 K 个数字作为位图中的下标，将对应的 BitMap[X1]，BitMap[X2]，BitMap[X3]，…，BitMap[X*K]都设置成 true，也就是说，我们用 K 个二进制位，来表示一个数字的存在。 当我们要查询某个数字是否存在的时候，我们用同样的 K 个哈希函数，对这个数字求哈希值，分别得到 Y1，Y2，Y3，…，Y*K*。我们看这 K 个哈希值，对应位图中的数值是否都为 true，如果都是 true，则说明，这个数字存在，如果有其中任意一个不为 true，那就说明这个数字不存在。 对于两个不同的数字来说，经过一个哈希函数处理之后，可能会产生相同的哈希值。但是经过 K 个哈希函数处理之后，K 个哈希值都相同的概率就非常低了。尽管采用 K 个哈希函数之后，两个数字哈希冲突的概率降低了，但是，这种处理方式又带来了新的问题，那就是容易误判。我们看下面这个例子。 布隆过滤器的误判有一个特点，那就是，它只会对存在的情况有误判。如果某个数字经过布隆过滤器判断不存在，那说明这个数字真的不存在，不会发生误判；如果某个数字经过布隆过滤器判断存在，这个时候才会有可能误判，有可能并不存在。不过，只要我们调整哈希函数的个数、位图大小跟要存储数字的个数之间的比例，那就可以将这种误判的概率降到非常低。 尽管布隆过滤器会存在误判，但是，这并不影响它发挥大作用。很多场景对误判有一定的容忍度。比如我们今天要解决的爬虫判重这个问题，即便一个没有被爬取过的网页，被误判为已经被爬取，对于搜索引擎来说，也并不是什么大事情，是可以容忍的，毕竟网页太多了，搜索引擎也不可能 100% 都爬取到。 弄懂了布隆过滤器，我们今天的爬虫网页去重的问题，就很简单了。 我们用布隆过滤器来记录已经爬取过的网页链接，假设需要判重的网页有 10 亿，那我们可以用一个 10 倍大小的位图来存储，也就是 100 亿个二进制位，换算成字节，那就是大约 1.2GB。之前我们用散列表判重，需要至少 100GB 的空间。相比来讲，布隆过滤器在存储空间的消耗上，降低了非常多。 那我们再来看下，利用布隆过滤器，在执行效率方面，是否比散列表更加高效呢？ 布隆过滤器用多个哈希函数对同一个网页链接进行处理，CPU 只需要将网页链接从内存中读取一次，进行多次哈希计算，理论上讲这组操作是 CPU 密集型的。而在散列表的处理方式中，需要读取散列值相同（散列冲突）的多个网页链接，分别跟待判重的网页链接，进行字符串匹配。这个操作涉及很多内存数据的读取，所以是内存密集型的。我们知道 CPU 计算可能是要比内存访问更快速的，所以，理论上讲，布隆过滤器的判重方式，更加快速。 总结引申今天，关于搜索引擎爬虫网页去重问题的解决，我们从散列表讲到位图，再讲到布隆过滤器。布隆过滤器非常适合这种不需要 100% 准确的、允许存在小概率误判的大规模判重场景。除了爬虫网页去重这个例子，还有比如统计一个大型网站的每天的 UV 数，也就是每天有多少用户访问了网站，我们就可以使用布隆过滤器，对重复访问的用户进行去重。 我们前面讲到，布隆过滤器的误判率，主要跟哈希函数的个数、位图的大小有关。当我们往布隆过滤器中不停地加入数据之后，位图中不是 true 的位置就越来越少了，误判率就越来越高了。所以，对于无法事先知道要判重的数据个数的情况，我们需要支持自动扩容的功能。 当布隆过滤器中，数据个数与位图大小的比例超过某个阈值的时候，我们就重新申请一个新的位图。后面来的新数据，会被放置到新的位图中。但是，如果我们要判断某个数据是否在布隆过滤器中已经存在，我们就需要查看多个位图，相应的执行效率就降低了一些。 位图、布隆过滤器应用如此广泛，很多编程语言都已经实现了。比如 Java 中的 BitSet 类就是一个位图，Redis 也提供了 BitMap 位图类，Google 的 Guava 工具包提供了 BloomFilter 布隆过滤器的实现。如果你感兴趣，你可以自己去研究下这些实现的源码。 课后思考假设我们有 1 亿个整数，数据范围是从 1 到 10 亿，如何快速并且省内存地给这 1 亿个数据从小到大排序？ 还记得我们在哈希函数（下）讲过的利用分治思想，用散列表以及哈希函数，实现海量图库中的判重功能吗？如果我们允许小概率的误判，那是否可以用今天的布隆过滤器来解决呢？你可以参照我们当时的估算方法，重新估算下，用布隆过滤器需要多少台机器？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 46 | 概率统计：如何利用朴素贝叶斯算法过滤垃圾短信？王争 2019-01-11  00:08 1.25x 讲述：冯永吉 大小：13.37M 时长：14:35 上一节我们讲到，如何用位图、布隆过滤器，来过滤重复的数据。今天，我们再讲一个跟过滤相关的问题，如何过滤垃圾短信？ 垃圾短信和骚扰电话，我想每个人都收到过吧？买房、贷款、投资理财、开发票，各种垃圾短信和骚扰电话，不胜其扰。如果你是一名手机应用开发工程师，让你实现一个简单的垃圾短信过滤功能以及骚扰电话拦截功能，该用什么样的数据结构和算法实现呢？ 算法解析实际上，解决这个问题并不会涉及很高深的算法。今天，我就带你一块看下，如何利用简单的数据结构和算法，解决这种看似非常复杂的问题。 1. 基于黑名单的过滤器我们可以维护一个骚扰电话号码和垃圾短信发送号码的黑名单。这个黑名单的收集，有很多途径，比如，我们可以从一些公开的网站上下载，也可以通过类似“360 骚扰电话拦截”的功能，通过用户自主标记骚扰电话来收集。对于被多个用户标记，并且标记个数超过一定阈值的号码，我们就可以定义为骚扰电话，并将它加入到我们的黑名单中。 如果黑名单中的电话号码不多的话，我们可以使用散列表、二叉树等动态数据结构来存储，对内存的消耗并不会很大。如果我们把每个号码看作一个字符串，并且假设平均长度是 16 个字节，那存储 50 万个电话号码，大约需要 10MB 的内存空间。即便是对于手机这样的内存有限的设备来说，这点内存的消耗也是可以接受的。 但是，如果黑名单中的电话号码很多呢？比如有 500 万个。这个时候，如果再用散列表存储，就需要大约 100MB 的存储空间。为了实现一个拦截功能，耗费用户如此多的手机内存，这显然有点儿不合理。 上一节我们讲了，布隆过滤器最大的特点就是比较省存储空间，所以，用它来解决这个问题再合适不过了。如果我们要存储 500 万个手机号码，我们把位图大小设置为 10 倍数据大小，也就是 5000 万，那也只需要使用 5000 万个二进制位（5000 万 bits），换算成字节，也就是不到 7MB 的存储空间。比起散列表的解决方案，内存的消耗减少了很多。 实际上，我们还有一种时间换空间的方法，可以将内存的消耗优化到极致。 我们可以把黑名单存储在服务器端上，把过滤和拦截的核心工作，交给服务器端来做。手机端只负责将要检查的号码发送给服务器端，服务器端通过查黑名单，判断这个号码是否应该被拦截，并将结果返回给手机端。 用这个解决思路完全不需要占用手机内存。不过，有利就有弊。我们知道，网络通信是比较慢的，所以，网络延迟就会导致处理速度降低。而且，这个方案还有个硬性要求，那就是只有在联网的情况下，才能正常工作。 基于黑名单的过滤器我就讲完了，不过，你可能还会说，布隆过滤器会有判错的概率呀！如果它把一个重要的电话或者短信，当成垃圾短信或者骚扰电话拦截了，对于用户来说，这是无法接受的。你说得没错，这是一个很大的问题。不过，我们现在先放一放，等三种过滤器都讲完之后，我再来解答。 2. 基于规则的过滤器刚刚讲了一种基于黑名单的垃圾短信过滤方法，但是，如果某个垃圾短信发送者的号码并不在黑名单中，那这种方法就没办法拦截了。所以，基于黑名单的过滤方式，还不够完善，我们再继续看一种基于规则的过滤方式。 对于垃圾短信来说，我们还可以通过短信的内容，来判断某条短信是否是垃圾短信。我们预先设定一些规则，如果某条短信符合这些规则，我们就可以判定它是垃圾短信。实际上，规则可以有很多，比如下面这几个： 短信中包含特殊单词（或词语），比如一些非法、淫秽、反动词语等； 短信发送号码是群发号码，非我们正常的手机号码，比如 +60389585； 短信中包含回拨的联系方式，比如手机号码、微信、QQ、网页链接等，因为群发短信的号码一般都是无法回拨的； 短信格式花哨、内容很长，比如包含各种表情、图片、网页链接等； 符合已知垃圾短信的模板。垃圾短信一般都是重复群发，对于已经判定为垃圾短信的短信，我们可以抽象成模板，将获取到的短信与模板匹配，一旦匹配，我们就可以判定为垃圾短信。 当然，如果短信只是满足其中一条规则，如果就判定为垃圾短信，那会存在比较大的误判的情况。我们可以综合多条规则进行判断。比如，满足 2 条以上才会被判定为垃圾短信；或者每条规则对应一个不同的得分，满足哪条规则，我们就累加对应的分数，某条短信的总得分超过某个阈值，才会被判定为垃圾短信。 不过，我只是给出了一些制定规则的思路，具体落实到执行层面，其实还有很大的距离，还有很多细节需要处理。比如，第一条规则中，我们该如何定义特殊单词；第二条规则中，我们该如何定义什么样的号码是群发号码等等。限于篇幅，我就不一一详细展开来讲了。我这里只讲一下，如何定义特殊单词？ 如果我们只是自己拍脑袋想，哪些单词属于特殊单词，那势必有比较大的主观性，也很容易漏掉某些单词。实际上，我们可以基于概率统计的方法，借助计算机强大的计算能力，找出哪些单词最常出现在垃圾短信中，将这些最常出现的单词，作为特殊单词，用来过滤短信。 不过这种方法的前提是，我们有大量的样本数据，也就是说，要有大量的短信（比如 1000 万条短信），并且我们还要求，每条短信都做好了标记，它是垃圾短信还是非垃圾短信。 我们对这 1000 万条短信，进行分词处理（借助中文或者英文分词算法），去掉“的、和、是”等没有意义的停用词（Stop words），得到 n 个不同的单词。针对每个单词，我们统计有多少个垃圾短信出现了这个单词，有多少个非垃圾短信会出现这个单词，进而求出每个单词出现在垃圾短信中的概率，以及出现在非垃圾短信中的概率。如果某个单词出现在垃圾短信中的概率，远大于出现在非垃圾短信中的概率，那我们就把这个单词作为特殊单词，用来过滤垃圾短信。 文字描述不好理解，我举个例子来解释一下。 3. 基于概率统计的过滤器基于规则的过滤器，看起来很直观，也很好理解，但是它也有一定的局限性。一方面，这些规则受人的思维方式局限，规则未免太过简单；另一方面，垃圾短信发送者可能会针对规则，精心设计短信，绕过这些规则的拦截。对此，我们再来看一种更加高级的过滤方式，基于概率统计的过滤方式。 这种基于概率统计的过滤方式，基础理论是基于朴素贝叶斯算法。为了让你更好地理解下面的内容，我们先通过一个非常简单的例子来看下，什么是朴素贝叶斯算法？ 假设事件 A 是“小明不去上学”，事件 B 是“下雨了”。我们现在统计了一下过去 10 天的下雨情况和小明上学的情况，作为样本数据。 我们来分析一下，这组样本有什么规律。在这 10 天中，有 4 天下雨，所以下雨的概率 P(B)=4/10。10 天中有 3 天，小明没有去上学，所以小明不去上学的概率 P(A)=3/10。在 4 个下雨天中，小明有 2 天没去上学，所以下雨天不去上学的概率 P(A|B)=2/4。在小明没有去上学的 3 天中，有 2 天下雨了，所以小明因为下雨而不上学的概率是 P(B|A)=2/3。实际上，这 4 个概率值之间，有一定的关系，这个关系就是朴素贝叶斯算法，我们用公式表示出来，就是下面这个样子。 朴素贝叶斯算法是不是非常简单？我们用一个公式就可以将它概括。弄懂了朴素贝叶斯算法，我们再回到垃圾短信过滤这个问题上，看看如何利用朴素贝叶斯算法，来做垃圾短信的过滤。 基于概率统计的过滤器，是基于短信内容来判定是否是垃圾短信。而计算机没办法像人一样理解短信的含义。所以，我们需要把短信抽象成一组计算机可以理解并且方便计算的特征项，用这一组特征项代替短信本身，来做垃圾短信过滤。 我们可以通过分词算法，把一个短信分割成 n 个单词。这 n 个单词就是一组特征项，全权代表这个短信。因此，判定一个短信是否是垃圾短信这样一个问题，就变成了，判定同时包含这几个单词的短信是否是垃圾短信。 不过，这里我们并不像基于规则的过滤器那样，非黑即白，一个短信要么被判定为垃圾短信、要么被判定为非垃圾短息。我们使用概率，来表征一个短信是垃圾短信的可信程度。如果我们用公式将这个概率表示出来，就是下面这个样子： 尽管我们有大量的短信样本，但是我们没法通过样本数据统计得到这个概率。为什么不可以呢？你可能会说，我只需要统计同时包含 W1，W2，W3，…，W**n 这 n 个单词的短信有多少个（我们假设有 x 个），然后看这里面属于垃圾短信的有几个（我们假设有 y 个），那包含 W1，W2，W3，…，W**n 这 n 个单词的短信是垃圾短信的概率就是 y/x。 理想很丰满，但现实往往很骨感。你忽视了非常重要的一点，那就是样本的数量再大，毕竟也是有限的，样本中不会有太多同时包含 W1，W2，W3，…，W**n 的短信的，甚至很多时候，样本中根本不存在这样的短信。没有样本，也就无法计算概率。所以这样的推理方式虽然正确，但是实践中并不好用。 这个时候，朴素贝叶斯公式就可以派上用场了。我们通过朴素贝叶斯公式，将这个概率的求解，分解为其他三个概率的求解。你可以看我画的图。那转化之后的三个概率是否可以通过样本统计得到呢？ P（W1，W2，W3，…，W**n 同时出现在一条短信中 | 短信是垃圾短信）这个概率照样无法通过样本来统计得到。但是我们可以基于下面这条著名的概率规则来计算。 独立事件发生的概率计算公式：P(AB) = P(A)P(B) 如果事件 A 和事件 B 是独立事件，两者的发生没有相关性，事件 A 发生的概率 P(A) 等于 p1，事件 B 发生的概率 P(B) 等于 p2，那两个同时发生的概率 P(AB) 就等于 P(A)P(B)。 基于这条独立事件发生概率的计算公式，我们可以把 P（W1，W2，W3，…，Wn 同时出现在一条短信中 | 短信是垃圾短信）分解为下面这个公式： 其中，P（W**i 出现在短信中 | 短信是垃圾短信）表示垃圾短信中包含 W**i 这个单词的概率有多大。这个概率值通过统计样本很容易就能获得。我们假设垃圾短信有 y 个，其中包含 W**i 的有 x 个，那这个概率值就等于 x/y。 P（W1，W2，W3，…，W**n 同时出现在一条短信中 | 短信是垃圾短信）这个概率值，我们就计算出来了，我们再来看下剩下两个。 P（短信是垃圾短信）表示短信是垃圾短信的概率，这个很容易得到。我们把样本中垃圾短信的个数除以总样本短信个数，就是短信是垃圾短信的概率。 不过，P（W1，W2，W3，…，W**n 同时出现在一条短信中）这个概率还是不好通过样本统计得到，原因我们前面说过了，样本空间有限。不过，我们没必要非得计算这一部分的概率值。为什么这么说呢？ 实际上，我们可以分别计算同时包含 W1，W2，W3，…，W**n 这 n 个单词的短信，是垃圾短信和非垃圾短信的概率。假设它们分别是 p1 和 p2。我们并不需要单纯地基于 p1 值的大小来判断是否是垃圾短信，而是通过对比 p1 和 p2 值的大小，来判断一条短信是否是垃圾短信。更细化一点讲，那就是，如果 p1 是 p2 的很多倍（比如 10 倍），我们才确信这条短信是垃圾短信。 基于这两个概率的倍数来判断是否是垃圾短信的方法，我们就可以不用计算 P（W1，W2，W3，…，W**n 同时出现在一条短信中）这一部分的值了，因为计算 p1 与 p2 的时候，都会包含这个概率值的计算，所以在求解 p1 和 p2 倍数（p1/p2）的时候，我们也就不需要这个值。 总结引申今天，我们讲了基于黑名单、规则、概率统计三种垃圾短信的过滤方法，实际上，今天讲的这三种方法，还可以应用到很多类似的过滤、拦截的领域，比如垃圾邮件的过滤等等。 在讲黑名单过滤的时候，我讲到布隆过滤器可能会存在误判情况，可能会导致用户投诉。实际上，我们可以结合三种不同的过滤方式的结果，对同一个短信处理，如果三者都表明这个短信是垃圾短信，我们才把它当作垃圾短信拦截过滤，这样就会更精准。 当然，在实际的工程中，我们还需要结合具体的场景，以及大量的实验，不断去调整策略，权衡垃圾短信判定的准确率（是否会把不是垃圾的短信错判为垃圾短信）和召回率（是否能把所有的垃圾短信都找到），来实现我们的需求。 课后思考关于垃圾短信过滤和骚扰电话的拦截，我们可以一块儿头脑风暴一下，看看你还有没有其他方法呢？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 47 | 向量空间：如何实现一个简单的音乐推荐系统？王争 2019-01-14  00:07 1.25x 讲述：冯永吉 大小：7.67M 时长：08:22 很多人都喜爱听歌，以前我们用 MP3 听歌，现在直接通过音乐 App 在线就能听歌。而且，各种音乐 App 的功能越来越强大，不仅可以自己选歌听，还可以根据你听歌的口味偏好，给你推荐可能会喜爱的音乐，而且有时候，推荐的音乐还非常适合你的口味，甚至会惊艳到你！如此智能的一个功能，你知道它是怎么实现的吗？ 算法解析实际上，要解决这个问题，并不需要特别高深的理论。解决思路的核心思想非常简单、直白，用两句话就能总结出来。 找到跟你口味偏好相似的用户，把他们爱听的歌曲推荐给你； 找出跟你喜爱的歌曲特征相似的歌曲，把这些歌曲推荐给你。 接下来，我就分别讲解一下这两种思路的具体实现方法。 1. 基于相似用户做推荐如何找到跟你口味偏好相似的用户呢？或者说如何定义口味偏好相似呢？实际上，思路也很简单，我们把跟你听类似歌曲的人，看作口味相似的用户。你可以看我下面画的这个图。我用“1”表示“喜爱”，用“0”笼统地表示“不发表意见”。从图中我们可以看出，你跟小明共同喜爱的歌曲最多，有 5 首。于是，我们就可以说，小明跟你的口味非常相似。 我们只需要遍历所有的用户，对比每个用户跟你共同喜爱的歌曲个数，并且设置一个阈值，如果你和某个用户共同喜爱的歌曲个数超过这个阈值，我们就把这个用户看作跟你口味相似的用户，把这个用户喜爱但你还没听过的歌曲，推荐给你。 不过，刚刚的这个解决方案中有一个问题，我们如何知道用户喜爱哪首歌曲呢？也就是说，如何定义用户对某首歌曲的喜爱程度呢？ 实际上，我们可以通过用户的行为，来定义这个喜爱程度。我们给每个行为定义一个得分，得分越高表示喜爱程度越高。 还是刚刚那个例子，我们如果把每个人对每首歌曲的喜爱程度表示出来，就是下面这个样子。图中，某个人对某首歌曲是否喜爱，我们不再用“1”或者“0”来表示，而是对应一个具体的分值。 有了这样一个用户对歌曲的喜爱程度的对应表之后，如何来判断两个用户是否口味相似呢？ 显然，我们不能再像之前那样，采用简单的计数来统计两个用户之间的相似度。还记得我们之前讲字符串相似度度量时，提到的编辑距离吗？这里的相似度度量，我们可以使用另外一个距离，那就是欧几里得距离（Euclidean distance）。欧几里得距离是用来计算两个向量之间的距离的。这个概念中有两个关键词，向量和距离，我来给你解释一下。 一维空间是一条线，我们用 1，2，3……这样单个的数，来表示一维空间中的某个位置；二维空间是一个面，我们用（1，3）（4，2）（2，2）……这样的两个数，来表示二维空间中的某个位置；三维空间是一个立体空间，我们用（1，3，5）（3，1，7）（2，4，3）……这样的三个数，来表示三维空间中的某个位置。一维、二维、三维应该都不难理解，那更高维中的某个位置该如何表示呢？ 类比一维、二维、三维的表示方法，K 维空间中的某个位置，我们可以写作（X1，X2，X3，…，X*K）。这种表示方法就是*向量**（vector）。我们知道，二维、三维空间中，两个位置之间有距离的概念，类比到高纬空间，同样也有距离的概念，这就是我们说的两个向量之间的距离。 那如何计算两个向量之间的距离呢？我们还是可以类比到二维、三维空间中距离的计算方法。通过类比，我们就可以得到两个向量之间距离的计算公式。这个计算公式就是欧几里得距离的计算公式： 我们把每个用户对所有歌曲的喜爱程度，都用一个向量表示。我们计算出两个向量之间的欧几里得距离，作为两个用户的口味相似程度的度量。从图中的计算可以看出，小明与你的欧几里得距离距离最小，也就是说，你俩在高维空间中靠得最近，所以，我们就断定，小明跟你的口味最相似。 2. 基于相似歌曲做推荐刚刚我们讲了基于相似用户的歌曲推荐方法，但是，如果用户是一个新用户，我们还没有收集到足够多的行为数据，这个时候该如何推荐呢？我们现在再来看另外一种推荐方法，基于相似歌曲的推荐方法，也就是说，如果某首歌曲跟你喜爱的歌曲相似，我们就把它推荐给你。 如何判断两首歌曲是否相似呢？对于人来说，这个事情可能会比较简单，但是对于计算机来说，判断两首歌曲是否相似，那就需要通过量化的数据来表示了。我们应该通过什么数据来量化两个歌曲之间的相似程度呢？ 最容易想到的是，我们对歌曲定义一些特征项，比如是伤感的还是愉快的，是摇滚还是民谣，是柔和的还是高亢的等等。类似基于相似用户的推荐方法，我们给每个歌曲的每个特征项打一个分数，这样每个歌曲就都对应一个特征项向量。我们可以基于这个特征项向量，来计算两个歌曲之间的欧几里得距离。欧几里得距离越小，表示两个歌曲的相似程度越大。 但是，要实现这个方案，需要有一个前提，那就是我们能够找到足够多，并且能够全面代表歌曲特点的特征项，除此之外，我们还要人工给每首歌标注每个特征项的得分。对于收录了海量歌曲的音乐 App 来说，这显然是一个非常大的工程。此外，人工标注有很大的主观性，也会影响到推荐的准确性。 既然基于歌曲特征项计算相似度不可行，那我们就换一种思路。对于两首歌，如果喜欢听的人群都是差不多的，那侧面就可以反映出，这两首歌比较相似。如图所示，每个用户对歌曲有不同的喜爱程度，我们依旧通过上一个解决方案中定义得分的标准，来定义喜爱程度。 你有没有发现，这个图跟基于相似用户推荐中的图几乎一样。只不过这里把歌曲和用户主次颠倒了一下。基于相似用户的推荐方法中，针对每个用户，我们将对各个歌曲的喜爱程度作为向量。基于相似歌曲的推荐思路中，针对每个歌曲，我们将每个用户的打分作为向量。 有了每个歌曲的向量表示，我们通过计算向量之间的欧几里得距离，来表示歌曲之间的相似度。欧几里得距离越小，表示两个歌曲越相似。然后，我们就在用户已经听过的歌曲中，找出他喜爱程度较高的歌曲。然后，我们找出跟这些歌曲相似度很高的其他歌曲，推荐给他。 总结引申实际上，这个问题是推荐系统（Recommendation System）里最典型的一类问题。之所以讲这部分内容，主要还是想给你展示，算法的强大之处，利用简单的向量空间的欧几里得距离，就能解决如此复杂的问题。不过，今天，我只给你讲解了基本的理论，实践中遇到的问题还有很多，比如冷启动问题，产品初期积累的数据不多，不足以做推荐等等。这些更加深奥的内容，你可以之后自己在实践中慢慢探索。 课后思考关于今天讲的推荐算法，你还能想到其他应用场景吗？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 48 | B+树：MySQL数据库索引是如何实现的？王争 2019-01-16  00:23 1.25x 讲述：冯永吉 大小：12.84M 时长：14:00 作为一个软件开发工程师，你对数据库肯定再熟悉不过了。作为主流的数据存储系统，它在我们的业务开发中，有着举足轻重的地位。在工作中，为了加速数据库中数据的查找速度，我们常用的处理思路是，对表中数据创建索引。那你是否思考过，数据库索引是如何实现的呢？底层使用的是什么数据结构和算法呢？ 算法解析思考的过程比结论更重要。跟着我学习了这么多节课，很多同学已经意识到这一点，比如 Jerry 银银同学。我感到很开心。所以，今天的讲解，我会尽量还原这个解决方案的思考过程，让你知其然，并且知其所以然。 1. 解决问题的前提是定义清楚问题如何定义清楚问题呢？除了对问题进行详细的调研，还有一个办法，那就是，通过对一些模糊的需求进行假设，来限定**要**解决的问题的范围。 如果你对数据库的操作非常了解，针对我们现在这个问题，你就能把索引的需求定义得非常清楚。但是，对于大部分软件工程师来说，我们可能只了解一小部分常用的 SQL 语句，所以，这里我们假设要解决的问题，只包含这样两个常用的需求： 根据某个值查找数据，比如 select * from user where id=1234； 根据区间值来查找某些数据，比如 select * from user where id &gt; 1234 and id &lt; 2345。 除了这些功能性需求之外，这种问题往往还会涉及一些非功能性需求，比如安全、性能、用户体验等等。限于专栏要讨论的主要是数据结构和算法，对于非功能性需求，我们着重考虑性能方面的需求。性能方面的需求，我们主要考察时间和空间两方面，也就是执行效率和存储空间。 在执行效率方面，我们希望通过索引，查询数据的效率尽可能地高；在存储空间方面，我们希望索引不要消耗太多的内存空间。 2. 尝试用学过的数据结构解决这个问题问题的需求大致定义清楚了，我们现在回想一下，能否利用已经学习过的数据结构解决这个问题呢？支持快速查询、插入等操作的动态数据结构，我们已经学习过散列表、平衡二叉查找树、跳表。 我们先来看散列表。散列表的查询性能很好，时间复杂度是 O(1)。但是，散列表不能支持按照区间快速查找数据。所以，散列表不能满足我们的需求。 我们再来看平衡二叉查找树。尽管平衡二叉查找树查询的性能也很高，时间复杂度是 O(logn)。而且，对树进行中序遍历，我们还可以得到一个从小到大有序的数据序列，但这仍然不足以支持按照区间快速查找数据。 我们再来看跳表。跳表是在链表之上加上多层索引构成的。它支持快速地插入、查找、删除数据，对应的时间复杂度是 O(logn)。并且，跳表也支持按照区间快速地查找数据。我们只需要定位到区间起点值对应在链表中的结点，然后从这个结点开始，顺序遍历链表，直到区间终点对应的结点为止，这期间遍历得到的数据就是满足区间值的数据。 这样看来，跳表是可以解决这个问题。实际上，数据库索引所用到的数据结构跟跳表非常相似，叫作 B+ 树。不过，它是通过二叉查找树演化过来的，而非跳表。为了给你还原发明 B+ 树的整个思考过程，所以，接下来，我还要从二叉查找树讲起，看它是如何一步一步被改造成 B+ 树的。 3. 改造二叉查找树来解决这个问题为了让二叉查找树支持按照区间来查找数据，我们可以对它进行这样的改造：树中的节点并不存储数据本身，而是只是作为索引。除此之外，我们把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的。经过改造之后的二叉树，就像图中这样，看起来是不是很像跳表呢？ 改造之后，如果我们要求某个区间的数据。我们只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，我们再顺着链表往后遍历，直到链表中的结点数据值大于区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。 但是，我们要为几千万、上亿的数据构建索引，如果将索引存储在内存中，尽管内存访问的速度非常快，查询的效率非常高，但是，占用的内存会非常多。 比如，我们给一亿个数据构建二叉查找树索引，那索引中会包含大约 1 亿个节点，每个节点假设占用 16 个字节，那就需要大约 1GB 的内存空间。给一张表建立索引，我们需要 1GB 的内存空间。如果我们要给 10 张表建立索引，那对内存的需求是无法满足的。如何解决这个索引占用太多内存的问题呢？ 我们可以借助时间换空间的思路，把索引存储在硬盘中，而非内存中。我们都知道，硬盘是一个非常慢速的存储设备。通常内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。 这种将索引存储在硬盘中的方案，尽管减少了内存消耗，但是在数据查找的过程中，需要读取磁盘中的索引，因此数据查询效率就相应降低很多。 二叉查找树，经过改造之后，支持区间查找的功能就实现了。不过，为了节省内存，如果把树存储在硬盘中，那么每个节点的读取（或者访问），都对应一次磁盘 IO 操作。树的高度就等于每次查询数据时磁盘 IO 操作的次数。 我们前面讲到，比起内存读写操作，磁盘 IO 操作非常耗时，所以我们优化的重点就是尽量减少磁盘 IO 操作，也就是，尽量降低树的高度。那如何降低树的高度呢？ 我们来看下，如果我们把索引构建成 m 叉树，高度是不是比二叉树要小呢？如图所示，给 16 个数据构建二叉树索引，树的高度是 4，查找一个数据，就需要 4 个磁盘 IO 操作（如果根节点存储在内存中，其他节点存储在磁盘中），如果对 16 个数据构建五叉树索引，那高度只有 2，查找一个数据，对应只需要 2 次磁盘操作。如果 m 叉树中的 m 是 100，那对一亿个数据构建索引，树的高度也只是 3，最多只要 3 次磁盘 IO 就能获取到数据。磁盘 IO 变少了，查找数据的效率也就提高了。 如果我们将 m 叉树实现 B+ 树索引，用代码实现出来，就是下面这个样子（假设我们给 int 类型的数据库字段添加索引，所以代码中的 keywords 是 int 类型的）： /** * 这是B+树非叶子节点的定义。 * * 假设keywords=[3, 5, 8, 10] * 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF) * 5个区间分别对应：children[0]…children[4] * * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = (m-1)4[keywordss大小]+m8[children大小] */ public class BPlusTreeNode { public static int m = 5; // 5叉树 public int[] keywords = new int[m-1]; // 键值，用来划分数据区间 public BPlusTreeNode[] children = new BPlusTreeNode[m];//保存子节点指针 } /** * 这是B+树中叶子节点的定义。 * * B+树中的叶子节点跟内部节点是不一样的, * 叶子节点存储的是值，而非区间。 * 这个定义里，每个叶子节点存储3个数据行的键值及地址信息。 * * k值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = k4[keyw..大小]+k8[dataAd..大小]+8[prev大小]+8[next大小] */ public class BPlusTreeLeafNode { public static int k = 3; public int[] keywords = new int[k]; // 数据的键值 public long[] dataAddress = new long[k]; // 数据地址 public BPlusTreeLeafNode prev; // 这个结点在链表中的前驱结点 public BPlusTreeLeafNode next; // 这个结点在链表中的后继结点 } 我稍微解释一下这段代码。 对于相同个数的数据构建 m 叉树索引，m 叉树中的 m 越大，那树的高度就越小，那 m 叉树中的 m 是不是越大越好呢？到底多大才最合适呢？ 不管是内存中的数据，还是磁盘中的数据，操作系统都是按页（一页大小通常是 4KB，这个值可以通过 getconfig PAGE_SIZE 命令查看）来读取的，一次会读一页的数据。如果要读取的数据量超过一页的大小，就会触发多次 IO 操作。所以，我们在选择 m 大小的时候，要尽量让每个节点的大小等于一个页的大小。读取一个节点，只需要一次磁盘 IO 操作。 尽管索引可以提高数据库的查询效率，但是，作为一名开发工程师，你应该也知道，索引有利也有弊，它也会让写入数据的效率下降。这是为什么呢？ 数据的写入过程，会涉及索引的更新，这是索引导致写入变慢的主要原因。 对于一个 B+ 树来说，m 值是根据页的大小事先计算好的，也就是说，每个节点最多只能有 m 个子节点。在往数据库中写入数据的过程中，这样就有可能使索引中某些节点的子节点个数超过 m，这个节点的大小超过了一个页的大小，读取这样一个节点，就会导致多次磁盘 IO 操作。我们该如何解决这个问题呢？ 实际上，处理思路并不复杂。我们只需要将这个节点分裂成两个节点。但是，节点分裂之后，其上层父节点的子节点个数就有可能超过 m 个。不过这也没关系，我们可以用同样的方法，将父节点也分裂成两个节点。这种级联反应会从下往上，一直影响到根节点。这个分裂过程，你可以结合着下面这个图一块看，会更容易理解（图中的 B+ 树是一个三叉树。我们限定叶子节点中，数据的个数超过 2 个就分裂节点；非叶子节点中，子节点的个数超过 3 个就分裂节点）。 正是因为要时刻保证 B+ 树索引是一个 m 叉树，所以，索引的存在会导致数据库写入的速度降低。实际上，不光写入数据会变慢，删除数据也会变慢。这是为什么呢？ 我们在删除某个数据的时候，也要对应地更新索引节点。这个处理思路有点类似跳表中删除数据的处理思路。频繁的数据删除，就会导致某些节点中，子节点的个数变得非常少，长此以往，如果每个节点的子节点都比较少，势必会影响索引的效率。 我们可以设置一个阈值。在 B+ 树中，这个阈值等于 m/2。如果某个节点的子节点个数小于 m/2，我们就将它跟相邻的兄弟节点合并。不过，合并之后节点的子节点个数有可能会超过 m。针对这种情况，我们可以借助插入数据时候的处理方法，再分裂节点。 文字描述不是很直观，我举了一个删除操作的例子，你可以对比着看下（图中的 B+ 树是一个五叉树。我们限定叶子节点中，数据的个数少于 2 个就合并节点；非叶子节点中，子节点的个数少于 3 个就合并节点。）。 数据库索引以及 B+ 树的由来，到此就讲完了。你有没有发现，B+ 树的结构和操作，跟跳表非常类似。理论上讲，对跳表稍加改造，也可以替代 B+ 树，作为数据库的索引实现的。 B+ 树发明于 1972 年，跳表发明于 1989 年，我们可以大胆猜想下，跳表的作者有可能就是受了 B+ 树的启发，才发明出跳表来的。不过，这个也无从考证了。 总结引申今天，我们讲解了数据库索引实现，依赖的底层数据结构，B+ 树。它通过存储在磁盘的多叉树结构，做到了时间、空间的平衡，既保证了执行效率，又节省了内存。 前面的讲解中，为了一步一步详细地给你介绍 B+ 树的由来，内容看起来比较零散。为了方便你掌握和记忆，我这里再总结一下 B+ 树的特点： 每个节点中子节点的个数不能超过 m，也不能小于 m/2； 根节点的子节点个数可以不超过 m/2，这是一个例外； m 叉树只存储索引，并不真正存储数据，这个有点儿类似跳表； 通过链表将叶子节点串联在一起，这样可以方便按区间查找； 一般情况，根节点会被存储在内存中，其他节点存储在磁盘中。 除了 B+ 树，你可能还听说过 B 树、B- 树，我这里简单提一下。实际上，B- 树就是 B 树，英文翻译都是 B-Tree，这里的“-”并不是相对 B+ 树中的“+”，而只是一个连接符。这个很容易误解，所以我强调下。 而 B 树实际上是低级版的 B+ 树，或者说 B+ 树是 B 树的改进版。B 树跟 B+ 树的不同点主要集中在这几个地方： B+ 树中的节点不存储数据，只是索引，而 B 树中的节点存储数据； B 树中的叶子节点并不需要链表来串联。 也就是说，B 树只是一个每个节点的子节点个数不能小于 m/2 的 m 叉树。 课后思考B+ 树中，将叶子节点串起来的链表，是单链表还是双向链表？为什么？ 我们对平衡二叉查找树进行改造，将叶子节点串在链表中，就支持了按照区间来查找数据。我们在散列表（下）讲到，散列表也经常跟链表一块使用，如果我们把散列表中的结点，也用链表串起来，能否支持按照区间查找数据呢？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 49 | 搜索：如何用A*搜索算法实现游戏中的寻路功能？王争 2019-01-18  00:22 1.25x 讲述：冯永吉 大小：10.28M 时长：11:13 魔兽世界、仙剑奇侠传这类 MMRPG 游戏，不知道你有没有玩过？在这些游戏中，有一个非常重要的功能，那就是人物角色自动寻路。当人物处于游戏地图中的某个位置的时候，我们用鼠标点击另外一个相对较远的位置，人物就会自动地绕过障碍物走过去。玩过这么多游戏，不知你是否思考过，这个功能是怎么实现的呢？ 算法解析实际上，这是一个非常典型的搜索问题。人物的起点就是他当下所在的位置，终点就是鼠标点击的位置。我们需要在地图中，找一条从起点到终点的路径。这条路径要绕过地图中所有障碍物，并且看起来要是一种非常聪明的走法。所谓“聪明”，笼统地解释就是，走的路不能太绕。理论上讲，最短路径显然是最聪明的走法，是这个问题的最优解。 不过，在第 44 节最优出行路线规划问题中，我们也讲过，如果图非常大，那 Dijkstra 最短路径算法的执行耗时会很多。在真实的软件开发中，我们面对的是超级大的地图和海量的寻路请求，算法的执行效率太低，这显然是无法接受的。 实际上，像出行路线规划、游戏寻路，这些真实软件开发中的问题，一般情况下，我们都不需要非得求最优解（也就是最短路径）。在权衡路线规划质量和执行效率的情况下，我们只需要寻求一个次优解就足够了。那如何快速找出一条接近于最短路线的次优路线呢？ 这个快速的路径规划算法，就是我们今天要学习的 A* 算法。实际上，A* 算法是对 Dijkstra 算法的优化和改造。如何将 Dijkstra 算法改造成 A* 算法呢？为了更好地理解接下来要讲的内容，我建议你先温习下第 44 节中的 Dijkstra 算法的实现原理。 Dijkstra 算法有点儿类似 BFS 算法，它每次找到跟起点最近的顶点，往外扩展。这种往外扩展的思路，其实有些盲目。为什么这么说呢？我举一个例子来给你解释一下。下面这个图对应一个真实的地图，每个顶点在地图中的位置，我们用一个二维坐标（x，y）来表示，其中，x 表示横坐标，y 表示纵坐标。 在 Dijkstra 算法的实现思路中，我们用一个优先级队列，来记录已经遍历到的顶点以及这个顶点与起点的路径长度。顶点与起点路径长度越小，就越先被从优先级队列中取出来扩展，从图中举的例子可以看出，尽管我们找的是从 s 到 t 的路线，但是最先被搜索到的顶点依次是 1，2，3。通过肉眼来观察，这个搜索方向跟我们期望的路线方向（s 到 t 是从西向东）是反着的，路线搜索的方向明显“跑偏”了。 之所以会“跑偏”，那是因为我们是按照顶点与起点的路径长度的大小，来安排出队列顺序的。与起点越近的顶点，就会越早出队列。我们并没有考虑到这个顶点到终点的距离，所以，在地图中，尽管 1，2，3 三个顶点离起始顶点最近，但离终点却越来越远。 如果我们综合更多的因素，把这个顶点到终点可能还要走多远，也考虑进去，综合来判断哪个顶点该先出队列，那是不是就可以避免“跑偏”呢？ 当我们遍历到某个顶点的时候，从起点走到这个顶点的路径长度是确定的，我们记作 g(i)（i 表示顶点编号）。但是，从这个顶点到终点的路径长度，我们是未知的。虽然确切的值无法提前知道，但是我们可以用其他估计值来代替。 这里我们可以通过这个顶点跟终点之间的直线距离，也就是欧几里得距离，来近似地估计这个顶点跟终点的路径长度（注意：路径长度跟直线距离是两个概念）。我们把这个距离记作 h(i)（i 表示这个顶点的编号），专业的叫法是启发函数（heuristic function）。因为欧几里得距离的计算公式，会涉及比较耗时的开根号计算，所以，我们一般通过另外一个更加简单的距离计算公式，那就是曼哈顿距离（Manhattan distance）。曼哈顿距离是两点之间横纵坐标的距离之和。计算的过程只涉及加减法、符号位反转，所以比欧几里得距离更加高效。 int hManhattan(Vertex v1, Vertex v2) { // Vertex表示顶点，后面有定义 return Math.abs(v1.x - v2.x) + Math.abs(v1.y - v2.y); } 原来只是单纯地通过顶点与起点之间的路径长度 g(i)，来判断谁先出队列，现在有了顶点到终点的路径长度估计值，我们通过两者之和 f(i)=g(i)+h(i)，来判断哪个顶点该最先出队列。综合两部分，我们就能有效避免刚刚讲的“跑偏”。这里 f(i) 的专业叫法是估价函数（evaluation function）。 从刚刚的描述，我们可以发现，A* 算法就是对 Dijkstra 算法的简单改造。实际上，代码实现方面，我们也只需要稍微改动几行代码，就能把 Dijkstra 算法的代码实现，改成 A* 算法的代码实现。 在 A* 算法的代码实现中，顶点 Vertex 类的定义，跟 Dijkstra 算法中的定义，稍微有点儿区别，多了 x，y 坐标，以及刚刚提到的 f(i) 值。图 Graph 类的定义跟 Dijkstra 算法中的定义一样。为了避免重复，我这里就没有再贴出来了。 private class Vertex { public int id; // 顶点编号ID public int dist; // 从起始顶点，到这个顶点的距离，也就是g(i) public int f; // 新增：f(i)=g(i)+h(i) public int x, y; // 新增：顶点在地图中的坐标（x, y） public Vertex(int id, int x, int y) { ​ this.id = id; ​ this.x = x; ​ this.y = y; ​ this.f = Integer.MAX_VALUE; ​ this.dist = Integer.MAX_VALUE; } } // Graph类的成员变量，在构造函数中初始化 Vertex[] vertexes = new Vertex[this.v]; // 新增一个方法，添加顶点的坐标 public void addVetex(int id, int x, int y) { vertexes[id] = new Vertex(id, x, y) } A* 算法的代码实现的主要逻辑是下面这段代码。它跟 Dijkstra 算法的代码实现，主要有 3 点区别： 优先级队列构建的方式不同。A* 算法是根据 f 值（也就是刚刚讲到的 f(i)=g(i)+h(i)）来构建优先级队列，而 Dijkstra 算法是根据 dist 值（也就是刚刚讲到的 g(i)）来构建优先级队列； A* 算法在更新顶点 dist 值的时候，会同步更新 f 值； 循环结束的条件也不一样。Dijkstra 算法是在终点出队列的时候才结束，A* 算法是一旦遍历到终点就结束。 public void astar(int s, int t) { // 从顶点s到顶点t的路径 int[] predecessor = new int[this.v]; // 用来还原路径 // 按照vertex的f值构建的小顶堆，而不是按照dist PriorityQueue queue = new PriorityQueue(this.v); boolean[] inqueue = new boolean[this.v]; // 标记是否进入过队列 vertexes[s].dist = 0; vertexes[s].f = 0; queue.add(vertexes[s]); inqueue[s] = true; while (!queue.isEmpty()) { ​ Vertex minVertex = queue.poll(); // 取堆顶元素并删除 ​ for (int i = 0; i &lt; adj[minVertex.id].size(); ++i) { ​ Edge e = adj[minVertex.id].get(i); // 取出一条minVetex相连的边 ​ Vertex nextVertex = vertexes[e.tid]; // minVertex–&gt;nextVertex ​ if (minVertex.dist + e.w &lt; nextVertex.dist) { // 更新next的dist,f ​ nextVertex.dist = minVertex.dist + e.w; ​ nextVertex.f ​ = nextVertex.dist+hManhattan(nextVertex, vertexes[t]); ​ predecessor[nextVertex.id] = minVertex.id; ​ if (inqueue[nextVertex.id] == true) { ​ queue.update(nextVertex); ​ } else { ​ queue.add(nextVertex); ​ inqueue[nextVertex.id] = true; ​ } ​ } ​ if (nextVertex.id == t) { // 只要到达t就可以结束while了 ​ queue.clear(); // 清空queue，才能推出while循环 ​ break; ​ } ​ } } // 输出路径 System.out.print(s); print(s, t, predecessor); // print函数请参看Dijkstra算法的实现 } 尽管 A* 算法可以更加快速地找到从起点到终点的路线，但是它并不能像 Dijkstra 算法那样，找到最短路线。这是为什么呢？ 要找出起点 s 到终点 t 的最短路径，最简单的方法是，通过回溯穷举所有从 s 到达 t 的不同路径，然后对比找出最短的那个。不过很显然，回溯算法的执行效率非常低，是指数级的。 Dijkstra 算法在此基础之上，利用动态规划的思想，对回溯搜索进行了剪枝，只保留起点到某个顶点的最短路径，继续往外扩展搜索。动态规划相较于回溯搜索，只是换了一个实现思路，但它实际上也考察到了所有从起点到终点的路线，所以才能得到最优解。 A* 算法之所以不能像 Dijkstra 算法那样，找到最短路径，主要原因是两者的 while 循环结束条件不一样。刚刚我们讲过，Dijkstra 算法是在终点出队列的时候才结束，A* 算法是一旦遍历到终点就结束。对于 Dijkstra 算法来说，当终点出队列的时候，终点的 dist 值是优先级队列中所有顶点的最小值，即便再运行下去，终点的 dist 值也不会再被更新了。对于 A* 算法来说，一旦遍历到终点，我们就结束 while 循环，这个时候，终点的 dist 值未必是最小值。 A* 算法利用贪心算法的思路，每次都找 f 值最小的顶点出队列，一旦搜索到终点就不在继续考察其他顶点和路线了。所以，它并没有考察所有的路线，也就不可能找出最短路径了。 搞懂了 A* 算法，我们再来看下，如何借助 A* 算法解决今天的游戏寻路问题？ 要利用 A* 算法解决这个问题，我们只需要把地图，抽象成图就可以了。不过，游戏中的地图跟第 44 节中讲的我们平常用的地图是不一样的。因为游戏中的地图并不像我们现实生活中那样，存在规划非常清晰的道路，更多的是宽阔的荒野、草坪等。所以，我们没法利用 44 节中讲到的抽象方法，把岔路口抽象成顶点，把道路抽象成边。 实际上，我们可以换一种抽象的思路，把整个地图分割成一个一个的小方块。在某一个方块上的人物，只能往上下左右四个方向的方块上移动。我们可以把每个方块看作一个顶点。两个方块相邻，我们就在它们之间，连两条有向边，并且边的权值都是 1。所以，这个问题就转化成了，在一个有向有权图中，找某个顶点到另一个顶点的路径问题。将地图抽象成边权值为 1 的有向图之后，我们就可以套用 A* 算法，来实现游戏中人物的自动寻路功能了。 总结引申我们今天讲的 A* 算法属于一种启发式搜索算法（Heuristically Search Algorithm）。实际上，启发式搜索算法并不仅仅只有 A* 算法，还有很多其他算法，比如 IDA* 算法、蚁群算法、遗传算法、模拟退火算法等。如果感兴趣，你可以自行研究下。 启发式搜索算法利用估价函数，避免“跑偏”，贪心地朝着最有可能到达终点的方向前进。这种算法找出的路线，并不是最短路线。但是，实际的软件开发中的路线规划问题，我们往往并不需要非得找最短路线。所以，鉴于启发式搜索算法能很好地平衡路线质量和执行效率，它在实际的软件开发中的应用更加广泛。实际上，在第 44 节中，我们讲到的地图 App 中的出行路线规划问题，也可以利用启发式搜索算法来实现。 课后思考我们之前讲的“迷宫问题”是否可以借助 A* 算法来更快速地找到一个走出去的路线呢？如果可以，请具体讲讲该怎么来做；如果不可以，请说说原因。 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 50 | 索引：如何在海量数据中快速查找某个数据？王争 2019-01-21  00:10 1.25x 讲述：冯永吉 大小：9.82M 时长：10:42 在第 48 节中，我们讲了 MySQL 数据库索引的实现原理。MySQL 底层依赖的是 B+ 树这种数据结构。留言里有同学问我，那类似 Redis 这样的 Key-Value 数据库中的索引，又是怎么实现的呢？底层依赖的又是什么数据结构呢？ 今天，我就来讲一下索引这种常用的技术解决思路，底层往往会依赖哪些数据结构。同时，通过索引这个应用场景，我也带你回顾一下，之前我们学过的几种支持动态集合的数据结构。 为什么需要索引？在实际的软件开发中，业务纷繁复杂，功能千变万化，但是，万变不离其宗。如果抛开这些业务和功能的外壳，其实它们的本质都可以抽象为“对数据的存储和计算”。对应到数据结构和算法中，那“存储”需要的就是数据结构，“计算”需要的就是算法。 对于存储的需求，功能上无外乎增删改查。这其实并不复杂。但是，一旦存储的数据很多，那性能就成了这些系统要关注的重点，特别是在一些跟存储相关的基础系统（比如 MySQL 数据库、分布式文件系统等）、中间件（比如消息中间件 RocketMQ 等）中。 “如何节省存储空间、如何提高数据增删改查的执行效率”，这样的问题就成了设计的重点。而这些系统的实现，都离不开一个东西，那就是索引。不夸张地说，索引设计得好坏，直接决定了这些系统是否优秀。 索引这个概念，非常好理解。你可以类比书籍的目录来理解。如果没有目录，我们想要查找某个知识点的时候，就要一页一页翻。通过目录，我们就可以快速定位相关知识点的页数，查找的速度也会有质的提高。 索引的需求定义索引的概念不难理解，我想你应该已经搞明白。接下来，我们就分析一下，在设计索引的过程中，需要考虑到的一些因素，换句话说就是，我们该如何定义清楚需求呢？ 对于系统设计需求，我们一般可以从功能性需求和非功能性需求两方面来分析，这个我们之前也说过。因此，这个问题也不例外。 1. 功能性需求对于功能性需求需要考虑的点，我把它们大致概括成下面这几点。 数据是格式化数据还是非格式化数据？要构建索引的原始数据，类型有很多。我把它分为两类，一类是结构化数据，比如，MySQL 中的数据；另一类是非结构化数据，比如搜索引擎中网页。对于非结构化数据，我们一般需要做预处理，提取出查询关键词，对关键词构建索引。 数据是静态数据还是动态数据？如果原始数据是一组静态数据，也就是说，不会有数据的增加、删除、更新操作，所以，我们在构建索引的时候，只需要考虑查询效率就可以了。这样，索引的构建就相对简单些。不过，大部分情况下，我们都是对动态数据构建索引，也就是说，我们不仅要考虑到索引的查询效率，在原始数据更新的同时，我们还需要动态地更新索引。支持动态数据集合的索引，设计起来相对也要更加复杂些。 索引存储在内存还是硬盘？如果索引存储在内存中，那查询的速度肯定要比存储在磁盘中的高。但是，如果原始数据量很大的情况下，对应的索引可能也会很大。这个时候，因为内存有限，我们可能就不得不将索引存储在磁盘中了。实际上，还有第三种情况，那就是一部分存储在内存，一部分存储在磁盘，这样就可以兼顾内存消耗和查询效率。 单值查找还是区间查找？所谓单值查找，也就是根据查询关键词等于某个值的数据。这种查询需求最常见。所谓区间查找，就是查找关键词处于某个区间值的所有数据。你可以类比 MySQL 数据库的查询需求，自己想象一下。实际上，不同的应用场景，查询的需求会多种多样。 单关键词查找还是多关键词组合查找？比如，搜索引擎中构建的索引，既要支持一个关键词的查找，比如“数据结构”，也要支持组合关键词查找，比如“数据结构 AND 算法”。对于单关键词的查找，索引构建起来相对简单些。对于多关键词查询来说，要分多种情况。像 MySQL 这种结构化数据的查询需求，我们可以实现针对多个关键词的组合，建立索引；对于像搜索引擎这样的非结构数据的查询需求，我们可以针对单个关键词构建索引，然后通过集合操作，比如求并集、求交集等，计算出多个关键词组合的查询结果。 实际上，不同的场景，不同的原始数据，对于索引的需求也会千差万别。我这里只列举了一些比较有共性的需求。 2. 非功能性需求讲完了功能性需求，我们再来看，索引设计的非功能性需求。 不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大。如果存储在内存中，索引对占用存储空间的限制就会非常苛刻。毕竟内存空间非常有限，一个中间件启动后就占用几个 GB 的内存，开发者显然是无法接受的。如果存储在硬盘中，那索引对占用存储空间的限制，稍微会放宽一些。但是，我们也不能掉以轻心。因为，有时候，索引对存储空间的消耗会超过原始数据。 在考虑索引查询效率的同时，我们还要考虑索引的维护成本。索引的目的是提高查询效率，但是，基于动态数据集合构建的索引，我们还要考虑到，索引的维护成本。因为在原始数据动态增删改的同时，我们也需要动态地更新索引。而索引的更新势必会影响到增删改操作的性能。 构建索引常用的数据结构有哪些？我刚刚从很宏观的角度，总结了在索引设计的过程中，需要考虑的一些共性因素。现在，我们就来看，对于不同需求的索引结构，底层一般使用哪种数据结构。 实际上，常用来构建索引的数据结构，就是我们之前讲过的几种支持动态数据集合的数据结构。比如，散列表、红黑树、跳表、B+ 树。除此之外，位图、布隆过滤器可以作为辅助索引，有序数组可以用来对静态数据构建索引。 我们知道，散列表增删改查操作的性能非常好，时间复杂度是 O(1)。一些键值数据库，比如 Redis、Memcache，就是使用散列表来构建索引的。这类索引，一般都构建在内存中。 红黑树作为一种常用的平衡二叉查找树，数据插入、删除、查找的时间复杂度是 O(logn)，也非常适合用来构建内存索引。Ext 文件系统中，对磁盘块的索引，用的就是红黑树。 B+ 树比起红黑树来说，更加适合构建存储在磁盘中的索引。B+ 树是一个多叉树，所以，对相同个数的数据构建索引，B+ 树的高度要低于红黑树。当借助索引查询数据的时候，读取 B+ 树索引，需要的磁盘 IO 次数会更少。所以，大部分关系型数据库的索引，比如 MySQL、Oracle，都是用 B+ 树来实现的。 跳表也支持快速添加、删除、查找数据。而且，我们通过灵活调整索引结点个数和数据个数之间的比例，可以很好地平衡索引对内存的消耗及其查询效率。Redis 中的有序集合，就是用跳表来构建的。 除了散列表、红黑树、B+ 树、跳表之外，位图和布隆过滤器这两个数据结构，也可以用于索引中，辅助存储在磁盘中的索引，加速数据查找的效率。我们来看下，具体是怎么做的？ 我们知道，布隆过滤器有一定的判错率。但是，我们可以规避它的短处，发挥它的长处。尽管对于判定存在的数据，有可能并不存在，但是对于判定不存在的数据，那肯定就不存在。而且，布隆过滤器还有一个更大的特点，那就是内存占用非常少。我们可以针对数据，构建一个布隆过滤器，并且存储在内存中。当要查询数据的时候，我们可以先通过布隆过滤器，判定是否存在。如果通过布隆过滤器判定数据不存在，那我们就没有必要读取磁盘中的索引了。对于数据不存在的情况，数据查询就更加快速了。 实际上，有序数组也可以被作为索引。如果数据是静态的，也就是不会有插入、删除、更新操作，那我们可以把数据的关键词（查询用的）抽取出来，组织成有序数组，然后利用二分查找算法来快速查找数据。 总结引申今天这节算是一节总结课。我从索引这个非常常用的技术方案，给你展示了散列表、红黑树、跳表、位图、布隆过滤器、有序数组这些数据结构的应用场景。学习完这节课之后，不知道你对这些数据结构以及索引，有没有更加清晰的认识呢？ 从这一节内容中，你应该可以看出，架构设计离不开数据结构和算法。要想成长为一个优秀的业务架构师、基础架构师，数据结构和算法的根基一定要打稳。因为，那些看似很惊艳的架构设计思路，实际上，都是来自最常用的数据结构和算法。 课后思考你知道基础系统、中间件、开源软件等系统中，有哪些用到了索引吗？这些系统的索引是如何实现的呢？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 51 | 并行算法：如何利用并行处理提高算法的执行效率？王争 2019-01-23  00:09 1.25x 讲述：冯永吉 大小：9.22M 时长：10:03 时间复杂度是衡量算法执行效率的一种标准。但是，时间复杂度并不能跟性能划等号。在真实的软件开发中，即便在不降低时间复杂度的情况下，也可以通过一些优化手段，提升代码的执行效率。毕竟，对于实际的软件开发来说，即便是像 10%、20% 这样微小的性能提升，也是非常可观的。 算法的目的就是为了提高代码执行的效率。那当算法无法再继续优化的情况下，我们该如何来进一步提高执行效率呢？我们今天就讲一种非常简单但又非常好用的优化方法，那就是并行计算。今天，我就通过几个例子，给你展示一下，如何借助并行计算的处理思想对算法进行改造？ 并行排序假设我们要给大小为 8GB 的数据进行排序，并且，我们机器的内存可以一次性容纳这么多数据。对于排序来说，最常用的就是时间复杂度为 O(nlogn) 的三种排序算法，归并排序、快速排序、堆排序。从理论上讲，这个排序问题，已经很难再从算法层面优化了。而利用并行的处理思想，我们可以很轻松地将这个给 8GB 数据排序问题的执行效率提高很多倍。具体的实现思路有下面两种。 第一种是对归并排序并行化处理。我们可以将这 8GB 的数据划分成 16 个小的数据集合，每个集合包含 500MB 的数据。我们用 16 个线程，并行地对这 16 个 500MB 的数据集合进行排序。这 16 个小集合分别排序完成之后，我们再将这 16 个有序集合合并。 第二种是对快速排序并行化处理。我们通过扫描一遍数据，找到数据所处的范围区间。我们把这个区间从小到大划分成 16 个小区间。我们将 8GB 的数据划分到对应的区间中。针对这 16 个小区间的数据，我们启动 16 个线程，并行地进行排序。等到 16 个线程都执行结束之后，得到的数据就是有序数据了。 对比这两种处理思路，它们利用的都是分治的思想，对数据进行分片，然后并行处理。它们的区别在于，第一种处理思路是，先随意地对数据分片，排序之后再合并。第二种处理思路是，先对数据按照大小划分区间，然后再排序，排完序就不需要再处理了。这个跟归并和快排的区别如出一辙。 这里我还要多说几句，如果要排序的数据规模不是 8GB，而是 1TB，那问题的重点就不是算法的执行效率了，而是数据的读取效率。因为 1TB 的数据肯定是存在硬盘中，无法一次性读取到内存中，这样在排序的过程中，就会有频繁地磁盘数据的读取和写入。如何减少磁盘的 IO 操作，减少磁盘数据读取和写入的总量，就变成了优化的重点。不过这个不是我们这节要讨论的重点，你可以自己思考下。 并行查找我们知道，散列表是一种非常适合快速查找的数据结构。 如果我们是给动态数据构建索引，在数据不断加入的时候，散列表的装载因子就会越来越大。为了保证散列表性能不下降，我们就需要对散列表进行动态扩容。对如此大的散列表进行动态扩容，一方面比较耗时，另一方面比较消耗内存。比如，我们给一个 2GB 大小的散列表进行扩容，扩展到原来的 1.5 倍，也就是 3GB 大小。这个时候，实际存储在散列表中的数据只有不到 2GB，所以内存的利用率只有 60%，有 1GB 的内存是空闲的。 实际上，我们可以将数据随机分割成 k 份（比如 16 份），每份中的数据只有原来的 1/k，然后我们针对这 k 个小数据集合分别构建散列表。这样，散列表的维护成本就变低了。当某个小散列表的装载因子过大的时候，我们可以单独对这个散列表进行扩容，而其他散列表不需要进行扩容。 还是刚才那个例子，假设现在有 2GB 的数据，我们放到 16 个散列表中，每个散列表中的数据大约是 150MB。当某个散列表需要扩容的时候，我们只需要额外增加 150*0.5=75MB 的内存（假设还是扩容到原来的 1.5 倍）。无论从扩容的执行效率还是内存的利用率上，这种多个小散列表的处理方法，都要比大散列表高效。 当我们要查找某个数据的时候，我们只需要通过 16 个线程，并行地在这 16 个散列表中查找数据。这样的查找性能，比起一个大散列表的做法，也并不会下降，反倒有可能提高。 当往散列表中添加数据的时候，我们可以选择将这个新数据放入装载因子最小的那个散列表中，这样也有助于减少散列冲突。 并行字符串匹配我们前面学过，在文本中查找某个关键词这样一个功能，可以通过字符串匹配算法来实现。我们之前学过的字符串匹配算法有 KMP、BM、RK、BF 等。当在一个不是很长的文本中查找关键词的时候，这些字符串匹配算法中的任何一个，都可以表现得非常高效。但是，如果我们处理的是超级大的文本，那处理的时间可能就会变得很长，那有没有办法加快匹配速度呢？ 我们可以把大的文本，分割成 k 个小文本。假设 k 是 16，我们就启动 16 个线程，并行地在这 16 个小文本中查找关键词，这样整个查找的性能就提高了 16 倍。16 倍效率的提升，从理论的角度来说并不多。但是，对于真实的软件开发来说，这显然是一个非常可观的优化。 不过，这里还有一个细节要处理，那就是原本包含在大文本中的关键词，被一分为二，分割到两个小文本中，这就会导致尽管大文本中包含这个关键词，但在这 16 个小文本中查找不到它。实际上，这个问题也不难解决，我们只需要针对这种特殊情况，做一些特殊处理就可以了。 我们假设关键词的长度是 m。我们在每个小文本的结尾和开始各取 m 个字符串。前一个小文本的末尾 m 个字符和后一个小文本的开头 m 个字符，组成一个长度是 2m 的字符串。我们再拿关键词，在这个长度为 2m 的字符串中再重新查找一遍，就可以补上刚才的漏洞了。 并行搜索前面我们学习过好几种搜索算法，它们分别是广度优先搜索、深度优先搜索、Dijkstra 最短路径算法、A* 启发式搜索算法。对于广度优先搜索算法，我们也可以将其改造成并行算法。 广度优先搜索是一种逐层搜索的搜索策略。基于当前这一层顶点，我们可以启动多个线程，并行地搜索下一层的顶点。在代码实现方面，原来广度优先搜索的代码实现，是通过一个队列来记录已经遍历到但还没有扩展的顶点。现在，经过改造之后的并行广度优先搜索算法，我们需要利用两个队列来完成扩展顶点的工作。 假设这两个队列分别是队列 A 和队列 B。多线程并行处理队列 A 中的顶点，并将扩展得到的顶点存储在队列 B 中。等队列 A 中的顶点都扩展完成之后，队列 A 被清空，我们再并行地扩展队列 B 中的顶点，并将扩展出来的顶点存储在队列 A。这样两个队列循环使用，就可以实现并行广度优先搜索算法。 总结引申上一节，我们通过实际软件开发中的“索引”这一技术点，回顾了之前学过的一些支持动态数据集合的数据结构。今天，我们又通过“并行算法”这个话题，回顾了之前学过的一些算法。 今天的内容比较简单，没有太复杂的知识点。我通过一些例子，比如并行排序、查找、搜索、字符串匹配，给你展示了并行处理的实现思路，也就是对数据进行分片，对没有依赖关系的任务，并行地执行。 并行计算是一个工程上的实现思路，尽管跟算法关系不大，但是，在实际的软件开发中，它确实可以非常巧妙地提高程序的运行效率，是一种非常好用的性能优化手段。 特别是，当要处理的数据规模达到一定程度之后，我们无法通过继续优化算法，来提高执行效率 的时候，我们就需要在实现的思路上做文章，利用更多的硬件资源，来加快执行的效率。所以，在很多超大规模数据处理中，并行处理的思想，应用非常广泛，比如 MapReduce 实际上就是一种并行计算框架。 课后思考假设我们有 n 个任务，为了提高执行的效率，我们希望能并行执行任务，但是各个任务之间又有一定的依赖关系，如何根据依赖关系找出可以并行执行的任务？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 =======================================实战篇 52-56（5讲）52 | 算法实战（一）：剖析Redis常用数据类型对应的数据结构王争 2019-01-25  00:19 1.25x 讲述：冯永吉 大小：11.29M 时长：12:19 到此为止，专栏前三部分我们全部讲完了。从今天开始，我们就正式进入实战篇的部分。这部分我主要通过一些开源项目、经典系统，真枪实弹地教你，如何将数据结构和算法应用到项目中。所以这部分的内容，更多的是知识点的回顾，相对于基础篇、高级篇的内容，其实这部分会更加容易看懂。 不过，我希望你不要只是看懂就完了。你要多举一反三地思考，自己接触过的开源项目、基础框架、中间件中，都用过哪些数据结构和算法。你也可以想一想，在自己做的项目中，有哪些可以用学过的数据结构和算法进一步优化。这样的学习效果才会更好。 好了，今天我就带你一块儿看下，经典数据库 Redis 中的常用数据类型，底层都是用哪种数据结构实现的？ Redis 数据库介绍Redis 是一种键值（Key-Value）数据库。相对于关系型数据库（比如 MySQL），Redis 也被叫作非关系型数据库。 像 MySQL 这样的关系型数据库，表的结构比较复杂，会包含很多字段，可以通过 SQL 语句，来实现非常复杂的查询需求。而 Redis 中只包含“键”和“值”两部分，只能通过“键”来查询“值”。正是因为这样简单的存储结构，也让 Redis 的读写效率非常高。 除此之外，Redis 主要是作为内存数据库来使用，也就是说，数据是存储在内存中的。尽管它经常被用作内存数据库，但是，它也支持将数据存储在硬盘中。这一点，我们后面会介绍。 Redis 中，键的数据类型是字符串，但是为了丰富数据存储的方式，方便开发者使用，值的数据类型有很多，常用的数据类型有这样几种，它们分别是字符串、列表、字典、集合、有序集合。 “字符串（string）”这种数据类型非常简单，对应到数据结构里，就是字符串。你应该非常熟悉，这里我就不多介绍了。我们着重看下，其他四种比较复杂点的数据类型，看看它们底层都依赖了哪些数据结构。 列表（list）我们先来看列表。列表这种数据类型支持存储一组数据。这种数据类型对应两种实现方法，一种是压缩列表（ziplist），另一种是双向循环链表。 当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面两个条件： 列表中保存的单个数据（有可能是字符串类型的）小于 64 字节； 列表中数据个数少于 512 个。 关于压缩列表，我这里稍微解释一下。它并不是基础数据结构，而是 Redis 自己设计的一种数据存储结构。它有点儿类似数组，通过一片连续的内存空间，来存储数据。不过，它跟数组不同的一点是，它允许存储的数据大小不同。具体的存储结构也非常简单，你可以看我下面画的这幅图。 现在，我们来看看，压缩列表中的“压缩”两个字该如何理解？ 听到“压缩”两个字，直观的反应就是节省内存。之所以说这种存储结构节省内存，是相较于数组的存储思路而言的。我们知道，数组要求每个元素的大小相同，如果我们要存储不同长度的字符串，那我们就需要用最大长度的字符串大小作为元素的大小（假设是 20 个字节）。那当我们存储小于 20 个字节长度的字符串的时候，便会浪费部分存储空间。听起来有点儿拗口，我画个图解释一下。 压缩列表这种存储结构，一方面比较节省内存，另一方面可以支持不同类型数据的存储。而且，因为数据存储在一片连续的内存空间，通过键来获取值为列表类型的数据，读取的效率也非常高。 当列表中存储的数据量比较大的时候，也就是不能同时满足刚刚讲的两个条件的时候，列表就要通过双向循环链表来实现了。 在链表里，我们已经讲过双向循环链表这种数据结构了，如果不记得了，你可以先回去复习一下。这里我们着重看一下 Redis 中双向链表的编码实现方式。 Redis 的这种双向链表的实现方式，非常值得借鉴。它额外定义一个 list 结构体，来组织链表的首、尾指针，还有长度等信息。这样，在使用的时候就会非常方便。 // 以下是C语言代码，因为Redis是用C语言实现的。 typedef struct listnode { struct listNode *prev; struct listNode *next; void *value; } listNode; typedef struct list { listNode *head; listNode *tail; unsigned long len; // ….省略其他定义 } list; 字典（hash）字典类型用来存储一组数据对。每个数据对又包含键值两部分。字典类型也有两种实现方式。一种是我们刚刚讲到的压缩列表，另一种是散列表。 同样，只有当存储的数据量比较小的情况下，Redis 才使用压缩列表来实现字典类型。具体需要满足两个条件： 字典中保存的键和值的大小都要小于 64 字节； 字典中键值对的个数要小于 512 个。 当不能同时满足上面两个条件的时候，Redis 就使用散列表来实现字典类型。Redis 使用MurmurHash2这种运行速度快、随机性好的哈希算法作为哈希函数。对于哈希冲突问题，Redis 使用链表法来解决。除此之外，Redis 还支持散列表的动态扩容、缩容。 当数据动态增加之后，散列表的装载因子会不停地变大。为了避免散列表性能的下降，当装载因子大于 1 的时候，Redis 会触发扩容，将散列表扩大为原来大小的 2 倍左右（具体值需要计算才能得到，如果感兴趣，你可以去阅读源码）。 当数据动态减少之后，为了节省内存，当装载因子小于 0.1 的时候，Redis 就会触发缩容，缩小为字典中数据个数的大约 2 倍大小（这个值也是计算得到的，如果感兴趣，你也可以去阅读源码）。 我们前面讲过，扩容缩容要做大量的数据搬移和哈希值的重新计算，所以比较耗时。针对这个问题，Redis 使用我们在散列表（中）讲的渐进式扩容缩容策略，将数据的搬移分批进行，避免了大量数据一次性搬移导致的服务停顿。 集合（set）集合这种数据类型用来存储一组不重复的数据。这种数据类型也有两种实现方法，一种是基于有序数组，另一种是基于散列表。 当要存储的数据，同时满足下面这样两个条件的时候，Redis 就采用有序数组，来实现集合这种数据类型。 存储的数据都是整数； 存储的数据元素个数不超过 512 个。 当不能同时满足这两个条件的时候，Redis 就使用散列表来存储集合中的数据。 有序集合（sortedset）有序集合这种数据类型，我们在跳表里已经详细讲过了。它用来存储一组数据，并且每个数据会附带一个得分。通过得分的大小，我们将数据组织成跳表这样的数据结构，以支持快速地按照得分值、得分区间获取数据。 实际上，跟 Redis 的其他数据类型一样，有序集合也并不仅仅只有跳表这一种实现方式。当数据量比较小的时候，Redis 会用压缩列表来实现有序集合。具体点说就是，使用压缩列表来实现有序集合的前提，有这样两个： 所有数据的大小都要小于 64 字节； 元素个数要小于 128 个。 数据结构持久化尽管 Redis 经常会被用作内存数据库，但是，它也支持数据落盘，也就是将内存中的数据存储到硬盘中。这样，当机器断电的时候，存储在 Redis 中的数据也不会丢失。在机器重新启动之后，Redis 只需要再将存储在硬盘中的数据，重新读取到内存，就可以继续工作了。 刚刚我们讲到，Redis 的数据格式由“键”和“值”两部分组成。而“值”又支持很多数据类型，比如字符串、列表、字典、集合、有序集合。像字典、集合等类型，底层用到了散列表，散列表中有指针的概念，而指针指向的是内存中的存储地址。 那 Redis 是如何将这样一个跟具体内存地址有关的数据结构存储到磁盘中的呢？ 实际上，Redis 遇到的这个问题并不特殊，很多场景中都会遇到。我们把它叫作数据结构的持久化问题，或者对象的持久化问题。这里的“持久化”，你可以笼统地理解为“存储到磁盘”。 如何将数据结构持久化到硬盘？我们主要有两种解决思路。 第一种是清除原有的存储结构，只将数据存储到磁盘中。当我们需要从磁盘还原数据到内存的时候，再重新将数据组织成原来的数据结构。实际上，Redis 采用的就是这种持久化思路。 不过，这种方式也有一定的弊端。那就是数据从硬盘还原到内存的过程，会耗用比较多的时间。比如，我们现在要将散列表中的数据存储到磁盘。当我们从磁盘中，取出数据重新构建散列表的时候，需要重新计算每个数据的哈希值。如果磁盘中存储的是几 GB 的数据，那重构数据结构的耗时就不可忽视了。 第二种方式是保留原来的存储格式，将数据按照原有的格式存储在磁盘中。我们拿散列表这样的数据结构来举例。我们可以将散列表的大小、每个数据被散列到的槽的编号等信息，都保存在磁盘中。有了这些信息，我们从磁盘中将数据还原到内存中的时候，就可以避免重新计算哈希值。 总结引申今天，我们学习了 Redis 中常用数据类型底层依赖的数据结构，总结一下大概有这五种：压缩列表（可以看作一种特殊的数组）、有序数组、链表、散列表、跳表。实际上，Redis 就是这些常用数据结构的封装。 你有没有发现，有了数据结构和算法的基础之后，再去阅读 Redis 的源码，理解起来就容易多了？很多原来觉得很深奥的设计思想，是不是就都会觉得顺理成章了呢？ 还是那句话，夯实基础很重要。同样是看源码，有些人只能看个热闹，了解一些皮毛，无法形成自己的知识结构，不能化为己用，过不几天就忘了。而有些人基础很好，不但能知其然，还能知其所以然，从而真正理解作者设计的动机。这样不但能有助于我们理解所用的开源软件，还能为我们自己创新添砖加瓦。 课后思考你有没有发现，在数据量比较小的情况下，Redis 中的很多数据类型，比如字典、有序集合等，都是通过多种数据结构来实现的，为什么会这样设计呢？用一种固定的数据结构来实现，不是更加简单吗？ 我们讲到数据结构持久化有两种方法。对于二叉查找树这种数据结构，我们如何将它持久化到磁盘中呢？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 53 | 算法实战（二）：剖析搜索引擎背后的经典数据结构和算法王争 2019-01-28  00:00 1.25x 讲述：冯永吉 大小：19.22M 时长：20:59 像百度、Google 这样的搜索引擎，在我们平时的工作、生活中，几乎天天都会用到。如果我们把搜索引擎也当作一个互联网产品的话，那它跟社交、电商这些类型的产品相比，有一个非常大的区别，那就是，它是一个技术驱动的产品。所谓技术驱动是指，搜索引擎实现起来，技术难度非常大，技术的好坏直接决定了这个产品的核心竞争力。 在搜索引擎的设计与实现中，会用到大量的算法。有很多针对特定问题的算法，也有很多我们专栏中讲到的基础算法。所以，百度、Google 这样的搜索引擎公司，在面试的时候，会格外重视考察候选人的算法能力。 今天我就借助搜索引擎，这样一个非常有技术含量的产品，来给你展示一下，数据结构和算法是如何应用在其中的。 整体系统介绍像 Google 这样的大型商用搜索引擎，有成千上万的工程师，十年如一日地对它进行优化改进，所以，它所包含的技术细节非常多。我很难、也没有这个能力，通过一篇文章把所有细节都讲清楚，当然这也不是我们专栏所专注的内容。 所以，接下来的讲解，我主要给你展示，如何在一台机器上（假设这台机器的内存是 8GB， 硬盘是 100 多 GB），通过少量的代码，实现一个小型搜索引擎。不过，麻雀虽小，五脏俱全。跟大型搜索引擎相比，实现这样一个小型搜索引擎所用到的理论基础是相通的。 搜索引擎大致可以分为四个部分：搜集、分析、索引、查询。其中，搜集，就是我们常说的利用爬虫爬取网页。分析，主要负责网页内容抽取、分词，构建临时索引，计算 PageRank 值这几部分工作。索引，主要负责通过分析阶段得到的临时索引，构建倒排索引。查询，主要负责响应用户的请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果给用户。 接下来，我就按照网页处理的生命周期，从这四个阶段，依次来给你讲解，一个网页从被爬取到最终展示给用户，这样一个完整的过程。与此同时，我会穿插讲解，这个过程中需要用到哪些数据结构和算法。 搜集现在，互联网越来越发达，网站越来越多，对应的网页也就越来越多。对于搜索引擎来说，它事先并不知道网页都在哪里。打个比方来说就是，我们只知道海里面有很多鱼，但却并不知道鱼在哪里。那搜索引擎是如何爬取网页的呢？ 搜索引擎把整个互联网看作数据结构中的有向图，把每个页面看作一个顶点。如果某个页面中包含另外一个页面的链接，那我们就在两个顶点之间连一条有向边。我们可以利用图的遍历搜索算法，来遍历整个互联网中的网页。 我们前面介绍过两种图的遍历方法，深度优先和广度优先。搜索引擎采用的是广度优先搜索策略。具体点讲的话，那就是，我们先找一些比较知名的网页（专业的叫法是权重比较高）的链接（比如新浪主页网址、腾讯主页网址等），作为种子网页链接，放入到队列中。爬虫按照广度优先的策略，不停地从队列中取出链接，然后去爬取对应的网页，解析出网页里包含的其他网页链接，再将解析出来的链接添加到队列中。 基本的原理就是这么简单。但落实到实现层面，还有很多技术细节。我下面借助搜集阶段涉及的几个重要文件，来给你解释一下搜集工程都有哪些关键技术细节。 1. 待爬取网页链接文件：links.bin在广度优先搜索爬取页面的过程中，爬虫会不停地解析页面链接，将其放到队列中。于是，队列中的链接就会越来越多，可能会多到内存放不下。所以，我们用一个存储在磁盘中的文件（links.bin）来作为广度优先搜索中的队列。爬虫从 links.bin 文件中，取出链接去爬取对应的页面。等爬取到网页之后，将解析出来的链接，直接存储到 links.bin 文件中。 这样用文件来存储网页链接的方式，还有其他好处。比如，支持断点续爬。也就是说，当机器断电之后，网页链接不会丢失；当机器重启之后，还可以从之前爬取到的位置继续爬取。 关于如何解析页面获取链接，我额外多说几句。我们可以把整个页面看作一个大的字符串，然后利用字符串匹配算法，在这个大字符串中，搜索这样一个网页标签，然后顺序读取之间的字符串。这其实就是网页链接。 2. 网页判重文件：bloom_filter.bin如何避免重复爬取相同的网页呢？这个问题我们在位图那一节已经讲过了。使用布隆过滤器，我们就可以快速并且非常节省内存地实现网页的判重。 不过，还是刚刚那个问题，如果我们把布隆过滤器存储在内存中，那机器宕机重启之后，布隆过滤器就被清空了。这样就可能导致大量已经爬取的网页会被重复爬取。 这个问题该怎么解决呢？我们可以定期地（比如每隔半小时）将布隆过滤器持久化到磁盘中，存储在 bloom_filter.bin 文件中。这样，即便出现机器宕机，也只会丢失布隆过滤器中的部分数据。当机器重启之后，我们就可以重新读取磁盘中的 bloom_filter.bin 文件，将其恢复到内存中。 3. 原始网页存储文件：doc_raw.bin爬取到网页之后，我们需要将其存储下来，以备后面离线分析、索引之用。那如何存储海量的原始网页数据呢？ 如果我们把每个网页都存储为一个独立的文件，那磁盘中的文件就会非常多，数量可能会有几千万，甚至上亿。常用的文件系统显然不适合存储如此多的文件。所以，我们可以把多个网页存储在一个文件中。每个网页之间，通过一定的标识进行分隔，方便后续读取。具体的存储格式，如下图所示。其中，doc_id 这个字段是网页的编号，我们待会儿再解释。 当然，这样的一个文件也不能太大，因为文件系统对文件的大小也有一定的限制。所以，我们可以设置每个文件的大小不能超过一定的值（比如 1GB）。随着越来越多的网页被添加到文件中，文件的大小就会越来越大，当超过 1GB 的时候，我们就创建一个新的文件，用来存储新爬取的网页。 假设一台机器的硬盘大小是 100GB 左右，一个网页的平均大小是 64KB。那在一台机器上，我们可以存储 100 万到 200 万左右的网页。假设我们的机器的带宽是 10MB，那下载 100GB 的网页，大约需要 10000 秒。也就是说，爬取 100 多万的网页，也就是只需要花费几小时的时间。 4. 网页链接及其编号的对应文件：doc_id.bin刚刚我们提到了网页编号这个概念，我现在解释一下。网页编号实际上就是给每个网页分配一个唯一的 ID，方便我们后续对网页进行分析、索引。那如何给网页编号呢？ 我们可以按照网页被爬取的先后顺序，从小到大依次编号。具体是这样做的：我们维护一个中心的计数器，每爬取到一个网页之后，就从计数器中拿一个号码，分配给这个网页，然后计数器加一。在存储网页的同时，我们将网页链接跟编号之间的对应关系，存储在另一个 doc_id.bin 文件中。 爬虫在爬取网页的过程中，涉及的四个重要的文件，我就介绍完了。其中，links.bin 和 bloom_filter.bin 这两个文件是爬虫自身所用的。另外的两个（doc_raw.bin、doc_id.bin）是作为搜集阶段的成果，供后面的分析、索引、查询用的。 分析网页爬取下来之后，我们需要对网页进行离线分析。分析阶段主要包括两个步骤，第一个是抽取网页文本信息，第二个是分词并创建临时索引。我们逐一来讲解。 1. 抽取网页文本信息网页是半结构化数据，里面夹杂着各种标签、JavaScript 代码、CSS 样式。对于搜索引擎来说，它只关心网页中的文本信息，也就是，网页显示在浏览器中时，能被用户肉眼看到的那部分信息。我们如何从半结构化的网页中，抽取出搜索引擎关系的文本信息呢？ 我们之所以把网页叫作半结构化数据，是因为它本身是按照一定的规则来书写的。这个规则就是 HTML 语法规范。我们依靠 HTML 标签来抽取网页中的文本信息。这个抽取的过程，大体可以分为两步。 第一步是去掉 JavaScript 代码、CSS 格式以及下拉框中的内容（因为下拉框在用户不操作的情况下，也是看不到的）。也就是，，这三组标签之间的内容。我们可以利用 AC 自动机这种多模式串匹配算法，在网页这个大字符串中，一次性查找, , 这三个关键词。当找到某个关键词出现的位置之后，我们只需要依次往后遍历，直到对应结束标签（, , &lt;/option）为止。而这期间遍历到的字符串连带着标签就应该从网页中删除。 第二步是去掉所有 HTML 标签。这一步也是通过字符串匹配算法来实现的。过程跟第一步类似，我就不重复讲了。 2. 分词并创建临时索引经过上面的处理之后，我们就从网页中抽取出了我们关心的文本信息。接下来，我们要对文本信息进行分词，并且创建临时索引。 对于英文网页来说，分词非常简单。我们只需要通过空格、标点符号等分隔符，将每个单词分割开来就可以了。但是，对于中文来说，分词就复杂太多了。我这里介绍一种比较简单的思路，基于字典和规则的分词方法。 其中，字典也叫词库，里面包含大量常用的词语（我们可以直接从网上下载别人整理好的）。我们借助词库并采用最长匹配规则，来对文本进行分词。所谓最长匹配，也就是匹配尽可能长的词语。我举个例子解释一下。 比如要分词的文本是“中国人民解放了”，我们词库中有“中国”“中国人”“中国人民”“中国人民解放军”这几个词，那我们就取最长匹配，也就是“中国人民”划为一个词，而不是把“中国”、“中国人”划为一个词。具体到实现层面，我们可以将词库中的单词，构建成 Trie 树结构，然后拿网页文本在 Trie 树中匹配。 每个网页的文本信息在分词完成之后，我们都得到一组单词列表。我们把单词与网页之间的对应关系，写入到一个临时索引文件中（tmp_Index.bin），这个临时索引文件用来构建倒排索引文件。临时索引文件的格式如下： 在临时索引文件中，我们存储的是单词编号，也就是图中的 term_id，而非单词本身。这样做的目的主要是为了节省存储的空间。那这些单词的编号是怎么来的呢？ 给单词编号的方式，跟给网页编号类似。我们维护一个计数器，每当从网页文本信息中分割出一个新的单词的时候，我们就从计数器中取一个编号，分配给它，然后计数器加一。 在这个过程中，我们还需要使用散列表，记录已经编过号的单词。在对网页文本信息分词的过程中，我们拿分割出来的单词，先到散列表中查找，如果找到，那就直接使用已有的编号；如果没有找到，我们再去计数器中拿号码，并且将这个新单词以及编号添加到散列表中。 当所有的网页处理（分词及写入临时索引）完成之后，我们再将这个单词跟编号之间的对应关系，写入到磁盘文件中，并命名为 term_id.bin。 经过分析阶段，我们得到了两个重要的文件。它们分别是临时索引文件（tmp_index.bin）和单词编号文件（term_id.bin）。 索引索引阶段主要负责将分析阶段产生的临时索引，构建成倒排索引。倒排索引（ Inverted index）中记录了每个单词以及包含它的网页列表。文字描述比较难理解，我画了一张倒排索引的结构图，你一看就明白。 我们刚刚讲到，在临时索引文件中，记录的是单词跟每个包含它的文档之间的对应关系。那如何通过临时索引文件，构建出倒排索引文件呢？这是一个非常典型的算法问题，你可以先自己思考一下，再看我下面的讲解。 解决这个问题的方法有很多。考虑到临时索引文件很大，无法一次性加载到内存中，搜索引擎一般会选择使用多路归并排序的方法来实现。 我们先对临时索引文件，按照单词编号的大小进行排序。因为临时索引很大，所以一般基于内存的排序算法就没法处理这个问题了。我们可以用之前讲到的归并排序的处理思想，将其分割成多个小文件，先对每个小文件独立排序，最后再合并在一起。当然，实际的软件开发中，我们其实可以直接利用 MapReduce 来处理。 临时索引文件排序完成之后，相同的单词就被排列到了一起。我们只需要顺序地遍历排好序的临时索引文件，就能将每个单词对应的网页编号列表找出来，然后把它们存储在倒排索引文件中。具体的处理过程，我画成了一张图。通过图，你应该更容易理解。 除了倒排文件之外，我们还需要一个文件，来记录每个单词编号在倒排索引文件中的偏移位置。我们把这个文件命名为 term_offset.bin。这个文件的作用是，帮助我们快速地查找某个单词编号在倒排索引中存储的位置，进而快速地从倒排索引中读取单词编号对应的网页编号列表。 经过索引阶段的处理，我们得到了两个有价值的文件，它们分别是倒排索引文件（index.bin）和记录单词编号在索引文件中的偏移位置的文件（term_offset.bin）。 查询前面三个阶段的处理，只是为了最后的查询做铺垫。因此，现在我们就要利用之前产生的几个文件，来实现最终的用户搜索功能。 doc_id.bin：记录网页链接和编号之间的对应关系。 term_id.bin：记录单词和编号之间的对应关系。 index.bin：倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表。 term_offsert.bin：记录每个单词编号在倒排索引文件中的偏移位置。 这四个文件中，除了倒排索引文件（index.bin）比较大之外，其他的都比较小。为了方便快速查找数据，我们将其他三个文件都加载到内存中，并且组织成散列表这种数据结构。 当用户在搜索框中，输入某个查询文本的时候，我们先对用户输入的文本进行分词处理。假设分词之后，我们得到 k 个单词。 我们拿这 k 个单词，去 term_id.bin 对应的散列表中，查找对应的单词编号。经过这个查询之后，我们得到了这 k 个单词对应的单词编号。 我们拿这 k 个单词编号，去 term_offset.bin 对应的散列表中，查找每个单词编号在倒排索引文件中的偏移位置。经过这个查询之后，我们得到了 k 个偏移位置。 我们拿这 k 个偏移位置，去倒排索引（index.bin）中，查找 k 个单词对应的包含它的网页编号列表。经过这一步查询之后，我们得到了 k 个网页编号列表。 我们针对这 k 个网页编号列表，统计每个网页编号出现的次数。具体到实现层面，我们可以借助散列表来进行统计。统计得到的结果，我们按照出现次数的多少，从小到大排序。出现次数越多，说明包含越多的用户查询单词（用户输入的搜索文本，经过分词之后的单词）。 经过这一系列查询，我们就得到了一组排好序的网页编号。我们拿着网页编号，去 doc_id.bin 文件中查找对应的网页链接，分页显示给用户就可以了。 总结引申今天，我给你展示了一个小型搜索引擎的设计思路。这只是一个搜索引擎设计的基本原理，有很多优化、细节我们并未涉及，比如计算网页权重的PageRank算法、计算查询结果排名的tf-idf模型等等。 在讲解的过程中，我们涉及的数据结构和算法有：图、散列表、Trie 树、布隆过滤器、单模式字符串匹配算法、AC 自动机、广度优先遍历、归并排序等。如果对其中哪些内容不清楚，你可以回到对应的章节进行复习。 最后，如果有时间的话，我强烈建议你，按照我的思路，自己写代码实现一个简单的搜索引擎。这样写出来的，即便只是一个 demo，但对于你深入理解数据结构和算法，也是很有帮助的。 课后思考图的遍历方法有两种，深度优先和广度优先。我们讲到，搜索引擎中的爬虫是通过广度优先策略来爬取网页的。搜索引擎为什么选择广度优先策略，而不是深度优先策略呢？ 大部分搜索引擎在结果显示的时候，都支持摘要信息和网页快照。实际上，你只需要对我今天讲的设计思路，稍加改造，就可以支持这两项功能。你知道如何改造吗？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 54 | 算法实战（三）：剖析高性能队列Disruptor背后的数据结构和算法王争 2019-01-30  00:00 1.25x 讲述：冯永吉 大小：10.67M 时长：11:38 Disruptor 你是否听说过呢？它是一种内存消息队列。从功能上讲，它其实有点儿类似 Kafka。不过，和 Kafka 不同的是，Disruptor 是线程之间用于消息传递的队列。它在 Apache Storm、Camel、Log4j 2 等很多知名项目中都有广泛应用。 之所以如此受青睐，主要还是因为它的性能表现非常优秀。它比 Java 中另外一个非常常用的内存消息队列 ArrayBlockingQueue（ABS）的性能，要高一个数量级，可以算得上是最快的内存消息队列了。它还因此获得过 Oracle 官方的 Duke 大奖。 如此高性能的内存消息队列，在设计和实现上，必然有它独到的地方。今天，我们就来一块儿看下，Disruptor 是如何做到如此高性能的？其底层依赖了哪些数据结构和算法？ 基于循环队列的“生产者 - 消费者模型”什么是内存消息队列？对很多业务工程师或者前端工程师来说，可能会比较陌生。不过，如果我说“生产者 - 消费者模型”，估计大部分人都知道。在这个模型中，“生产者”生产数据，并且将数据放到一个中心存储容器中。之后，“消费者”从中心存储容器中，取出数据消费。 这个模型非常简单、好理解，那你有没有思考过，这里面存储数据的中心存储容器，是用什么样的数据结构来实现的呢？ 实际上，实现中心存储容器最常用的一种数据结构，就是我们在第 9 节讲的队列。队列支持数据的先进先出。正是这个特性，使得数据被消费的顺序性可以得到保证，也就是说，早被生产的数据就会早被消费。 我们在第 9 节讲过，队列有两种实现思路。一种是基于链表实现的链式队列，另一种是基于数组实现的顺序队列。不同的需求背景下，我们会选择不同的实现方式。 如果我们要实现一个无界队列，也就是说，队列的大小事先不确定，理论上可以支持无限大。这种情况下，我们适合选用链表来实现队列。因为链表支持快速地动态扩容。如果我们要实现一个有界队列，也就是说，队列的大小事先确定，当队列中数据满了之后，生产者就需要等待。直到消费者消费了数据，队列有空闲位置的时候，生产者才能将数据放入。 实际上，相较于无界队列，有界队列的应用场景更加广泛。毕竟，我们的机器内存是有限的。而无界队列占用的内存数量是不可控的。对于实际的软件开发来说，这种不可控的因素，就会有潜在的风险。在某些极端情况下，无界队列就有可能因为内存持续增长，而导致 OOM（Out of Memory）错误。 在第 9 节中，我们还讲过一种特殊的顺序队列，循环队列。我们讲过，非循环的顺序队列在添加、删除数据的工程中，会涉及数据的搬移操作，导致性能变差。而循环队列正好可以解决这个数据搬移的问题，所以，性能更加好。所以，大部分用到顺序队列的场景中，我们都选择用顺序队列中的循环队列。 实际上，循环队列这种数据结构，就是我们今天要讲的内存消息队列的雏形。我借助循环队列，实现了一个最简单的“生产者 - 消费者模型”。对应的代码我贴到这里，你可以看看。 为了方便你理解，对于生产者和消费者之间操作的同步，我并没有用到线程相关的操作。而是采用了“当队列满了之后，生产者就轮训等待；当队列空了之后，消费者就轮训等待”这样的措施。 public class Queue { private Long[] data; private int size = 0, head = 0, tail = 0; public Queue(int size) { ​ this.data = new Long[size]; ​ this.size = size; } public boolean add(Long element) { ​ if ((tail + 1) % size == head) return false; ​ data[tail] = element; ​ tail = (tail + 1) % size; ​ return true; } public Long poll() { ​ if (head == tail) return null; ​ long ret = data[head]; ​ head = (head + 1) % size; ​ return ret; } } public class Producer { private Queue queue; public Producer(Queue queue) { ​ this.queue = queue; } public void produce(Long data) throws InterruptedException { ​ while (!queue.add(data)) { ​ Thread.sleep(100); ​ } } } public class Consumer { private Queue queue; public Consumer(Queue queue) { ​ this.queue = queue; } public void comsume() throws InterruptedException { ​ while (true) { ​ Long data = queue.poll(); ​ if (data == null) { ​ Thread.sleep(100); ​ } else { ​ // TODO:…消费数据的业务逻辑… ​ } ​ } } } 基于加锁的并发“生产者 - 消费者模型”实际上，刚刚的“生产者 - 消费者模型”实现代码，是不完善的。为什么这么说呢？ 如果我们只有一个生产者往队列中写数据，一个消费者从队列中读取数据，那上面的代码是没有问题的。但是，如果有多个生产者在并发地往队列中写入数据，或者多个消费者并发地从队列中消费数据，那上面的代码就不能正确工作了。我来给你讲讲为什么。 在多个生产者或者多个消费者并发操作队列的情况下，刚刚的代码主要会有下面两个问题： 多个生产者写入的数据可能会互相覆盖； 多个消费者可能会读取重复的数据。 因为第一个问题和第二个问题产生的原理是类似的。所以，我着重讲解第一个问题是如何产生的以及该如何解决。对于第二个问题，你可以类比我对第一个问题的解决思路自己来想一想。 两个线程同时往队列中添加数据，也就相当于两个线程同时执行类 Queue 中的 add() 函数。我们假设队列的大小 size 是 10，当前的 tail 指向下标 7，head 指向下标 3，也就是说，队列中还有空闲空间。这个时候，线程 1 调用 add() 函数，往队列中添加一个值为 12 的数据；线程 2 调用 add() 函数，往队列中添加一个值为 15 的数据。在极端情况下，本来是往队列中添加了两个数据（12 和 15），最终可能只有一个数据添加成功，另一个数据会被覆盖。这是为什么呢？ 为了方便你查看队列 Queue 中的 add() 函数，我把它从上面的代码中摘录出来，贴在这里。 public boolean add(Long element) { if ((tail + 1) % size == head) return false; data[tail] = element; tail = (tail + 1) % size; return true; } 从这段代码中，我们可以看到，第 3 行给 data[tail]赋值，然后第 4 行才给 tail 的值加一。赋值和 tail 加一两个操作，并非原子操作。这就会导致这样的情况发生：当线程 1 和线程 2 同时执行 add() 函数的时候，线程 1 先执行完了第 3 行语句，将 data[7]（tail 等于 7）的值设置为 12。在线程 1 还未执行到第 4 行语句之前，也就是还未将 tail 加一之前，线程 2 执行了第 3 行语句，又将 data[7]的值设置为 15，也就是说，那线程 2 插入的数据覆盖了线程 1 插入的数据。原本应该插入两个数据（12 和 15）的，现在只插入了一个数据（15）。 那如何解决这种线程并发往队列中添加数据时，导致的数据覆盖、运行不正确问题呢？ 最简单的处理方法就是给这段代码加锁，同一时间只允许一个线程执行 add() 函数。这就相当于将这段代码的执行，由并行改成了串行，也就不存在我们刚刚说的问题了。 不过，天下没有免费的午餐，加锁将并行改成串行，必然导致多个生产者同时生产数据的时候，执行效率的下降。当然，我们可以继续优化代码，用CAS（compare and swap，比较并交换）操作等减少加锁的粒度，但是，这不是我们这节的重点。我们直接看 Disruptor 的处理方法。 基于无锁的并发“生产者 - 消费者模型”尽管 Disruptor 的源码读起来很复杂，但是基本思想其实非常简单。实际上，它是换了一种队列和“生产者 - 消费者模型”的实现思路。 之前的实现思路中，队列只支持两个操作，添加数据和读取并移除数据，分别对应代码中的 add() 函数和 poll() 函数，而 Disruptor 采用了另一种实现思路。 对于生产者来说，它往队列中添加数据之前，先申请可用空闲存储单元，并且是批量地申请连续的 n 个（n≥1）存储单元。当申请到这组连续的存储单元之后，后续往队列中添加元素，就可以不用加锁了，因为这组存储单元是这个线程独享的。不过，从刚刚的描述中，我们可以看出，申请存储单元的过程是需要加锁的。 对于消费者来说，处理的过程跟生产者是类似的。它先去申请一批连续可读的存储单元（这个申请的过程也是需要加锁的），当申请到这批存储单元之后，后续的读取操作就可以不用加锁了。 不过，还有一个需要特别注意的地方，那就是，如果生产者 A 申请到了一组连续的存储单元，假设是下标为 3 到 6 的存储单元，生产者 B 紧跟着申请到了下标是 7 到 9 的存储单元，那在 3 到 6 没有完全写入数据之前，7 到 9 的数据是无法读取的。这个也是 Disruptor 实现思路的一个弊端。 文字描述不好理解，我画了一个图，给你展示一下这个操作过程。 实际上，Disruptor 采用的是 RingBuffer 和 AvailableBuffer 这两个结构，来实现我刚刚讲的功能。不过，因为我们主要聚焦在数据结构和算法上，所以我对这两种结构做了简化，但是基本思想是一致的。如果你对 Disruptor 感兴趣，可以去阅读一下它的源码。 总结引申今天，我讲了如何实现一个高性能的并发队列。这里的“并发”两个字，实际上就是多线程安全的意思。 常见的内存队列往往采用循环队列来实现。这种实现方法，对于只有一个生产者和一个消费者的场景，已经足够了。但是，当存在多个生产者或者多个消费者的时候，单纯的循环队列的实现方式，就无法正确工作了。 这主要是因为，多个生产者在同时往队列中写入数据的时候，在某些情况下，会存在数据覆盖的问题。而多个消费者同时消费数据，在某些情况下，会存在消费重复数据的问题。 针对这个问题，最简单、暴力的解决方法就是，对写入和读取过程加锁。这种处理方法，相当于将原来可以并行执行的操作，强制串行执行，相应地就会导致操作性能的下降。 为了在保证逻辑正确的前提下，尽可能地提高队列在并发情况下的性能，Disruptor 采用了“两阶段写入”的方法。在写入数据之前，先加锁申请批量的空闲存储单元，之后往队列中写入数据的操作就不需要加锁了，写入的性能因此就提高了。Disruptor 对消费过程的改造，跟对生产过程的改造是类似的。它先加锁申请批量的可读取的存储单元，之后从队列中读取数据的操作也就不需要加锁了，读取的性能因此也就提高了。 你可能会觉得这个优化思路非常简单。实际上，不管架构设计还是产品设计，往往越简单的设计思路，越能更好地解决问题。正所谓“大道至简”，就是这个意思。 课后思考为了提高存储性能，我们往往通过分库分表的方式设计数据库表。假设我们有 8 张表用来存储用户信息。这个时候，每张用户表中的 ID 字段就不能通过自增的方式来产生了。因为这样的话，就会导致不同表之间的用户 ID 值重复。 为了解决这个问题，我们需要实现一个 ID 生成器，可以为所有的用户表生成唯一的 ID 号。那现在问题是，如何设计一个高性能、支持并发的、能够生成全局唯一 ID 的 ID 生成器呢？ 欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。 55 | 算法实战（四）：剖析微服务接口鉴权限流背后的数据结构和算法王争 2019-02-01  00:00 1.25x 讲述：冯永吉 大小：13.42M 时长：14:38 微服务是最近几年才兴起的概念。简单点讲，就是把复杂的大应用，解耦拆分成几个小的应用。这样做的好处有很多。比如，这样有利于团队组织架构的拆分，毕竟团队越大协作的难度越大；再比如，每个应用都可以独立运维，独立扩容，独立上线，各个应用之间互不影响。不用像原来那样，一个小功能上线，整个大应用都要重新发布。 不过，有利就有弊。大应用拆分成微服务之后，服务之间的调用关系变得更复杂，平台的整体复杂熵升高，出错的概率、debug 问题的难度都高了好几个数量级。所以，为了解决这些问题，服务治理便成了微服务的一个技术重点。 所谓服务治理，简单点讲，就是管理微服务，保证平台整体正常、平稳地运行。服务治理涉及的内容比较多，比如鉴权、限流、降级、熔断、监控告警等等。这些服务治理功能的实现，底层依赖大量的数据结构和算法。今天，我就拿其中的鉴权和限流这两个功能，来带你看看，它们的实现过程中都要用到哪些数据结构和算法。 鉴权背景介绍以防你之前可能对微服务没有太多了解，所以我对鉴权的背景做了简化。 假设我们有一个微服务叫用户服务（User Service）。它提供很多用户相关的接口，比如获取用户信息、注册、登录等，给公司内部的其他应用使用。但是，并不是公司内部所有应用，都可以访问这个用户服务，也并不是每个有访问权限的应用，都可以访问用户服务的所有接口。 我举了一个例子给你讲解一下，你可以看我画的这幅图。这里面，只有 A、B、C、D 四个应用可以访问用户服务，并且，每个应用只能访问用户服务的部分接口。 要实现接口鉴权功能，我们需要事先将应用对接口的访问权限规则设置好。当某个应用访问其中一个接口的时候，我们就可以拿应用的请求 URL，在规则中进行匹配。如果匹配成功，就说明允许访问；如果没有可以匹配的规则，那就说明这个应用没有这个接口的访问权限，我们就拒绝服务。 如何实现快速鉴权？接口的格式有很多，有类似 Dubbo 这样的 RPC 接口，也有类似 Spring Cloud 这样的 HTTP 接口。不同接口的鉴权实现方式是类似的，我这里主要拿 HTTP 接口给你讲解。 鉴权的原理比较简单、好理解。那具体到实现层面，我们该用什么数据结构来存储规则呢？用户请求 URL 在规则中快速匹配，又该用什么样的算法呢？ 实际上，不同的规则和匹配模式，对应的数据结构和匹配算法也是不一样的。所以，关于这个问题，我继续细化为三个更加详细的需求给你讲解。 1. 如何实现精确匹配规则？我们先来看最简单的一种匹配模式。只有当请求 URL 跟规则中配置的某个接口精确匹配时，这个请求才会被接受、处理。为了方便你理解，我举了一个例子，你可以看一下。 不同的应用对应不同的规则集合。我们可以采用散列表来存储这种对应关系。我这里着重讲下，每个应用对应的规则集合，该如何存储和匹配。 针对这种匹配模式，我们可以将每个应用对应的权限规则，存储在一个字符串数组中。当用户请求到来时，我们拿用户的请求 URL，在这个字符串数组中逐一匹配，匹配的算法就是我们之前学过的字符串匹配算法（比如 KMP、BM、BF 等）。 规则不会经常变动，所以，为了加快匹配速度，我们可以按照字符串的大小给规则排序，把它组织成有序数组这种数据结构。当要查找某个 URL 能否匹配其中某条规则的时候，我们可以采用二分查找算法，在有序数组中进行匹配。 而二分查找算法的时间复杂度是 O(logn)（n 表示规则的个数），这比起时间复杂度是 O(n) 的顺序遍历快了很多。对于规则中接口长度比较长，并且鉴权功能调用量非常大的情况，这种优化方法带来的性能提升还是非常可观的 。 2. 如何实现前缀匹配规则？我们再来看一种稍微复杂的匹配模式。只要某条规则可以匹配请求 URL 的前缀，我们就说这条规则能够跟这个请求 URL 匹配。同样，为了方便你理解这种匹配模式，我还是举一个例子说明一下。 不同的应用对应不同的规则集合。我们采用散列表来存储这种对应关系。我着重讲一下，每个应用的规则集合，最适合用什么样的数据结构来存储。 在Trie 树那节，我们讲到，Trie 树非常适合用来做前缀匹配。所以，针对这个需求，我们可以将每个用户的规则集合，组织成 Trie 树这种数据结构。 不过，Trie 树中的每个节点不是存储单个字符，而是存储接口被“/”分割之后的子目录（比如“/user/name”被分割为“user”“name”两个子目录）。因为规则并不会经常变动，所以，在 Trie 树中，我们可以把每个节点的子节点们，组织成有序数组这种数据结构。在匹配的过程中，我们可以利用二分查找算法，决定从一个节点应该跳到哪一个子节点。 3. 如何实现模糊匹配规则？如果我们的规则更加复杂，规则中包含通配符，比如“*”表示匹配任意多个子目录，“”表示匹配任意一个子目录。只要用户请求 URL 可以跟某条规则模糊匹配，我们就说这条规则适用于这个请求。为了方便你理解，我举一个例子来解释一下。 不同的应用对应不同的规则集合。我们还是采用散列表来存储这种对应关系。这点我们刚才讲过了，这里不再重复说了。我们着重看下，每个用户对应的规则集合，该用什么数据结构来存储？针对这种包含通配符的模糊匹配，我们又该使用什么算法来实现呢？ 还记得我们在回溯算法那节讲的正则表达式的例子吗？我们可以借助正则表达式那个例子的解决思路，来解决这个问题。我们采用回溯算法，拿请求 URL 跟每条规则逐一进行模糊匹配。如何用回溯算法进行模糊匹配，这部分我就不重复讲了。你如果忘记了，可以回到相应章节复习一下。 不过，这个解决思路的时间复杂度是非常高的。我们需要拿每一个规则，跟请求 URL 匹配一遍。那有没有办法可以继续优化一下呢？ 实际上，我们可以结合实际情况，挖掘出这样一个隐形的条件，那就是，并不是每条规则都包含通配符，包含通配符的只是少数。于是，我们可以把不包含通配符的规则和包含通配符的规则分开处理。 我们把不包含通配符的规则，组织成有序数组或者 Trie 树（具体组织成什么结构，视具体的需求而定，是精确匹配，就组织成有序数组，是前缀匹配，就组织成 Trie 树），而这一部分匹配就会非常高效。剩下的是少数包含通配符的规则，我们只要把它们简单存储在一个数组中就可以了。尽管匹配起来会比较慢，但是毕竟这种规则比较少，所以这种方法也是可以接受的。 当接收到一个请求 URL 之后，我们可以先在不包含通配符的有序数组或者 Trie 树中查找。如果能够匹配，就不需要继续在通配符规则中匹配了；如果不能匹配，就继续在通配符规则中查找匹配。 限流背景介绍讲完了鉴权的实现思路，我们再来看一下限流。 所谓限流，顾名思义，就是对接口调用的频率进行限制。比如每秒钟不能超过 100 次调用，超过之后，我们就拒绝服务。限流的原理听起来非常简单，但它在很多场景中，发挥着重要的作用。比如在秒杀、大促、双 11、618 等场景中，限流已经成为了保证系统平稳运行的一种标配的技术解决方案。 按照不同的限流粒度，限流可以分为很多种类型。比如给每个接口限制不同的访问频率，或者给所有接口限制总的访问频率，又或者更细粒度地限制某个应用对某个接口的访问频率等等。 不同粒度的限流功能的实现思路都差不多，所以，我今天主要针对限制所有接口总的访问频率这样一个限流需求来讲解。其他粒度限流需求的实现思路，你可以自己思考。 如何实现精准限流？最简单的限流算法叫固定时间窗口限流算法。这种算法是如何工作的呢？首先我们需要选定一个时间起点，之后每当有接口请求到来，我们就将计数器加一。如果在当前时间窗口内，根据限流规则（比如每秒钟最大允许 100 次访问请求），出现累加访问次数超过限流值的情况时，我们就拒绝后续的访问请求。当进入下一个时间窗口之后，计数器就清零重新计数。 这种基于固定时间窗口的限流算法的缺点是，限流策略过于粗略，无法应对两个时间窗口临界时间内的突发流量。这是怎么回事呢？我举一个例子给你解释一下。 假设我们的限流规则是，每秒钟不能超过 100 次接口请求。第一个 1s 时间窗口内，100 次接口请求都集中在最后 10ms 内。在第二个 1s 的时间窗口内，100 次接口请求都集中在最开始的 10ms 内。虽然两个时间窗口内流量都符合限流要求（≤100 个请求），但在两个时间窗口临界的 20ms 内，会集中有 200 次接口请求。固定时间窗口限流算法并不能对这种情况做限制，所以，集中在这 20ms 内的 200 次请求就有可能压垮系统。 为了解决这个问题，我们可以对固定时间窗口限流算法稍加改造。我们可以限制任意时间窗口（比如 1s）内，接口请求数都不能超过某个阈值（ 比如 100 次）。因此，相对于固定时间窗口限流算法，这个算法叫滑动时间窗口限流算法。 流量经过滑动时间窗口限流算法整形之后，可以保证任意一个 1s 的时间窗口内，都不会超过最大允许的限流值，从流量曲线上来看会更加平滑。那具体到实现层面，我们该如何来做呢？ 我们假设限流的规则是，在任意 1s 内，接口的请求次数都不能大于 K 次。我们就维护一个大小为 K+1 的循环队列，用来记录 1s 内到来的请求。注意，这里循环队列的大小等于限流次数加一，因为循环队列存储数据时会浪费一个存储单元。 当有新的请求到来时，我们将与这个新请求的时间间隔超过 1s 的请求，从队列中删除。然后，我们再来看循环队列中是否有空闲位置。如果有，则把新请求存储在队列尾部（tail 指针所指的位置）；如果没有，则说明这 1 秒内的请求次数已经超过了限流值 K，所以这个请求被拒绝服务。 为了方便你理解，我举一个例子，给你解释一下。在这个例子中，我们假设限流的规则是，任意 1s 内，接口的请求次数都不能大于 6 次。 即便滑动时间窗口限流算法可以保证任意时间窗口内，接口请求次数都不会超过最大限流值，但是仍然不能防止，在细时间粒度上访问过于集中的问题。 比如我刚刚举的那个例子，第一个 1s 的时间窗口内，100 次请求都集中在最后 10ms 中，也就是说，基于时间窗口的限流算法，不管是固定时间窗口还是滑动时间窗口，只能在选定的时间粒度上限流，对选定时间粒度内的更加细粒度的访问频率不做限制。 实际上，针对这个问题，还有很多更加平滑的限流算法，比如令牌桶算法、漏桶算法等。如果感兴趣，你可以自己去研究一下。 总结引申今天，我们讲解了跟微服务相关的接口鉴权和限流功能的实现思路。现在，我稍微总结一下。 关于鉴权，我们讲了三种不同的规则匹配模式。不管是哪种匹配模式，我们都可以用散列表来存储不同应用对应的不同规则集合。对于每个应用的规则集合的存储，三种匹配模式使用不同的数据结构。 对于第一种精确匹配模式，我们利用有序数组来存储每个应用的规则集合，并且通过二分查找和字符串匹配算法，来匹配请求 URL 与规则。对于第二种前缀匹配模式，我们利用 Trie 树来存储每个应用的规则集合。对于第三种模糊匹配模式，我们采用普通的数组来存储包含通配符的规则，通过回溯算法，来进行请求 URL 与规则的匹配。 关于限流，我们讲了两种限流算法，第一种是固定时间窗口限流算法，第二种是滑动时间窗口限流算法。对于滑动时间窗口限流算法，我们用了之前学习过的循环队列来实现。比起固定时间窗口限流算法，它对流量的整形效果更好，流量更加平滑。 从今天的学习中，我们也可以看出，对于基础架构工程师来说，如果不精通数据结构和算法，我们就很难开发出性能卓越的基础架构、中间件。这其实就体现了数据结构和算法的重要性。 课后思考除了用循环队列来实现滑动时间窗口限流算法之外，我们是否还可以用其他数据结构来实现呢？请对比一下这些数据结构跟循环队列在解决这个问题时的优劣之处。 分析一下鉴权那部分内容中，前缀匹配算法的时间复杂度和空间复杂度。 最后，有个消息提前通知你一下。本节是专栏的倒数第二节课了，不知道学到现在，你掌握得怎么样呢？为了帮你复习巩固，做到真正掌握这些知识，我针对专栏涉及的数据结构和算法，精心编制了一套练习题。从正月初一到初七，每天发布一篇。你要做好准备哦！ 56 | 算法实战（五）：如何用学过的数据结构和算法实现一个短网址系统？王争 2019-02-04  00:00 1.25x 讲述：冯永吉 大小：13.98M 时长：15:15 短网址服务你用过吗？如果我们在微博里发布一条带网址的信息，微博会把里面的网址转化成一个更短的网址。我们只要访问这个短网址，就相当于访问原始的网址。比如下面这两个网址，尽管长度不同，但是都可以跳转到我的一个 GitHub 开源项目里。其中，第二个网址就是通过新浪提供的短网址服务生成的。 原始网址：https://github.com/wangzheng0822/ratelimiter4j 短网址：http://t.cn/EtR9QEG 从功能上讲，短网址服务其实非常简单，就是把一个长的网址转化成一个短的网址。作为一名软件工程师，你是否思考过，这样一个简单的功能，是如何实现的呢？底层都依赖了哪些数据结构和算法呢？ 短网址服务整体介绍刚刚我们讲了，短网址服务的一个核心功能，就是把原始的长网址转化成短网址。除了这个功能之外，短网址服务还有另外一个必不可少的功能。那就是，当用户点击短网址的时候，短网址服务会将浏览器重定向为原始网址。这个过程是如何实现的呢？ 为了方便你理解，我画了一张对比图，你可以看下。 从图中我们可以看出，浏览器会先访问短网址服务，通过短网址获取到原始网址，再通过原始网址访问到页面。不过这部分功能并不是我们今天要讲的重点。我们重点来看，如何将长网址转化成短网址？ 如何通过哈希算法生成短网址？我们前面学过哈希算法。哈希算法可以将一个不管多长的字符串，转化成一个长度固定的哈希值。我们可以利用哈希算法，来生成短网址。 前面我们已经提过一些哈希算法了，比如 MD5、SHA 等。但是，实际上，我们并不需要这些复杂的哈希算法。在生成短网址这个问题上，毕竟，我们不需要考虑反向解密的难度，所以我们只需要关心哈希算法的计算速度和冲突概率。 能够满足这样要求的哈希算法有很多，其中比较著名并且应用广泛的一个哈希算法，那就是MurmurHash 算法。尽管这个哈希算法在 2008 年才被发明出来，但现在它已经广泛应用到 Redis、MemCache、Cassandra、HBase、Lucene 等众多著名的软件中。 MurmurHash 算法提供了两种长度的哈希值，一种是 32bits，一种是 128bits。为了让最终生成的短网址尽可能短，我们可以选择 32bits 的哈希值。对于开头那个 GitHub 网址，经过 MurmurHash 计算后，得到的哈希值就是 181338494。我们再拼上短网址服务的域名，就变成了最终的短网址 http://t.cn/181338494（其中，http://t.cn 是短网址服务的域名）。 1. 如何让短网址更短？不过，你可能已经看出来了，通过 MurmurHash 算法得到的短网址还是很长啊，而且跟我们开头那个网址的格式好像也不一样。别着急，我们只需要稍微改变一个哈希值的表示方法，就可以轻松把短网址变得更短些。 我们可以将 10 进制的哈希值，转化成更高进制的哈希值，这样哈希值就变短了。我们知道，16 进制中，我们用 A～F，来表示 10～15。在网址 URL 中，常用的合法字符有 0～9、a～z、A～Z 这样 62 个字符。为了让哈希值表示起来尽可能短，我们可以将 10 进制的哈希值转化成 62 进制。具体的计算过程，我写在这里了。最终用 62 进制表示的短网址就是http://t.cn/cgSqq。 2. 如何解决哈希冲突问题？不过，我们前面讲过，哈希算法无法避免的一个问题，就是哈希冲突。尽管 MurmurHash 算法，冲突的概率非常低。但是，一旦冲突，就会导致两个原始网址被转化成同一个短网址。当用户访问短网址的时候，我们就无从判断，用户想要访问的是哪一个原始网址了。这个问题该如何解决呢？ 一般情况下，我们会保存短网址跟原始网址之间的对应关系，以便后续用户在访问短网址的时候，可以根据对应关系，查找到原始网址。存储这种对应关系的方式有很多，比如我们自己设计存储系统或者利用现成的数据库。前面我们讲到的数据库有 MySQL、Redis。我们就拿 MySQL 来举例。假设短网址与原始网址之间的对应关系，就存储在 MySQL 数据库中。 当有一个新的原始网址需要生成短网址的时候，我们先利用 MurmurHash 算法，生成短网址。然后，我们拿这个新生成的短网址，在 MySQL 数据库中查找。 如果没有找到相同的短网址，这也就表明，这个新生成的短网址没有冲突。于是我们就将这个短网址返回给用户（请求生成短网址的用户），然后将这个短网址与原始网址之间的对应关系，存储到 MySQL 数据库中。 如果我们在数据库中，找到了相同的短网址，那也并不一定说明就冲突了。我们从数据库中，将这个短网址对应的原始网址也取出来。如果数据库中的原始网址，跟我们现在正在处理的原始网址是一样的，这就说明已经有人请求过这个原始网址的短网址了。我们就可以拿这个短网址直接用。如果数据库中记录的原始网址，跟我们正在处理的原始网址不一样，那就说明哈希算法发生了冲突。不同的原始网址，经过计算，得到的短网址重复了。这个时候，我们该怎么办呢？ 我们可以给原始网址拼接一串特殊字符，比如“[DUPLICATED]”，然后再重新计算哈希值，两次哈希计算都冲突的概率，显然是非常低的。假设出现非常极端的情况，又发生冲突了，我们可以再换一个拼接字符串，比如“[OHMYGOD]”，再计算哈希值。然后把计算得到的哈希值，跟原始网址拼接了特殊字符串之后的文本，一并存储在 MySQL 数据库中。 当用户访问短网址的时候，短网址服务先通过短网址，在数据库中查找到对应的原始网址。如果原始网址有拼接特殊字符（这个很容易通过字符串匹配算法找到），我们就先将特殊字符去掉，然后再将不包含特殊字符的原始网址返回给浏览器。 3. 如何优化哈希算法生成短网址的性能？为了判断生成的短网址是否冲突，我们需要拿生成的短网址，在数据库中查找。如果数据库中存储的数据非常多，那查找起来就会非常慢，势必影响短网址服务的性能。那有没有什么优化的手段呢？ 还记得我们之前讲的 MySQL 数据库索引吗？我们可以给短网址字段添加 B+ 树索引。这样通过短网址查询原始网址的速度就提高了很多。实际上，在真实的软件开发中，我们还可以通过一个小技巧，来进一步提高速度。 在短网址生成的过程中，我们会跟数据库打两次交道，也就是会执行两条 SQL 语句。第一个 SQL 语句是通过短网址查询短网址与原始网址的对应关系，第二个 SQL 语句是将新生成的短网址和原始网址之间的对应关系存储到数据库。 我们知道，一般情况下，数据库和应用服务（只做计算不存储数据的业务逻辑部分）会部署在两个独立的服务器或者虚拟服务器上。那两条 SQL 语句的执行就需要两次网络通信。这种 IO 通信耗时以及 SQL 语句的执行，才是整个短网址服务的性能瓶颈所在。所以，为了提高性能，我们需要尽量减少 SQL 语句。那又该如何减少 SQL 语句呢？ 我们可以给数据库中的短网址字段，添加一个唯一索引（不只是索引，还要求表中不能有重复的数据）。当有新的原始网址需要生成短网址的时候，我们并不会先拿生成的短网址，在数据库中查找判重，而是直接将生成的短网址与对应的原始网址，尝试存储到数据库中。如果数据库能够将数据正常写入，那说明并没有违反唯一索引，也就是说，这个新生成的短网址并没有冲突。 当然，如果数据库反馈违反唯一性索引异常，那我们还得重新执行刚刚讲过的“查询、写入”过程，SQL 语句执行的次数不减反增。但是，在大部分情况下，我们把新生成的短网址和对应的原始网址，插入到数据库的时候，并不会出现冲突。所以，大部分情况下，我们只需要执行一条写入的 SQL 语句就可以了。所以，从整体上看，总的 SQL 语句执行次数会大大减少。 实际上，我们还有另外一个优化 SQL 语句次数的方法，那就是借助布隆过滤器。 我们把已经生成的短网址，构建成布隆过滤器。我们知道，布隆过滤器是比较节省内存的一种存储结构，长度是 10 亿的布隆过滤器，也只需要 125MB 左右的内存空间。 当有新的短网址生成的时候，我们先拿这个新生成的短网址，在布隆过滤器中查找。如果查找的结果是不存在，那就说明这个新生成的短网址并没有冲突。这个时候，我们只需要再执行写入短网址和对应原始网页的 SQL 语句就可以了。通过先查询布隆过滤器，总的 SQL 语句的执行次数减少了。 到此，利用哈希算法来生成短网址的思路，我就讲完了。实际上，这种解决思路已经完全满足需求了，我们已经可以直接用到真实的软件开发中。不过，我们还有另外一种短网址的生成算法，那就是利用自增的 ID 生成器来生成短网址。我们接下来就看一下，这种算法是如何工作的？对于哈希算法生成短网址来说，它又有什么优势和劣势？ 如何通过 ID 生成器生成短网址？我们可以维护一个 ID 自增生成器。它可以生成 1、2、3…这样自增的整数 ID。当短网址服务接收到一个原始网址转化成短网址的请求之后，它先从 ID 生成器中取一个号码，然后将其转化成 62 进制表示法，拼接到短网址服务的域名（比如http://t.cn/）后面，就形成了最终的短网址。最后，我们还是会把生成的短网址和对应的原始网址存储到数据库中。 理论非常简单好理解。不过，这里有几个细节问题需要处理。 1. 相同的原始网址可能会对应不同的短网址每次新来一个原始网址，我们就生成一个新的短网址，这种做法就会导致两个相同的原始网址生成了不同的短网址。这个该如何处理呢？实际上，我们有两种处理思路。 第一种处理思路是不做处理。听起来有点无厘头，我稍微解释下你就明白了。实际上，相同的原始网址对应不同的短网址，这个用户是可以接受的。在大部分短网址的应用场景里，用户只关心短网址能否正确地跳转到原始网址。至于短网址长什么样子，他其实根本就不关心。所以，即便是同一个原始网址，两次生成的短网址不一样，也并不会影响到用户的使用。 第二种处理思路是借助哈希算法生成短网址的处理思想，当要给一个原始网址生成短网址的时候，我们要先拿原始网址在数据库中查找，看数据库中是否已经存在相同的原始网址了。如果数据库中存在，那我们就取出对应的短网址，直接返回给用户。 不过，这种处理思路有个问题，我们需要给数据库中的短网址和原始网址这两个字段，都添加索引。短网址上加索引是为了提高用户查询短网址对应的原始网页的速度，原始网址上加索引是为了加快刚刚讲的通过原始网址查询短网址的速度。这种解决思路虽然能满足“相同原始网址对应相同短网址”这样一个需求，但是是有代价的：一方面两个索引会占用更多的存储空间，另一方面索引还会导致插入、删除等操作性能的下降。 2. 如何实现高性能的 ID 生成器？实现 ID 生成器的方法有很多，比如利用数据库自增字段。当然我们也可以自己维护一个计数器，不停地加一加一。但是，一个计数器来应对频繁的短网址生成请求，显然是有点吃力的（因为计数器必须保证生成的 ID 不重复，笼统概念上讲，就是需要加锁）。如何提高 ID 生成器的性能呢？关于这个问题，实际上，有很多解决思路。我这里给出两种思路。 第一种思路是借助第 54 节中讲的方法。我们可以给 ID 生成器装多个前置发号器。我们批量地给每个前置发号器发送 ID 号码。当我们接受到短网址生成请求的时候，就选择一个前置发号器来取号码。这样通过多个前置发号器，明显提高了并发发号的能力。 第二种思路跟第一种差不多。不过，我们不再使用一个 ID 生成器和多个前置发号器这样的架构，而是，直接实现多个 ID 生成器同时服务。为了保证每个 ID 生成器生成的 ID 不重复。我们要求每个 ID 生成器按照一定的规则，来生成 ID 号码。比如，第一个 ID 生成器只能生成尾号为 0 的，第二个只能生成尾号为 1 的，以此类推。这样通过多个 ID 生成器同时工作，也提高了 ID 生成的效率。 总结引申今天，我们讲了短网址服务的两种实现方法。我现在来稍微总结一下。 第一种实现思路是通过哈希算法生成短网址。我们采用计算速度快、冲突概率小的 MurmurHash 算法，并将计算得到的 10 进制数，转化成 62 进制表示法，进一步缩短短网址的长度。对于哈希算法的哈希冲突问题，我们通过给原始网址添加特殊前缀字符，重新计算哈希值的方法来解决。 第二种实现思路是通过 ID 生成器来生成短网址。我们维护一个 ID 自增的 ID 生成器，给每个原始网址分配一个 ID 号码，并且同样转成 62 进制表示法，拼接到短网址服务的域名之后，形成最终的短网址。 课后思考如果我们还要额外支持用户自定义短网址功能（http//t.cn/{用户自定部分}），我们又该如何改造刚刚的算法呢? 我们在讲通过 ID 生成器生成短网址这种实现思路的时候，讲到相同的原始网址可能会对应不同的短网址。针对这个问题，其中一个解决思路就是，不做处理。但是，如果每个请求都生成一个短网址，并且存储在数据库中，那这样会不会撑爆数据库呢？我们又该如何解决呢？ 今天是农历的大年三十，我们专栏的正文到这里也就全部结束了。从明天开始，我会每天发布一篇练习题，内容针对专栏涉及的数据结构和算法。从初一到初七，帮你复习巩固所学知识，拿下数据结构和算法，打响新年进步的第一枪！明天见！ 出自【1】 参考 参考1🔼","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"底层","slug":"底层","permalink":"http://yoursite.com/tags/底层/"},{"name":"网课","slug":"网课","permalink":"http://yoursite.com/tags/网课/"}],"author":"Dufy"},{"title":"⭐️并发编程【未看完】","slug":"并发编程","date":"2021-03-24T16:00:00.000Z","updated":"2021-03-30T15:43:56.014Z","comments":true,"path":"2021/03/25/并发编程/","link":"","permalink":"http://yoursite.com/2021/03/25/并发编程/","excerpt":"","text":"并发引入？ 提升程序运行速度 高级别+高薪必备 提速方法 python 中的支持 多线程Thread 多进程Process 多协程Coroutine CPU/IO CPU密集型（CPU-bound） 任务的运行会受到CPU限制 也叫计算密集型，是指I/O在很短时间内就可以完成，CPU 需要大量的计算和处理，特点是CPU占有率相当高 如，压缩解压缩、加密解密、正则表达式搜索 I/O密集型（I/O bound） 指的是系统运作大部分状况是CPU在等I/O（磁盘）的读/写操作，CPU占有率低 如，文件处理程序、网络爬虫程序、读写数据库程序 多线程、多进程、多协程对比 python多进程、多线程模块 多进程 不采用多进程 123456789101112131415161718192021from random import randintfrom time import time, sleepdef download_task(filename): print('开始下载%s...' % filename) time_to_download = randint(5, 10) sleep(time_to_download) print('%s下载完成! 耗费了%d秒' % (filename, time_to_download))def main(): start = time() download_task('Python从入门到住院.pdf') download_task('Peking Hot.avi') end = time() print('总共耗费了%.2f秒.' % (end - start))if __name__ == '__main__': main() 更换为多进程： 12345678910111213141516171819202122232425262728from multiprocessing import Processfrom os import getpidfrom random import randintfrom time import time, sleepdef download_task(filename): print('启动下载进程，进程号[%d].' % getpid()) print('开始下载%s...' % filename) time_to_download = randint(5, 10) sleep(time_to_download) print('%s下载完成! 耗费了%d秒' % (filename, time_to_download))def main(): start = time() p1 = Process(target=download_task, args=('Python从入门到住院.pdf', )) p1.start() p2 = Process(target=download_task, args=('Peking Hot.avi', )) p2.start() # start方法用来启动进程 p1.join() # join方法表示等待进程执行结束 p2.join() end = time() print('总共耗费了%.2f秒.' % (end - start))if __name__ == '__main__': main() 我们也可以使用subprocess模块中的类和函数来创建和启动子进程，然后通过管道来和子进程通信，这些内容我们不在此进行讲解，有兴趣的读者可以自己了解这些知识。 多线程 目前的多线程开发推荐使用threading模块 1234567891011121314151617181920212223242526from random import randintfrom threading import Threadfrom time import time, sleepdef download(filename): print('开始下载%s...' % filename) time_to_download = 3 sleep(time_to_download) print('%s下载完成! 耗费了%d秒' % (filename, time_to_download))def main(): start = time() t1 = Thread(target=download, args=('Python从入门到住院.pdf',)) t1.start() t2 = Thread(target=download, args=('Peking Hot.avi',)) t2.start() t1.join() t2.join() end = time() print('总共耗费了%.3f秒' % (end - start))if __name__ == '__main__': main() 还可以通过继承Thread类的方式来创建自定义的线程类，然后再创建线程对象并启动线程 1234567891011121314151617181920212223242526272829303132from random import randintfrom threading import Threadfrom time import time, sleepclass DownloadTask(Thread): def __init__(self, filename): super().__init__() self._filename = filename def run(self): print('开始下载%s...' % self._filename) time_to_download = 3 sleep(time_to_download) print('%s下载完成! 耗费了%d秒' % (self._filename, time_to_download))def main(): start = time() t1 = DownloadTask('Python从入门到住院.pdf') t1.start() t2 = DownloadTask('Peking Hot.avi') t2.start() t1.join() t2.join() end = time() print('总共耗费了%.2f秒.' % (end - start))if __name__ == '__main__': main() 如何选择技术 临界资源和锁 临界资源 因为多个线程可以共享进程的内存空间，因此要实现多个线程间的通信相对简单，大家能想到的最直接的办法就是设置一个全局变量，多个线程共享这个全局变量即可。但是当多个线程共享同一个变量（我们通常称之为“资源”）的时候，很有可能产生不可控的结果从而导致程序失效甚至崩溃。如果一个资源被多个线程竞争使用，那么我们通常称之为“临界资源” 对“临界资源”的访问需要加上保护，否则资源会处于“混乱”的状态。 e.g. 100个线程向同一个银行账户转账（转入1元钱）的场景，在这个例子中，银行账户就是一个临界资源，在没有保护的情况下我们很有可能会得到错误的结果。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from time import sleepfrom threading import Threadclass Account(object): def __init__(self): self._balance = 0 def deposit(self, money): # 计算存款后的余额 new_balance = self._balance + money # 模拟受理存款业务需要0.01秒的时间 sleep(0.01) # 修改账户余额 self._balance = new_balance @property def balance(self): return self._balanceclass AddMoneyThread(Thread): def __init__(self, account, money): super().__init__() self._account = account self._money = money def run(self): self._account.deposit(self._money)def main(): account = Account() threads = [] # 创建100个存款的线程向同一个账户中存钱 for _ in range(100): t = AddMoneyThread(account, 1) threads.append(t) t.start() # 等所有存款的线程都执行完毕 for t in threads: t.join() print('账户余额为: ￥%d元' % account.balance)if __name__ == '__main__': main() 1账户余额为: ￥2元 使用“锁”来保护对银行账户的操作，从而获得正确的结果: 123456789101112131415161718192021from threading import Thread,Lockclass Account(object): def __init__(self): self._balance = 0 self._lock = Lock() def deposit(self, money): # 先获取锁才能执行后续的代码 self._lock.acquire() try: new_balance = self._balance + money sleep(0.01) self._balance = new_balance finally: # 在finally中执行释放锁的操作保证正常异常锁都能释放 self._lock.release() @property def balance(self): return self._balance 1账户余额为: ￥100元 更多例子，参见：进程和线程之应用案例 例子1：将耗时间的任务放到线程中以获得更好的用户体验。 例子2：使用多进程对复杂任务进行“分而治之”。 python GILPython 为何慢 动态类型语言，边解释边执行 GIL GIL https://www.dabeaz.com/python/UnderstandingGIL.pdf 为何有GIL ？ 如何规避GIL? 多线程举例：爬虫 创建多线程 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# -*- coding: utf-8 -*-\"\"\"@Time : 2021/3/28 7:07 下午@Author : Dufy@Email : 813540660@qq.com@File : multi-thread-crawl.py@Software: PyCharm Description :1) 多线程网页加速效果展示2) \"\"\"import threadingimport timeimport requestsurls = [f\"https://www.cnblogs.com/#p&#123;i&#125;\" for i in range(1,50+1)]def crawl(url): r= requests.get(url) print(url, len(r.text))def single_thread(): print(\"single thread begin\") for url in urls: crawl(url) print(\"single thread end!!\")def multi_thread(): \"\"\"多线程\"\"\" print(\"multi thread begin\") threads = [] for url in urls: threads.append( threading.Thread(target=crawl, args=(url,)) ) for thread in threads: thread.start() for thread in threads: thread.join() print(\"multi thread end\")if __name__ == \"__main__\": pass start = time.time() single_thread() end = time.time() print(f'单线程花费：&#123;end-start&#125; s') start = time.time() multi_thread() end = time.time() print(f'多线程花费：&#123;end - start&#125; s') 单线程花费：8.063177108764648 s 多线程花费：0.9351308345794678 s 生产者-消费者模式的多线程爬虫多组件的Pipeline技术架构复杂 的事情一般都不会一下子做完，而是会分很多中间步骤一步步完成 生产者-消费者爬虫架构 多线程数据通信 queue.Queue post 方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# -*- coding: utf-8 -*-\"\"\"@Time : 2021/3/28 7:07 下午@Author : Dufy@Email : 813540660@qq.com@File : blog_crawl.py@Software: PyCharm Description :1) 基础版本展示2) post 方法 抓包\"\"\"import threadingimport timeimport requestsfrom bs4 import BeautifulSoupimport json# payload = &#123;\"CategoryType\":\"SiteHome\",\"ParentCategoryId\":0,\"CategoryId\":808,\"PageIndex\":3,\"TotalPostCount\":4000,\"ItemListActionName\":\"AggSitePostList\"&#125;header = &#123;'accept': 'text/plain, */*; q=0.01', 'accept-encoding': 'gzip, deflate, br', 'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8', 'content-type': 'application/json; charset=UTF-8', 'origin': 'https://www.cnblogs.com', 'referer': 'https://www.cnblogs.com/', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82, Safari/537.36', 'x-requested-with': 'XMLHttpRequest'&#125;def crawl(url, page_index): r= requests.post(url, data = json.dumps(&#123;\"CategoryType\":\"SiteHome\",\"ParentCategoryId\":0,\"CategoryId\":808,\"PageIndex\":page_index,\"TotalPostCount\":4000,\"ItemListActionName\":\"AggSitePostList\"&#125;), headers = header) # print(r.text) return r.textdef parse(html): # class =\"post-item-title\" soup = BeautifulSoup(html, \"html.parser\") links = soup.find_all(\"a\", class_ =\"post-item-title\") return [(link['href'], link.get_text())for link in links]if __name__ == \"__main__\": page_index = 10 for result in parse(crawl('https://www.cnblogs.com/AggSite/AggSitePostList', page_index)): print(result) links = soup.find_all(&quot;a&quot;, class_ =&quot;post-item-title&quot;)来自于： 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# -*- coding: utf-8 -*-\"\"\"@Time : 2021/3/29 9:58 下午@Author : Dufy@Email : 813540660@qq.com@File : producer-consumer-spider.py@Software: PyCharm Description :1) 生产者、消费者模式爬虫2) \"\"\"import queueimport blog_crawlimport timeimport randomimport threadingurl ='https://www.cnblogs.com/AggSite/AggSitePostList'def do_crawl(url_queue:queue.Queue, html_queue:queue.Queue): while 1: index = url_queue.get() html = blog_crawl.crawl(url,index) html_queue.put(html) print(threading.current_thread().name, f'crawl&#123;url&#125;', 'url_queue.size=', url_queue.qsize()) time.sleep(random.randint(1,2))def do_parse(html_queue:queue.Queue, fout): while 1: html = html_queue.get() results = blog_crawl.parse(html) for result in results: fout.write(str(result)+'\\n') print(threading.current_thread().name, f'results.size', len(results), 'html_queue.size=', html_queue.qsize()) time.sleep(random.randint(1,2))if __name__ == \"__main__\": pass url_queue = queue.Queue() html_queue = queue.Queue() for index in range(1,15): url_queue.put(index) for thread_num in range(3): # 生产者线程 t = threading.Thread(target=do_crawl, args=(url_queue, html_queue), name=f'crawl&#123;thread_num&#125;') t.start() fout = open('data.txt', 'w') # 文件输出 for thread_num in range(2): # 消费者线程 t = threading.Thread(target=do_parse, args=(html_queue, fout), name=f'parse&#123;thread_num&#125;') t.start() 结果写到 data.txt中 线程安全….. 参考 【2021最新版】Python 并发编程实战，用多线程、多进程、多协程加速程序运行","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"并发","slug":"并发","permalink":"http://yoursite.com/tags/并发/"}],"author":"Dufy"},{"title":"⭐️趣代码","slug":"趣代码","date":"2021-03-07T16:00:00.000Z","updated":"2021-05-10T15:36:21.169Z","comments":true,"path":"2021/03/08/趣代码/","link":"","permalink":"http://yoursite.com/2021/03/08/趣代码/","excerpt":"","text":"python 小猪佩奇 说明：turtle是Python内置的一个非常有趣的模块，特别适合对计算机程序设计进行初体验的小伙伴，它最早是Logo语言的一部分，Logo语言是Wally Feurzig和Seymour Papert在1966发明的编程语言。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268# coding:utf-8import turtle as tt.pensize(4) # 设置画笔的大小t.colormode(255) # 设置GBK颜色范围为0-255t.color((255,155,192),\"pink\") # 设置画笔颜色和填充颜色(pink)t.setup(840,500) # 设置主窗口的大小为840*500t.speed(10) # 设置画笔速度为10#鼻子t.pu() # 提笔t.goto(-100,100) # 画笔前往坐标(-100,100)t.pd() # 下笔t.seth(-30) # 笔的角度为-30°t.begin_fill() # 外形填充的开始标志a=0.4for i in range(120): if 0&lt;=i&lt;30 or 60&lt;=i&lt;90: a=a+0.08 t.lt(3) #向左转3度 t.fd(a) #向前走a的步长 else: a=a-0.08 t.lt(3) t.fd(a)t.end_fill() # 依据轮廓填充t.pu() # 提笔t.seth(90) # 笔的角度为90度t.fd(25) # 向前移动25t.seth(0) # 转换画笔的角度为0t.fd(10)t.pd()t.pencolor(255,155,192) # 设置画笔颜色t.seth(10)t.begin_fill()t.circle(5) # 画一个半径为5的圆t.color(160,82,45) # 设置画笔和填充颜色t.end_fill()t.pu()t.seth(0)t.fd(20)t.pd()t.pencolor(255,155,192)t.seth(10)t.begin_fill()t.circle(5)t.color(160,82,45)t.end_fill()#头t.color((255,155,192),\"pink\")t.pu()t.seth(90)t.fd(41)t.seth(0)t.fd(0)t.pd()t.begin_fill()t.seth(180)t.circle(300,-30) # 顺时针画一个半径为300,圆心角为30°的园t.circle(100,-60)t.circle(80,-100)t.circle(150,-20)t.circle(60,-95)t.seth(161)t.circle(-300,15)t.pu()t.goto(-100,100)t.pd()t.seth(-30)a=0.4for i in range(60): if 0&lt;=i&lt;30 or 60&lt;=i&lt;90: a=a+0.08 t.lt(3) #向左转3度 t.fd(a) #向前走a的步长 else: a=a-0.08 t.lt(3) t.fd(a)t.end_fill()#耳朵t.color((255,155,192),\"pink\")t.pu()t.seth(90)t.fd(-7)t.seth(0)t.fd(70)t.pd()t.begin_fill()t.seth(100)t.circle(-50,50)t.circle(-10,120)t.circle(-50,54)t.end_fill()t.pu()t.seth(90)t.fd(-12)t.seth(0)t.fd(30)t.pd()t.begin_fill()t.seth(100)t.circle(-50,50)t.circle(-10,120)t.circle(-50,56)t.end_fill()#眼睛t.color((255,155,192),\"white\")t.pu()t.seth(90)t.fd(-20)t.seth(0)t.fd(-95)t.pd()t.begin_fill()t.circle(15)t.end_fill()t.color(\"black\")t.pu()t.seth(90)t.fd(12)t.seth(0)t.fd(-3)t.pd()t.begin_fill()t.circle(3)t.end_fill()t.color((255,155,192),\"white\")t.pu()t.seth(90)t.fd(-25)t.seth(0)t.fd(40)t.pd()t.begin_fill()t.circle(15)t.end_fill()t.color(\"black\")t.pu()t.seth(90)t.fd(12)t.seth(0)t.fd(-3)t.pd()t.begin_fill()t.circle(3)t.end_fill()#腮t.color((255,155,192))t.pu()t.seth(90)t.fd(-95)t.seth(0)t.fd(65)t.pd()t.begin_fill()t.circle(30)t.end_fill()#嘴t.color(239,69,19)t.pu()t.seth(90)t.fd(15)t.seth(0)t.fd(-100)t.pd()t.seth(-80)t.circle(30,40)t.circle(40,80)#身体t.color(\"red\",(255,99,71))t.pu()t.seth(90)t.fd(-20)t.seth(0)t.fd(-78)t.pd()t.begin_fill()t.seth(-130)t.circle(100,10)t.circle(300,30)t.seth(0)t.fd(230)t.seth(90)t.circle(300,30)t.circle(100,3)t.color((255,155,192),(255,100,100))t.seth(-135)t.circle(-80,63)t.circle(-150,24)t.end_fill()#手t.color((255,155,192))t.pu()t.seth(90)t.fd(-40)t.seth(0)t.fd(-27)t.pd()t.seth(-160)t.circle(300,15)t.pu()t.seth(90)t.fd(15)t.seth(0)t.fd(0)t.pd()t.seth(-10)t.circle(-20,90)t.pu()t.seth(90)t.fd(30)t.seth(0)t.fd(237)t.pd()t.seth(-20)t.circle(-300,15)t.pu()t.seth(90)t.fd(20)t.seth(0)t.fd(0)t.pd()t.seth(-170)t.circle(20,90)#脚t.pensize(10)t.color((240,128,128))t.pu()t.seth(90)t.fd(-75)t.seth(0)t.fd(-180)t.pd()t.seth(-90)t.fd(40)t.seth(-180)t.color(\"black\")t.pensize(15)t.fd(20)t.pensize(10)t.color((240,128,128))t.pu()t.seth(90)t.fd(40)t.seth(0)t.fd(90)t.pd()t.seth(-90)t.fd(40)t.seth(-180)t.color(\"black\")t.pensize(15)t.fd(20)#尾巴t.pensize(4)t.color((255,155,192))t.pu()t.seth(90)t.fd(70)t.seth(0)t.fd(95)t.pd()t.seth(0)t.circle(70,20)t.circle(10,330)t.circle(70,30)t.mainloop() 跑马灯在屏幕上显示跑马灯文字 1234567891011121314151617import osimport timedef main(): content = '北京欢迎你为你开天辟地…………' while True: # 清理屏幕上的输出 os.system('cls') # os.system('clear') print(content) # 休眠200毫秒 time.sleep(0.2) content = content[1:] + content[0]if __name__ == '__main__': main() 123456789101112131415北京欢迎你为你开天辟地…………京欢迎你为你开天辟地…………北欢迎你为你开天辟地…………北京迎你为你开天辟地…………北京欢你为你开天辟地…………北京欢迎为你开天辟地…………北京欢迎你你开天辟地…………北京欢迎你为开天辟地…………北京欢迎你为你天辟地…………北京欢迎你为你开辟地…………北京欢迎你为你开天地…………北京欢迎你为你开天辟…………北京欢迎你为你开天辟地………北京欢迎你为你开天辟地………北京欢迎你为你开天辟地………北京欢迎你为你开天辟地……… 产生验证码12345678910111213141516171819import randomdef generate_code(code_len=4): \"\"\" 生成指定长度的验证码 :param code_len: 验证码的长度(默认4个字符) :return: 由大小写英文字母和数字构成的随机验证码 \"\"\" all_chars = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' last_pos = len(all_chars) - 1 code = '' for _ in range(code_len): index = random.randint(0, last_pos) code += all_chars[index] return codefor i in range(10): print(generate_code()) 12345678910o4epKSYNMye1kP8d87X8rpAHVC6yfOeQ7yl9TOCP 走迷宫https://www.cnblogs.com/Radium1209/p/10415341.html 出自【1】 参考 参考1🔼","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[],"author":"Dufy"},{"title":"⭐️影单","slug":"影单","date":"2021-02-28T16:00:00.000Z","updated":"2021-03-30T14:35:08.463Z","comments":true,"path":"2021/03/01/影单/","link":"","permalink":"http://yoursite.com/2021/03/01/影单/","excerpt":"","text":"这6部电影，让你和孩子宅着也能感受春天况味 《我的章鱼老师》 出自【1】 参考 参考1🔼","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[{"name":"电影","slug":"电影","permalink":"http://yoursite.com/tags/电影/"}],"author":"Dufy"},{"title":"⭐️阅读摘要","slug":"阅读摘要","date":"2021-02-26T16:00:00.000Z","updated":"2021-05-13T14:51:53.701Z","comments":true,"path":"2021/02/27/阅读摘要/","link":"","permalink":"http://yoursite.com/2021/02/27/阅读摘要/","excerpt":"","text":"谈职业 很多人建议，寻找人生方向时，你应该听从自己的内心，寻找真正热爱的事情。我现在觉得，更现实的建议应该是，寻找你愿意忍受的痛苦。 你在哪一个方向上，愿意心甘情愿地、经年累月地吃苦，具有最大的忍耐，“虽九死其尤未悔”，那就是你应该选择的方向。 你能在某件事上赢过别人，原因很可能不是你比他强，也不是你比他更热爱这个事业，而是你比他更抗打击。生活虐你千百遍，等到别人都放弃的时候，你还没有放弃。 科技爱好者周刊（第 147 期）：寻找你愿意忍受的痛苦 技术发展 在计算机图形渲染领域，光线追踪技术往往被视为该领域的「圣杯」，因为它可以为虚拟世界带来接近现实的光影效果。 而人工心脏，同样也是医疗界的圣杯之一。毕竟人造器官本就是一件看似不可能的事情，而心脏，更是人体最重要的部分。 另外对于想在计算机行业长期发展的朋友来说，夯实计算机体系基础知识其实是非常非常重要的。 基础往往决定了上层建筑，楼房再高，地基不稳最后也白搭。这些基础知识包括：算法、计算机网络、计算机组成原理、计算机编译原理、操作系统、数据库等等。 思维 “找到解决方案非常容易，最难的是找到要解决什么问题。”——印证了爱因斯坦“了解了问题，解决方案就会浮现”的智慧，这一点在今天极为重要，因为我们生活在问题不是那么明显而解决方案并不那么复杂的时代。 旅行不是为了看到不同的事物，而是为了学会不同地看待事物。 （You don’t travel to see different things, you travel to see things differently.） – Ben Davenpor，风险投资家 面向对象编程的哲学思想是，通过对语言建模来适应问题，而不是对问题建模来适应语言。 – 《C prime plus》教材 周孝正：中国社会问题分析（完整版） 商业 有一种普遍的误解，认为免费提供产品就无法产生任何收入。事实上，免费有时候不是定价策略，而是一种定位策略，尝试通过免费吸引用户，然后将产品定位在正确的受众群体面前。等有了用户以后，出售增值功能自然就成为最合适的扩展策略。 – 《免费增值是产品启动的有效方式》 有没有想过比特币（和其他加密货币）是如何工作的？ 出自【1】 参考 参考1🔼","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[{"name":"阅读","slug":"阅读","permalink":"http://yoursite.com/tags/阅读/"}],"author":"Dufy"},{"title":"⭐️搜索推荐","slug":"搜索推荐","date":"2021-01-15T16:00:00.000Z","updated":"2021-02-28T14:29:17.932Z","comments":true,"path":"2021/01/16/搜索推荐/","link":"","permalink":"http://yoursite.com/2021/01/16/搜索推荐/","excerpt":"这是摘要……","text":"这是摘要…… 小练习 1) Beginner Tutorial: Recommender Systems in Python 倒排索引 隐含语义分析 用隐藏语义分析（LSA）进行主题建模（附Python代码） 奇异值分解与LSA潜在语义分析 注意，TS矩阵的每一行，可以当做某一个特定的单词；DS矩阵的每一行，可以当做文档向量 e.g. 只取前两维，k=2： 出自【1】 参考 参考1🔼","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"}],"tags":[{"name":"搜索/推荐","slug":"搜索-推荐","permalink":"http://yoursite.com/tags/搜索-推荐/"}],"author":"Dufy"},{"title":"⭐️MySQL","slug":"MySQL","date":"2020-12-04T16:00:00.000Z","updated":"2021-02-02T15:13:36.809Z","comments":true,"path":"2020/12/05/MySQL/","link":"","permalink":"http://yoursite.com/2020/12/05/MySQL/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 安装CentOS 7.0下使用yum安装MySQL 注意，服务器链接失败，可能需要关闭防火墙，DataGrip无法连接阿里云服务器MySQL原因 查询 left join 123456select a.id, a.date, b.pname, b.category_id, b.price from t_order aleft join t_product b on a.pid = b.id 出自【1】 参考 参考1🔼 菜鸟–SQL语法手册","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}],"author":"Dufy"},{"title":"⭐️labuladong文章标题及链接","slug":"labuladong文章标题及链接","date":"2020-11-12T16:00:00.000Z","updated":"2021-02-27T06:33:36.846Z","comments":true,"path":"2020/11/13/labuladong文章标题及链接/","link":"","permalink":"http://yoursite.com/2020/11/13/labuladong文章标题及链接/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 数据结构和算法学习指南 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484852&amp;idx=1&amp;sn=85b50b8b0470bb4897e517955f4e5002&amp;chksm=9bd7fbbcaca072aa75e2a241064a403fde1e579d57ab846cd8537a54253ceb2c8b93cc3bf38e&amp;scene=21#wechat_redirect 动态规划解题框架 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484731&amp;idx=1&amp;sn=f1db6dee2c8e70c42240aead9fd224e6&amp;chksm=9bd7fb33aca07225bee0b23a911c30295e0b90f393af75eca377caa4598ffb203549e1768336&amp;scene=21#wechat_redirect 回溯算法解题框架 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484709&amp;idx=1&amp;sn=1c24a5c41a5a255000532e83f38f2ce4&amp;chksm=9bd7fb2daca0723be888b30345e2c5e64649fc31a00b05c27a0843f349e2dd9363338d0dac61&amp;scene=21#wechat_redirect BFS算法框架套路详解 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485134&amp;idx=1&amp;sn=fd345f8a93dc4444bcc65c57bb46fc35&amp;chksm=9bd7f8c6aca071d04c4d383f96f2b567ad44dc3e67d1c3926ec92d6a3bcc3273de138b36a0d9&amp;scene=21#wechat_redirect 我作了首诗，保你闭着眼睛也能写对二分查找 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485044&amp;idx=1&amp;sn=e6b95782141c17abe206bfe2323a4226&amp;chksm=9bd7f87caca0716aa5add0ddddce0bfe06f1f878aafb35113644ebf0cf0bfe51659da1c1b733&amp;scene=21#wechat_redirect 我写了套框架，把滑动窗口算法变成了默写题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485141&amp;idx=1&amp;sn=0e4583ad935e76e9a3f6793792e60734&amp;chksm=9bd7f8ddaca071cbb7570b2433290e5e2628d20473022a5517271de6d6e50783961bebc3dd3b&amp;scene=21#wechat_redirect 经典动态规划：0-1 背包问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485064&amp;idx=1&amp;sn=550705eb67f5e71487c8b218382919d6&amp;chksm=9bd7f880aca071962a5a17d0f85d979d6f0c5a5ce32c84b8fee88e36d451f9ccb3bb47b88f78&amp;scene=21#wechat_redirect 经典动态规划：高楼扔鸡蛋 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484675&amp;idx=1&amp;sn=4a4ac1c0f1279530b42fedacc6cca6e6&amp;chksm=9bd7fb0baca0721dda1eaa1d00b9a520672dc9d5c3be762eeca869be35d7ce232922ba8e928b&amp;scene=21#wechat_redirect 动态规划之 KMP 算法详解 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484731&amp;idx=2&amp;sn=d9d6b24c7f94d5e43e08666e82251984&amp;chksm=9bd7fb33aca0722548580dd27eb49880dc126ef87aeefedc33aa0f754f54691af6b09b41f45f&amp;scene=21#wechat_redirect 学习数据结构和算法看什么书 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484546&amp;idx=1&amp;sn=333c75230fec484d7473e1223581780d&amp;chksm=9bd7fa8aaca0739ccce48c1c1484b64795c0e36e7b47db4fb0477667080d25d738f8bc04fefd&amp;scene=21#wechat_redirect cookie 和 session 到底是什么 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484943&amp;idx=1&amp;sn=2db6af0e47cd6bdf185b0b440cf31da5&amp;chksm=9bd7f807aca0711183c4fe1f7b340a5f3399fc5517b77ec702f857818b0f3e000525363eda0a&amp;scene=21#wechat_redirect 一起刷题学习 Git/SQL/正则表达式 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484977&amp;idx=1&amp;sn=2c79a96aa3caf6acba22f8c0c114d676&amp;chksm=9bd7f839aca0712f044a11e1c582cd5d413bb676de3f889dfc73dcd0be473d0756d98456dfde&amp;scene=21#wechat_redirect Linux 进程、线程、文件描述符的底层原理 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484887&amp;idx=1&amp;sn=08860c04f6e79f363f4414a2c605b296&amp;chksm=9bd7fbdfaca072c961e057f204b82eaa168d32b7b26578bacd902bfccc12a3004e594fdd4b3d&amp;scene=21#wechat_redirect 算法学习之路 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484519&amp;idx=1&amp;sn=1c9a1f241b4794ad246a05640d603c27&amp;chksm=9bd7fa6faca073792f2589857a187001e1a32e2d46f3b20e7646eacf2896e0d223aebecfc72d&amp;scene=21#wechat_redirect 动态规划详解 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484524&amp;idx=1&amp;sn=302941466dbf594709b5436a59f8b06c&amp;chksm=9bd7fa64aca07372a8da4b9c4a5f1bab33ee0a93643bf8529c73f43c06b5e3adbbd42877775e&amp;scene=21#wechat_redirect 动态规划详解（修订版） http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484731&amp;idx=1&amp;sn=f1db6dee2c8e70c42240aead9fd224e6&amp;chksm=9bd7fb33aca07225bee0b23a911c30295e0b90f393af75eca377caa4598ffb203549e1768336&amp;scene=21#wechat_redirect 动态规划答疑篇 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484832&amp;idx=1&amp;sn=44ad2505ac5c276bf36eea1c503b78c3&amp;chksm=9bd7fba8aca072be32f66e6c39d76ef4e91bdbf4ef993014d4fee82896687ad61da4f4fc4eda&amp;scene=21#wechat_redirect 经典动态规划：0-1 背包问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485064&amp;idx=1&amp;sn=550705eb67f5e71487c8b218382919d6&amp;chksm=9bd7f880aca071962a5a17d0f85d979d6f0c5a5ce32c84b8fee88e36d451f9ccb3bb47b88f78&amp;scene=21#wechat_redirect 经典动态规划：0-1背包问题的变体 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485103&amp;idx=1&amp;sn=8a9752e18ed528e5c18d973dcd134260&amp;chksm=9bd7f8a7aca071b14c736a30ef7b23b80914c676414b01f8269808ef28da48eb13e90a432fff&amp;scene=21#wechat_redirect 经典动态规划：完全背包问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485124&amp;idx=1&amp;sn=52068c8000b90a7a972dbd04658d79b7&amp;chksm=9bd7f8ccaca071da66d3c9e567ab49b27c711db154c2f297f55fcd7c3c1156afa37b0ad60555&amp;scene=21#wechat_redirect 详解一道腾讯面试题：编辑距离 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484484&amp;idx=1&amp;sn=74594297022c84952162a68b7f739133&amp;chksm=9bd7fa4caca0735a1364dd13901311ecd6ec4913c8db05a1ff6cae8f069627eebe8d651bbeb1&amp;scene=21#wechat_redirect 经典动态规划：高楼扔鸡蛋 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484675&amp;idx=1&amp;sn=4a4ac1c0f1279530b42fedacc6cca6e6&amp;chksm=9bd7fb0baca0721dda1eaa1d00b9a520672dc9d5c3be762eeca869be35d7ce232922ba8e928b&amp;scene=21#wechat_redirect 经典动态规划：高楼扔鸡蛋（进阶篇） http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484690&amp;idx=1&amp;sn=eea075701a5d96dd5c6e3dc6a993cac5&amp;chksm=9bd7fb1aaca0720c58c9d9e02a8b9211a289bcea359633a95886d7808d2846898d489ce98078&amp;scene=21#wechat_redirect 团灭 LeetCode 打家劫舍系列问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484800&amp;idx=1&amp;sn=1016975b9e8df0b8f6df996a5fded0af&amp;chksm=9bd7fb88aca0729eb2d450cca8111abd8f861236b04125ce556171cb520e298ddec4d90823b3&amp;scene=21#wechat_redirect 团灭 LeetCode 股票买卖问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484508&amp;idx=1&amp;sn=42cae6e7c5ccab1f156a83ea65b00b78&amp;chksm=9bd7fa54aca07342d12ae149dac3dfa76dc42bcdd55df2c71e78f92dedbbcbdb36dec56ac13b&amp;scene=21#wechat_redirect 子序列解题模板：最长回文子序列 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484666&amp;idx=1&amp;sn=e3305be9513eaa16f7f1568c0892a468&amp;chksm=9bd7faf2aca073e4f08332a706b7c10af877fee3993aac4dae86d05783d3d0df31844287104e&amp;scene=21#wechat_redirect 贪心算法之区间调度问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484493&amp;idx=1&amp;sn=1615b8a875b770f25875dab54b7f0f6f&amp;chksm=9bd7fa45aca07353a347b7267aaab78b81502cf7eb60d0510ca9109d3b9c0a1d9dda10d99f50&amp;scene=21#wechat_redirect 动态规划之博弈问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484496&amp;idx=1&amp;sn=d04bb89cb1df241993c6b46ffcabae7e&amp;chksm=9bd7fa58aca0734e7241b3459c6775f9ae30d16cb8d30b5ad897600a87c69fd77d7284742043&amp;scene=21#wechat_redirect 动态规划之正则表达式 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484513&amp;idx=1&amp;sn=e5fc3cce76c1b916195e1793122c28b8&amp;chksm=9bd7fa69aca0737fe704ea5c6da28f47b9e3f0961df2eb40ef93a7d507ace8def1a18d013515&amp;scene=21#wechat_redirect 动态规划设计之最长递增子序列 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484498&amp;idx=1&amp;sn=df58ef249c457dd50ea632f7c2e6e761&amp;chksm=9bd7fa5aaca0734c29bcf7979146359f63f521e3060c2acbf57a4992c887aeebe2a9e4bd8a89&amp;scene=21#wechat_redirect 最长递增子序列之信封嵌套问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484494&amp;idx=1&amp;sn=0e90d7fbf812fd1f4c408b5cc5fdf8c6&amp;chksm=9bd7fa46aca07350f626e2365d9f71545aa304725b7122b5a44bcfd90cf0506c9036201f3b38&amp;scene=21#wechat_redirect 动态规划：不同的定义产生不同的解法 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484469&amp;idx=1&amp;sn=e8d321c8ad62483874a997e9dd72da8f&amp;chksm=9bd7fa3daca0732b316aa0afa58e70357e1cb7ab1fe0855d06bc4a852abb1b434c01c7dd19d6&amp;scene=21#wechat_redirect Union-Find 并查集算法详解 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484751&amp;idx=1&amp;sn=a873c1f51d601bac17f5078c408cc3f6&amp;chksm=9bd7fb47aca07251dd9146e745b4cc5cfdbc527abe93767691732dfba166dfc02fbb7237ddbf&amp;scene=21#wechat_redirect Union-Find 算法怎么应用？ http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484759&amp;idx=1&amp;sn=a88337164c741b9740e50523b41b7659&amp;chksm=9bd7fb5faca07249c15e925e596e8ab071731f0c996b1ba3e58a1b45052900a23278114f2720&amp;scene=21#wechat_redirect 经典面试题：最长回文子串 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484471&amp;idx=1&amp;sn=7c26d04a1f035770920d31377a1ebd42&amp;chksm=9bd7fa3faca07329189e9e8b51e1a665166946b66b8e8978299ba96d5f2c0d3eafa7db08b681&amp;scene=21#wechat_redirect 如何高效判断回文单链表？ http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484822&amp;idx=1&amp;sn=44742c9a3557038c8da7150100d94db9&amp;chksm=9bd7fb9eaca0728876e1146306a09f5453bcd5c35c4a264304ea6189faa83ec12a00322f0246&amp;scene=21#wechat_redirect 谁能想到，求最值的算法还能优化？ http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484640&amp;idx=1&amp;sn=8aec35c1416cee732d4aa1a95696a57d&amp;chksm=9bd7fae8aca073fe06c87ff650fdf300b86bbbec5b38402c99ab2533d8a66dd2c4d27a2c2d0f&amp;scene=21#wechat_redirect 二分搜索只能用来查找元素吗？ http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484598&amp;idx=1&amp;sn=69edaf4a7f6bfd0b1185cae5d0689c1d&amp;chksm=9bd7fabeaca073a8820bc93cb67a8e26fa9eaa1ab9717b7e3ac41b4aac12235067c8af3520d5&amp;scene=21#wechat_redirect 递归思想：用锅铲给烧饼排序 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484611&amp;idx=1&amp;sn=8c3b6c986830f4a801e9e237d9e1554d&amp;chksm=9bd7facbaca073ddc8158b78a29b96820993c446de56c013f274a6ee265cd98c47006ec0a23f&amp;scene=21#wechat_redirect 详解一道腾讯面试题：编辑距离 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484484&amp;idx=1&amp;sn=74594297022c84952162a68b7f739133&amp;chksm=9bd7fa4caca0735a1364dd13901311ecd6ec4913c8db05a1ff6cae8f069627eebe8d651bbeb1&amp;scene=21#wechat_redirect 如何实现 LRU 缓存机制 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484500&amp;idx=1&amp;sn=83f4df1253f597898b2f74ea9dca9fd9&amp;chksm=9bd7fa5caca0734ad182ba67651882647a71264938eaa98e49c5ff43369b807a094ad16efcd4&amp;scene=21#wechat_redirect 如何用算法高效寻找素数 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484472&amp;idx=1&amp;sn=ab8e97d0211de37bf6770a63caacc630&amp;chksm=9bd7fa30aca07326c4807b04141c57d3673eae92f878c4e56f89ff25faa0c06810a8f650379b&amp;scene=21#wechat_redirect 前缀和技巧： http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484488&amp;idx=1&amp;sn=848f76e86fce722e70e265d0c6f84dc3&amp;chksm=9bd7fa40aca07356a6f16db72f5a56529044b1bdb2dcce2de4efe59e0338f0c313de682aef29&amp;scene=21#wechat_redirect 解决子数组问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484488&amp;idx=1&amp;sn=848f76e86fce722e70e265d0c6f84dc3&amp;chksm=9bd7fa40aca07356a6f16db72f5a56529044b1bdb2dcce2de4efe59e0338f0c313de682aef29&amp;scene=21#wechat_redirect 经典面试题： http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484486&amp;idx=1&amp;sn=0bdcb94c6390307ea32427757ec0072c&amp;chksm=9bd7fa4eaca073583623cdb93b05dc9e1d0757b25697bb40b29b3e450124e929ff1a8eaac50f&amp;scene=21#wechat_redirect 最长公共子序列 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484486&amp;idx=1&amp;sn=0bdcb94c6390307ea32427757ec0072c&amp;chksm=9bd7fa4eaca073583623cdb93b05dc9e1d0757b25697bb40b29b3e450124e929ff1a8eaac50f&amp;scene=21#wechat_redirect 详解一道高频面试题： http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484482&amp;idx=1&amp;sn=9503dae2ec50bc8aa2ba96af11ea3311&amp;chksm=9bd7fa4aaca0735c37ab72b40f5594def9f4f8ad76df4450be0c8ae1cdc2e0105a3fce1bc502&amp;scene=21#wechat_redirect 接雨水 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484482&amp;idx=1&amp;sn=9503dae2ec50bc8aa2ba96af11ea3311&amp;chksm=9bd7fa4aaca0735c37ab72b40f5594def9f4f8ad76df4450be0c8ae1cdc2e0105a3fce1bc502&amp;scene=21#wechat_redirect Two Sum 问题的核心思想 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484474&amp;idx=1&amp;sn=dfbadbe6e17d695a1907e2adcd9f0d3c&amp;chksm=9bd7fa32aca0732406829a6d1de34b7e3144af239cc25e014f5349d73cea952d5f2b0473345a&amp;scene=21#wechat_redirect 经典贪心算法：跳跃游戏 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485087&amp;idx=1&amp;sn=ddbed992e5ad8f1aa3b3d4afcb17889b&amp;chksm=9bd7f897aca071817d3ea77acf4a8bc8e277bd38a43ebe2ceba2b42c3184886e07775a628fc7&amp;scene=21#wechat_redirect 贪心算法之区间调度问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484493&amp;idx=1&amp;sn=1615b8a875b770f25875dab54b7f0f6f&amp;chksm=9bd7fa45aca07353a347b7267aaab78b81502cf7eb60d0510ca9109d3b9c0a1d9dda10d99f50&amp;scene=21#wechat_redirect 区间问题之合并相交区间 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484492&amp;idx=1&amp;sn=578d4bf538908b8042ed38ee92405455&amp;chksm=9bd7fa44aca0735285c85f7418bddccf9cc9184a756c921c89faf501f32eeef79234dd787f4f&amp;scene=21#wechat_redirect 用算法优雅地求出两组区间的交集 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484491&amp;idx=1&amp;sn=031fde544d0d9f651cde8abf7ebe7f09&amp;chksm=9bd7fa43aca0735594c98543ee8591c3a3170b7874250a2d7368d5edabc3b15fca873ee98412&amp;scene=21#wechat_redirect 游戏中的敏感词过滤是如何实现的 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484473&amp;idx=2&amp;sn=52e0ff882932103280966d277aabf86a&amp;chksm=9bd7fa31aca07327ce026c2d4a89089769563200cf948e4484e3558f45355aca833945469040&amp;scene=21#wechat_redirect 随机算法之水塘抽样算法 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484974&amp;idx=1&amp;sn=795a33c338d4a5bd8d265bc7f9f63c03&amp;chksm=9bd7f826aca07130e303d3d6f5c901b8aa00f9c3d02ffc26d45b56f1d36b538990c9eebd06a8&amp;scene=21#wechat_redirect 教计算机做算术：字符串乘法 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484466&amp;idx=1&amp;sn=0281340cc1f41230e4512e905b9d27dd&amp;chksm=9bd7fa3aaca0732c95d25c637d42ad8d9b80f8165098ded837f83791c673b5d6a71721c738a3&amp;scene=21#wechat_redirect 这个问题不简单：寻找缺失元素 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484477&amp;idx=1&amp;sn=13834cfd618377385226c3dc598b2c28&amp;chksm=9bd7fa35aca0732374dc34c78c276b982605892caf69cb31ad9a6c3685de5dbccac81989b195&amp;scene=21#wechat_redirect 高效寻找缺失和重复的数字 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485050&amp;idx=1&amp;sn=dac757454b2df9a1291f1e8027f56c1b&amp;chksm=9bd7f872aca07164da3e1138df630d63b41e4f071f71194069bcedeef866c1b91650a77be3d2&amp;scene=21#wechat_redirect 一行代码就能解决的智力题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484497&amp;idx=1&amp;sn=8273da4b2cbdb9d8582bd7533713cb6c&amp;chksm=9bd7fa59aca0734ffcc95e4db4b9a4a4b5fd6e9341fb2d5be93cc7eda871ead83327072ff5e2&amp;scene=21#wechat_redirect 三个反直觉的概率问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484502&amp;idx=1&amp;sn=208119a72a3183a4456ce11efae81aff&amp;chksm=9bd7fa5eaca073487bc0998cff0bdd96b6a28d539bb8e4d1bd1452b053f1d7b2a2084af5c8a8&amp;scene=21#wechat_redirect 一些有趣有用的位操作 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484501&amp;idx=1&amp;sn=af4cbfd46c68be78b2ab5909e3389b8f&amp;chksm=9bd7fa5daca0734b1320f719332dd58f374a0f3363634e8876783f45e5596bb5fca14653d2b0&amp;scene=21#wechat_redirect 区间问题之合并相交区间 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484492&amp;idx=1&amp;sn=578d4bf538908b8042ed38ee92405455&amp;chksm=9bd7fa44aca0735285c85f7418bddccf9cc9184a756c921c89faf501f32eeef79234dd787f4f&amp;scene=21#wechat_redirect 用算法优雅地求出两组区间的交集 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484491&amp;idx=1&amp;sn=031fde544d0d9f651cde8abf7ebe7f09&amp;chksm=9bd7fa43aca0735594c98543ee8591c3a3170b7874250a2d7368d5edabc3b15fca873ee98412&amp;scene=21#wechat_redirect 洗牌算法详解： http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484503&amp;idx=1&amp;sn=e30ef74eb16ad385c16681cd6dfe15cf&amp;chksm=9bd7fa5faca07349c6877bc69f9a27e13585f2c5ed2237ad37ac5b272611039391acc1dcd33d&amp;scene=21#wechat_redirect 你会排序，但你会打乱吗 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484503&amp;idx=1&amp;sn=e30ef74eb16ad385c16681cd6dfe15cf&amp;chksm=9bd7fa5faca07349c6877bc69f9a27e13585f2c5ed2237ad37ac5b272611039391acc1dcd33d&amp;scene=21#wechat_redirect Flood Fill 算法详解 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484514&amp;idx=1&amp;sn=cc0531313d992eb68855457ec5ea08f8&amp;chksm=9bd7fa6aaca0737cf05e708ffd3253b327b665410ba93f3cf8ea5dc7f5c1bb1fef7104acb18f&amp;scene=21#wechat_redirect 交换数组的两部分真的很简单吗 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484526&amp;idx=1&amp;sn=ac8ef897b917321ee74b50eacb03c16e&amp;chksm=9bd7fa66aca0737056ae49bec67aa5147d2d3b84f9047e43cc4a38811c557d3cc060bc39aaeb&amp;scene=21#wechat_redirect 如何拆解复杂问题：实现一个计算器 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484903&amp;idx=1&amp;sn=184beaad36a71c9a8dd93c41a8ba74ac&amp;chksm=9bd7fbefaca072f9beccff92a715d92ee90f46c297277eec10c322bc5ccd053460da6afb76c2&amp;scene=21#wechat_redirect 如何高效判断回文单链表？ http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484822&amp;idx=1&amp;sn=44742c9a3557038c8da7150100d94db9&amp;chksm=9bd7fb9eaca0728876e1146306a09f5453bcd5c35c4a264304ea6189faa83ec12a00322f0246&amp;scene=21#wechat_redirect 3分钟看懂如何判断括号的合法性 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484470&amp;idx=1&amp;sn=599a41779689ea339147de3e6a68f493&amp;chksm=9bd7fa3eaca073288b59408173abd8bcf5404a821ad911fdf4493f1cb28136a545c7c474f90b&amp;scene=21#wechat_redirect 图文详解二叉堆，实现优先级队列 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484495&amp;idx=1&amp;sn=bbfeba9bb5cfd50598e2a4d08c839ee9&amp;chksm=9bd7fa47aca073512e094110a7fe7d9bac052be114d1db72fe07b7efa6beb915f51b3f19291e&amp;scene=21#wechat_redirect 递归反转链表：如何拆解复杂问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484467&amp;idx=1&amp;sn=beb3ae89993b812eeaa6bbdeda63c494&amp;chksm=9bd7fa3baca0732dc3f9ae9202ecaf5c925b4048514eeca6ac81bc340930a82fc62bb67681fa&amp;scene=21#wechat_redirect 递归思维：k 个一组反转链表 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484597&amp;idx=1&amp;sn=c603f1752e33cb2701e371d84254aee2&amp;chksm=9bd7fabdaca073abd512d8fff18016c9092ede45fed65c307852c65a2026d8568ee294563c78&amp;scene=21#wechat_redirect 设计 Tutter： http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484499&amp;idx=1&amp;sn=64f75d4bdbb4c5777ba199aae804d138&amp;chksm=9bd7fa5baca0734dc51f588af913140560b994e3811dac6a7fa8ccfc2a31aca327f1faf964c2&amp;scene=21#wechat_redirect 合并 k 个有序链表和面向对象设计 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484499&amp;idx=1&amp;sn=64f75d4bdbb4c5777ba199aae804d138&amp;chksm=9bd7fa5baca0734dc51f588af913140560b994e3811dac6a7fa8ccfc2a31aca327f1faf964c2&amp;scene=21#wechat_redirect 双指针技巧汇总 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484505&amp;idx=1&amp;sn=0e9517f7c4021df0e6146c6b2b0c4aba&amp;chksm=9bd7fa51aca07347009c591c403b3228f41617806429e738165bd58d60220bf8f15f92ff8a2e&amp;scene=21#wechat_redirect 滑动窗口算法解决子串问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484504&amp;idx=1&amp;sn=5ecbab87e42033cc0a62b635cc436977&amp;chksm=9bd7fa50aca07346a3ffa6be6fccc445968c162af9532fa9c6304eaab2e3a1b79a4bbe758c0a&amp;scene=21#wechat_redirect 递归实现链表操作 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484531&amp;idx=1&amp;sn=3a72d94271531b42c0fec60f89abd716&amp;chksm=9bd7fa7baca0736d5476e74bf2ca82edfcac3e8e7302af70ccd3cd253f70d0ec2b2e30115547&amp;scene=21#wechat_redirect 二叉搜索树操作集锦 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484518&amp;idx=1&amp;sn=f8ef8d7ce7959b4fd779e38f47419ac6&amp;chksm=9bd7fa6eaca073785cb6f808421241bcb641203c8ec7f30a9269a221b3d92c661334af1b75f5&amp;scene=21#wechat_redirect 如何高 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484478&amp;idx=1&amp;sn=685308e10c32ee5ad3508a5789633b3a&amp;chksm=9bd7fa36aca07320ecbae4a53ed44ff6acc95c69027aa917f5e10b93dedca86119e81c7bad26&amp;scene=21#wechat_redirect 序数组/链表去重？ http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484478&amp;idx=1&amp;sn=685308e10c32ee5ad3508a5789633b3a&amp;chksm=9bd7fa36aca07320ecbae4a53ed44ff6acc95c69027aa917f5e10b93dedca86119e81c7bad26&amp;scene=21#wechat_redirect http://mp.weixin.qq.com/s?__biz=MzU0MDg5OTYyOQ==&amp;mid=2247484512&amp;idx=1&amp;sn=d5ed6c145db1ebc7c061c7326dd603e5&amp;chksm=fb336422cc44ed34cb7236a918d3d2a861d29a30311541aa934bb6b3d84ad470264a3c3b7dcc&amp;scene=21#wechat_redirect 单调队列解决滑动窗口问题 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484506&amp;idx=1&amp;sn=fcaae7325b10905c808e085f8802b4eb&amp;chksm=9bd7fa52aca07344e72db849c7b40e9a3dac5275b94b50eb62ad75c51e73aafd92fa184960f3&amp;scene=21#wechat_redirect 单调栈 Monotonic Stack 的使用 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484525&amp;idx=1&amp;sn=3d2e63694607fec72455a52d9b15d4e5&amp;chksm=9bd7fa65aca073734df90b45054448e09c14e6e35ad7b778bff62f9bd6c2b4f6e1ca7bc4f844&amp;scene=21#wechat_redirect cookie 和 session 到底是什么 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484943&amp;idx=1&amp;sn=2db6af0e47cd6bdf185b0b440cf31da5&amp;chksm=9bd7f807aca0711183c4fe1f7b340a5f3399fc5517b77ec702f857818b0f3e000525363eda0a&amp;scene=21#wechat_redirect 一起刷题学习 Git/SQL/正则表达式 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484977&amp;idx=1&amp;sn=2c79a96aa3caf6acba22f8c0c114d676&amp;chksm=9bd7f839aca0712f044a11e1c582cd5d413bb676de3f889dfc73dcd0be473d0756d98456dfde&amp;scene=21#wechat_redirect Linux 进程、线程、文件描述符的底层原理 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484887&amp;idx=1&amp;sn=08860c04f6e79f363f4414a2c605b296&amp;chksm=9bd7fbdfaca072c961e057f204b82eaa168d32b7b26578bacd902bfccc12a3004e594fdd4b3d&amp;scene=21#wechat_redirect 关于 Linux shell 你必须知道的 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485000&amp;idx=1&amp;sn=6fcb162ca9dc85be23efb61db5f8a09d&amp;chksm=9bd7f840aca071568417b5f48517dfb9742da0025b2bae918306be6ccd72039d79a31f6a3c7a&amp;scene=21#wechat_redirect 学会这些 shell 小技巧，我就爱上 Linux 了 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247485072&amp;idx=1&amp;sn=f3af0d1b738f28565424217158f9911b&amp;chksm=9bd7f898aca0718e44bc6cb6cca5f49900e078909c1fa7f65ddf34b5ace9c26e49fab533e0bb&amp;scene=21#wechat_redirect 介绍 Linux 文件系统： http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484490&amp;idx=1&amp;sn=f313a6bc4f577de63b7c7a81eda0f343&amp;chksm=9bd7fa42aca0735409b97a1823e092240406652198c3536ddf3863d8eb325b93976670519a9f&amp;scene=21#wechat_redirect 这些目录都是什么鬼？ http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484490&amp;idx=1&amp;sn=f313a6bc4f577de63b7c7a81eda0f343&amp;chksm=9bd7fa42aca0735409b97a1823e092240406652198c3536ddf3863d8eb325b93976670519a9f&amp;scene=21#wechat_redirect http://mp.weixin.qq.com/s?__biz=MzU0MDg5OTYyOQ==&amp;mid=2247484383&amp;idx=1&amp;sn=1cc7104b4f8d76d9383d880b97ff6b7a&amp;chksm=fb33639dcc44ea8bb3407f79cc6af67541325df4ad0b07a60c8d518f8d35ab98182e23e39246&amp;scene=21#wechat_redirect x * x &gt;= 0 一定成立吗？ http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484510&amp;idx=1&amp;sn=05214c184ce7fc9602261bd41773e0dc&amp;chksm=9bd7fa56aca0734033cc8a7709cec5f74f60f2978be408efed4f15be1f45469c93f9a7fc5b68&amp;scene=21#wechat_redirect http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484510&amp;idx=1&amp;sn=05214c184ce7fc9602261bd41773e0dc&amp;chksm=9bd7fa56aca0734033cc8a7709cec5f74f60f2978be408efed4f15be1f45469c93f9a7fc5b68&amp;scene=21#wechat_redirect 计算机是如何通信的 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484481&amp;idx=2&amp;sn=42a550504e2cbee2f10f36d83e017f9d&amp;chksm=9bd7fa49aca0735f16d1602eb7ff08078a0efb9cc1672734bab7704c95b1e50acdaf9373cdba&amp;scene=21#wechat_redirect 收藏多年的 Chrome 插件 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484515&amp;idx=1&amp;sn=f1fd05c5cd8fa97586e9c004df2dfa9d&amp;chksm=9bd7fa6baca0737d057943ee5f873eba3bbd70a5eef74c77d7b4a38eb81d3c8ccba6a93b814a&amp;scene=21#wechat_redirect 学英语的一点经验总结 http://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&amp;mid=2247484537&amp;idx=1&amp;sn=32ac207236025d8dc53edfc14e27a13b&amp;chksm=9bd7fa71aca07367263a631be0868298499317cf54c81be2fdb57f68bd84ad104e7c04994b3e&amp;scene=21#wechat_redirect 树 二叉堆详解实现优先级队列 https://labuladong.gitbook.io/algo/shu-ju-jie-gou-xi-lie/2.4-shou-ba-shou-she-ji-shu-ju-jie-gou/er-cha-dui-xiang-jie-shi-xian-you-xian-ji-dui-lie 如何运用二分查找算法 https://labuladong.gitbook.io/algo/shu-ju-jie-gou-xi-lie/2.5-shou-ba-shou-shua-shu-zu-ti-mu/koko-tou-xiang-jiao 没看懂的 手把手带你刷二叉树（第三期）关于652.寻找重复的子树 https://labuladong.gitbook.io/algo/shu-ju-jie-gou-xi-lie/2.3-shou-ba-shou-shua-er-cha-shu-xun-lian-di-gui-si-wei/er-cha-shu-xi-lie-3 参考 参考1🔼","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"力扣","slug":"力扣","permalink":"http://yoursite.com/tags/力扣/"}],"author":"Dufy"},{"title":"⭐️Hadoop","slug":"Hadoop","date":"2020-11-07T16:00:00.000Z","updated":"2021-05-09T16:03:42.905Z","comments":true,"path":"2020/11/08/Hadoop/","link":"","permalink":"http://yoursite.com/2020/11/08/Hadoop/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 简介 深入浅出大数据：到底什么是Hadoop？ 出自【1】 Hadoop In 5 Minutes | What Is Hadoop? | Introduction To Hadoop | Hadoop Explained |Simplilearn 启动 zookeeper 1[root@master zookeeper-3.4.9]# bin/zkServer.sh start 对应进程： 15936 QuorumPeerMain hadoop 12[root@master hadoop-2.7.5]# sbin/start-dfs.shsbin/mr-jobhistory-daemon.sh start historyserver 15746 SecondaryNameNode15466 NameNode15936 QuorumPeerMain15565 DataNode15974 Jps 三个端口查看界面 http://node01:50070/explorer.html#/ 查看hdfs http://node01:8088/cluster 查看yarn集群 http://node01:19888/jobhistory 查看历史完成的任务 HDFS namenode 和datanode【视频p48】 HDFS 文件写入过程：【视频p57】 写入需要串行，一个个blk顺序写 hdfs读取过程 可以并行读 MapReduce计算框架【p75】 MapReduce运行在yarn集群 ResourceManager NodeManager Mapreduce 计算过程如下： 以统计词频为例： 一个完整的mapreduce程序在分布式运行时有三类实例进程: MRAppMaster 负责整个程序的过程调度及状态协调 MapTask 负责map阶段的整个数据处理流程 ReduceTask 负责reduce阶段的整个数据处理流程 MapReduce执行： 1hadoop jar hadoop_hdfs_operate-1.0-SNAPSHOT.jar cn.itcast.hdfs.demo1.JobMain ZookeeperHelloZooKeeper ZooKeeper 的中文教程，从最基础的安装使用到背后原理和源码，使用有趣诙谐的文字讲解。ZooKeeper 是大型分布式计算的配置服务工具。 学习网站 http://c.biancheng.net/view/3500.html 参考 B站https://www.bilibili.com/video/BV1ek4y117Yq?p=59","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"}],"author":"Dufy"},{"title":"⭐️简历思考","slug":"简历思考","date":"2020-07-24T16:00:00.000Z","updated":"2021-02-27T06:27:18.800Z","comments":true,"path":"2020/07/25/简历思考/","link":"","permalink":"http://yoursite.com/2020/07/25/简历思考/","excerpt":"[TOC] 参考：程序员的简历应该这么写！！（附简历模板）","text":"[TOC] 参考：程序员的简历应该这么写！！（附简历模板） CRF 与 HMM 区别概率图模型体系：HMM、MEMM、CRF 如何通俗地讲解 viterbi 算法？【强烈推荐！】 ner 项目 BiLSTM+CRF 输入、输出是什么？ 为何采用 BiLSTM+CRF 技术？ 具体网络架构如何？ ner 属于序列标注问题。 序列标注问题的重点在于学习序列位置之间的关系，然后解码出最大概率标签路径。 CRF 可以学到句子的约束条件 注意，这里的CRF不是完整版，是把CRF的一部分嫁接过来的—–参见【https://www.bilibili.com/video/BV1si4y1b7Yx/?p=5】 损失函数是什么？ 优化目标是：真实序列的概率最大化 损失函数如下：【论文：Neural Architectures for Named Entity Recognition】 对于每一个序列输出，定义得分S为： 从路径分数S可以看出，其包含两部分：转移分数和输出分数。输出分数高，整体分数不一定高，比如BB挨着这种，在CRF层就会对其有一个低的转移分数。最终模型也不会选择这种不合理的序列。这个得分函数S就很好地弥补了传统BiLSTM的不足 ——最通俗易懂的BiLSTM-CRF模型中的CRF层介绍 pytorch官方代码实现可以移步Bi-LSTM-CRF。 Simhash 为啥快？抽屉原理是怎么具体使用的？ 对simhash的简单理解 多传感器数据融合介绍输入、输出具体是？？ 卡尔曼滤波最完整公式推导 GoogleNet和Resnet差别 为什么Dropout有效？ Dropout背后理念和集成模型很相似。在Drpout层，不同的神经元组合被关闭，这代表了一种不同的结构，所有这些不同的结构使用一个的子数据集并行地带权重训练，而权重总和为1。如果Dropout层有 个神经元，那么会形成 个不同的子结构。在预测时，相当于集成这些模型并取均值。这种结构化的模型正则化技术有利于避免过拟合。Dropout有效的另外一个视点是：由于神经元是随机选择的，所以可以减少神经元之间的相互依赖，从而确保提取出相互独立的重要特征。 ResNet, AlexNet, VGG, Inception: 理解各种各样的CNN架构 池化操作的使用场景，什么时候不用？一定要用吗？ 超参数怎么调优参见《白面机器学习》p43 特征选取，关键特征如何挑选？ 主成分分析的原理 数据库的优化建索引，数据库故障排查 出自【1】 参考 参考1🔼","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[],"author":"Dufy"},{"title":"⭐️nlp笔记","slug":"nlp笔记","date":"2020-06-21T16:00:00.000Z","updated":"2021-04-26T16:54:03.459Z","comments":true,"path":"2020/06/22/nlp笔记/","link":"","permalink":"http://yoursite.com/2020/06/22/nlp笔记/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 大框架可以百度脑图查看，点击链接 出处：https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.%20NLP/README.md 链接：https://supcache.wyqrks.com/data/mantchs.com/img/mindmap.jpg 什么是nlp ？自然语言处理（Natural Language Processing，NLP）是一门集语言学，数学及计算机科学于一体的科学。它的核心目标就是把人的自然语言转换为计算机可以阅读的指令，简单来说就是让机器读懂人的语言。 NLP是人工智能领域一个非常重要的分支，其它重要分支包括计算机视觉，语音及机器学习和深度学习等。那么，NLP与机器学习，深度学习有什么关系呢？我们可以用下面的图来表示。可以看出，深度学习是机器学习的其中一个分支，而自然语言处理与机器学习之间是并行的，机器学习为自然语言处理提供了解决问题的许多模型和方法。所以，二者之间具有密不可分的关系。 另外的一种理解 https://easyai.tech/ai-definition/nlp/ 不同物种都有自己的沟通方式 自然语言处理（NLP）就是在机器语言和人类语言之间沟通的桥梁，以实现人机交流的目的。 NLP基础知识语言模型如何通俗易懂地让女朋友明白什么是语言模型？ 词嵌入概述我们要描述文章内容，首先要将文章内容表示出来。 一个比较直观的想法是采用 序号编码。 这对于具有大小关系的数据是有意义的，如表示‘低、中、高’三档次，可以用‘1、2、3’表示。转换后依然保留原先的大小关系。 低 1 中 2 高 3 但是，更多情况下，对于不具有大小关系的数据，该如何表示呢？ 可以考虑向量方法，如‘one-hot encoding’ 比如，词表有{‘水果’, ‘橘子’,’葡萄’,’汽车’, ‘大象’, ‘动物’}，共6个词 那么，这些词就可以采用如下方式表示： 水果 （1，0，0，0，0，0） 中 （0，1，0，0，0，0） 高 （0，0，0，0，1，0） 这样的问题是： 词表中词很多时候，向量维度会过大 词与词之间 的关系表示不出来，对于任意两个向量a,b，计算cos(a,b)==0 又引入了接下里的‘word2vec’模型 word2vec由谷歌于2013年提出，是一种浅层的神经网络模型，分为两种方法： CBOW—完形填空 Skip-gram—造句，跳字模型 TF-IDF衡量词在当前文档中的重要程度计算如下： 比如， 还是以《中国的蜜蜂养殖》为例，假定该文长度为1000个词，”中国”、”蜜蜂”、”养殖”各出现20次，则这三个词的”词频”（TF）都为0.02。然后，搜索Google发现，包含”的”字的网页共有250亿张，假定这就是中文网页总数。包含”中国”的网页共有62.3亿张，包含”蜜蜂”的网页为0.484亿张，包含”养殖”的网页为0.973亿张。则它们的逆文档频率（IDF）和TF-IDF如下： tf-idf(‘蜜蜂’)=0.02*lg_10(250/0.484)=0.02 x 2.713=0.0543 从上表可见，”蜜蜂”的TF-IDF值最高，”养殖”其次，”中国”最低。（如果还计算”的”字的TF-IDF，那将是一个极其接近0的值。）所以，如果只选择一个词，”蜜蜂”就是这篇文章的关键词。 应用场景【1】 自动提取关键词、信息检索、相似文章、自动摘要等 1) 自动提取关键词 2）信息检索 除了自动提取关键词，TF-IDF算法还可以用于许多别的地方。比如，信息检索时，对于每个文档，都可以分别计算一组搜索词（”中国”、”蜜蜂”、”养殖”）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。 3) 找出相似文章(结合余弦相似度) 使用TF-IDF算法，找出两篇文章的关键词； 每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）； 生成两篇文章各自的词频向量； 计算两个向量的余弦相似度，值越大就表示越相似。 词频向量（类似于词袋模型） e.g. 计算下面两个句子的相似性： 句子A：我喜欢看电视，不喜欢看电影。 句子B：我不喜欢看电视，也不喜欢看电影。 4）自动摘要 只考虑关键词首先出现的句子: 1234567891011121314151617181920212223242526272829Summarizer(originalText, maxSummarySize): // 计算原始文本的词频，生成一个数组，比如[(10,'the'), (3,'language'), (8,'code')...] wordFrequences = getWordCounts(originalText) // 过滤掉停用词，数组变成[(3, 'language'), (8, 'code')...] contentWordFrequences = filtStopWords(wordFrequences) // 按照词频进行排序，数组变成['code', 'language'...] contentWordsSortbyFreq = sortByFreqThenDropFreq(contentWordFrequences) // 将文章分成句子 sentences = getSentences(originalText) // 选择关键词首先出现的句子 setSummarySentences = &#123;&#125; foreach word in contentWordsSortbyFreq: firstMatchingSentence = search(sentences, word) setSummarySentences.add(firstMatchingSentence) if setSummarySentences.size() = maxSummarySize: break // 将选中的句子按照出现顺序，组成摘要 summary = \"\" foreach sentence in sentences: if sentence in setSummarySentences: summary = summary + \" \" + sentence return summary N-gram 模型是基于统计的语言模型 N 代表 考虑的前面词的相关的个数 当 m=1, 一个一元模型（unigram model)即为 ： 当 m=2, 一个二元模型（bigram model)即为 ： 当 m=3, 一个三元模型（trigram model)即为 而N-Gram模型也就是这样，当m=1，叫1-gram或者unigram ；m=2，叫2-gram或者bigram ；当 m=3叫3-gram或者trigram ；当m=N时，就表示的是N-gram啦。 应用场景：【2】 Word2vec Effectively, Word2Vec is based on distributional hypothesis where the context for each word is in its nearby words. Hence, by looking at its neighbouring words, we can attempt to predict the target word. ——–An implementation guide to Word2Vec using NumPy and Google Sheets: Learn the inner workings of Word2Vec 中译：手把手教你NumPy来实现Word2vec Cbow 与skip-gram区别 Skip-gram: works well with small amount of the training data, represents well even rare words or phrasesCBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words skip-gram 理解 Word2Vec 之 Skip-Gram 模型 里面介绍了负采样 的细节 一般使用跳字模型的中心词向量作为词的表征向量。 skip-gram 中概率乘积的理解【3】 条件概率公式如下： 根据其神经网络结构图，可知两个向量其实分别对应于两个 不同矩阵W中的行和列 Skip-gram 损失函数 FastTextfastText 是 2016年 FAIR(Facebook AIResearch) 推出的一款文本分类与向量化工具。 fastText 提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入 word2vec 中的 skip-gram。 比如，对于单词‘where’，当n = 3时，我们得到所有 ⻓度为3的子词:“&lt;wh”“whe”“her”“ere”“re&gt;”以及特殊子词“”。将中心词向量表示成单词的子词向量之和。 构词学（morphology）作为语言学的一个重要分支，研究的正是词的内部结构和形成方式。 在 word2vec 中，我们并没有直接利用构词学中的信息。无论是在 skip-gram 还是 CBOW 中，我们将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。 fastText 源码分析 FastText v0.1.0 中文文档 https://www.bookstack.cn/books/fasttext-doc-zh textRNN &amp; textCNN 相关文章 出身清华姚班，斯坦福博士毕业，她的毕业论文成了「爆款」 自然语言处理在电商的技术实践 解读NLP深度学习的各类模型 参考 TF-IDF与余弦相似性的应用（一）：自动提取关键词🔼 了解N-Gram模型 word2vec详解（CBOW，skip-gram，负采样，分层Softmax）","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"http://yoursite.com/tags/nlp/"}],"author":"Dufy"},{"title":"⭐️bert专题","slug":"bert专题","date":"2020-06-14T16:00:00.000Z","updated":"2021-03-07T14:22:00.237Z","comments":true,"path":"2020/06/15/bert专题/","link":"","permalink":"http://yoursite.com/2020/06/15/bert专题/","excerpt":"[TOC]","text":"[TOC] bert 模型结构BERT 模型的结构主要由三部分构成： 输入层 编码层 任务相关层 输入表示输入表示为每个词对应的词向量，segment向量，位置向量相加而成。 预训练预训练整体过程： Attention参考：《Attention is All You Need》浅读（简介+代码） 包含种类 attention 本质 以seq2seq为例，不采用attention 的结构如下： 采用attention 后，结构如下： 也就是，由原先的固定的中间语义表示C，变为随着输出Y来动态变化的Ci，从而实现对于输入信息的选择性关注。 脱离 encoder-decoder 架构，attention 要实现的一个操作是： Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。 优点： 远距离相互依赖特征的捕获 并行化 mask作用 参见：NLP 中的Mask全解 对于NLP中mask的作用，先上结论： 1、padding mask：处理非定长序列，区分padding和非padding部分，如在RNN等模型和Attention机制中的应用等2、sequence mask：防止标签泄露，如：Transformer decoder中的mask矩阵，BERT中的[Mask]位，XLNet中的mask矩阵等PS：padding mask 和 sequence mask非官方命名 发展 本质上，注意⼒机制能够为表征中较有价值的部分分配较多的计算资源。这个有趣的想法⾃提出后得到了快速发展，特别是启发了依靠注意⼒机制来编码输⼊序列并解码出输出序列的变换器（Transformer）模型的设计。变换器抛弃了卷积神经⽹络和循环神经⽹络的架构。它在计算效率上⽐基于循环神经⽹络的编码器—解码器模型通常更具明显优势。含注意⼒机制的变换器的编码结构在后来的BERT预训练模型中得以应⽤并令后者⼤放异彩：微调后的模型在多达11项⾃然语⾔处理任务中取得了当时最先进的结果。不久后，同样是基于变换器设计的GPT-2模型于新收集的语料数据集预训练后，在7个未参与训练的语⾔模型数据集上均取得了当时最先进的结果。除了⾃然语⾔处理领域，注意⼒机制还被⼴泛⽤于图像分类、⾃动图像描述、唇语解读以及语⾳识别。 GitHub:https://github.com/NLP-LOVE/ML-NLP Transformer 代码实现 https://github.com/harvardnlp/annotated-transformer 文章： http://nlp.seas.harvard.edu/2018/04/03/attention.html https://www.cnblogs.com/guoyaohua/p/transformer.html【译文】 强烈推荐：Transformer原理：自底向上解析 transformer结构： 解读 上图整体上依然是个seq2seq结构，左边表示Encoder，右边表示Decoder，两边的N均表示将对应的block重复N次。 首先将带有位置信息的Embedding形式的输入sequence a经过Multi-Head Attention处理成sequence b； Add：将Multi-Head Attention的input a和output b相加得到b&#39;； Norm：将b&#39;进行Layer Norm规范化处理； 为了增强模型表示能力，将上一步产生的结果递交给两层全连接的FFN，第一层ReLU激活，第二层线性激活，然后继续Add&amp;Norm处理； 将1-4步重复N次后进入Decoder，首先依然进行Multi-Head Attention处理，与上边不同的是加入Masked方法使得模型只能attend到已经产生的sequence，然后继续Add&amp;Norm处理； 将Encoder的结果进行Multi-Head Attention处理； 将上一步的结果进行FNN&amp;Add&amp;Norm操作； 将5-7步重复N次后得到最终output。 实际应用时的流程 Bert「行知」NLP新星：BERT的优雅解读 预训练语言模型整理（ELMo/GPT/BERT…） 你保存的BERT模型为什么那么大？ 实践美团BERT的探索和实践 NLP - 15 分钟搭建中文文本分类模型 Albert –文本分类 NLP（二十八）多标签文本分类 https://blog.csdn.net/jclian91/article/details/105386190 Albert –NERNLP（二十四）利用ALBERT实现命名实体识别 GitHub：https://github.com/percent4/ALBERT_NER_KERAS NLP（三十）利用ALBERT和机器学习来做文本分类 https://mp.weixin.qq.com/s/rt0yGcsYHQVRsXUYVmGoyQ 结合了常规机器学习方法，进行NER 效果的展示。当然，也可以结合深度学习：NLP（二十二）利用ALBERT实现文本二分类 NLP 基于kashgari和BERT实现中文命名实体识别（NER）https://www.shuzhiduo.com/A/x9J2EabKz6/ 同时，参考： NLP - 基于 BERT 的中文命名实体识别（NER)https://eliyar.biz/nlp_chinese_bert_ner/ 自己的github: http://localhost:8888/notebooks/machine_learning/BERT/kashgari-BERT-ner.ipynb Bert+bilstm+CRF实体识别,bertBiLSTMCRF基于Keras实现 BERT中文命名实体识别TensorFlow实现 Bert flaskhttps://github.com/luoyangbiao/bert_flask 使用bert训练MRPC数据集，写成API接口模式以及简易的html界面 BERT相关论文、文章和代码资源汇总 参考 PTMs：史上最全面总结 NLP 预训练模型","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"http://yoursite.com/tags/nlp/"},{"name":"bert","slug":"bert","permalink":"http://yoursite.com/tags/bert/"}],"author":"Dufy"},{"title":"⭐️NER专题","slug":"NER专题","date":"2020-06-13T16:00:00.000Z","updated":"2021-02-27T06:34:04.097Z","comments":true,"path":"2020/06/14/NER专题/","link":"","permalink":"http://yoursite.com/2020/06/14/NER专题/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 研究方法 基于规则 基于特征的机器学习方法 参考【2】 基于深度学习【6】、【7】 结构示例如下： Distributed representations for input consider word- and character-level embeddings as well as incorporation of additional features like POS tag and gazetteer that have been effective in feature-based based approaches. Context encoder is to capture the context dependencies using CNN, RNN, or other networks. Tag decoder predict tags for tokens in the input sequence. For instance, in Figure 2 each token is predicted with a tag indicated by B-(begin), I-(inside), E-(end), S-(singleton) of a named entity with its type, or O-(outside) of named entities. Note that there are other tag schemes or tag notations, e.g., BIO. Tag decoder may also be trained to detect entity boundaries and then the detected text spans are classified to the entity types. 一文详解深度学习在命名实体识别(NER)中的应用 输入的分布表示One-Hot编码独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。 Word-level 表示代表工具：Word2Vec、fastText等 Word2Vec Word2Vec 是谷歌的分布式词向量工具。使用它可以很方便的得到词向量。Word2Vec 分别使用两个模型来学习词向量，一个是 CBOW(Continuous bag-of-word)，另一个是 Skip-gram模型。 CBOW 模型是指根据上下文来预测当前单词。而Skip-gram 是根据给定的词去预测上下文。所以这两个模型的本质是让模型去学习词和上下文的 co-occurrence。有意思的是，我们需要的词向量只是两个模型的“副产物”。 Word2Vec 得到的词向量相比于传统的 one-hot 词向量相比，主要有以下两个优势： 低维稠密：Word2Vec 得到的词向量一般设置为50-500之间，而 one-hot 类型的词向量维度等于词表大小。 蕴含语义信息：one-hot 表示法中，每一个词与其他词都是正交的，也就是说词与词之间没有任何关系，这不符合实际情况。而 Word2Vec 的假设“具有相同上下文的词语包含相似的语义信息”，使得语义相近的词保留了一定的几何关系。 关于CBOW 和 Skip-gram 详细的解读参见Word2Vec HMM一文搞懂HMM（隐马尔可夫模型） CRF 特征函数【3】 输入： a sentence s the position i of a word in the sentence the label l_i of the current word the label l_i−1 of the previous word 输出： and outputs a real-valued number (though the numbers are often just either 0 or 1). 概率计算 给定输入s，计算标注 l 的概率：(注意其有2个求和：外层为所有特征函数，内层为句子s所有位置) 特征模板解读【4】 特征模板如下： 特征模板格式：%x[row,col] 其中，x可取U或B，对应两种类型。 U 反映了观测值对当前标签的影响，B反映不同位置标签的影响。 Unigram类型 e.g. 123人 B民 M网 E &lt;----当前行 U00:%x[-2,0] -2代表当前行偏移2个位置 将产生以下的特征函数：(注：下列伪码的x代表观测，y代表状态，也就是贴的标签) 1If(y==&apos;E&apos;&amp;&amp;x==&apos;人&apos;) return 1 else return 0; 每个特征函数会有一个权重参数W，需要训练得到这个参数。 关于生成的model文件【5】 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172version: 100cost-factor: 1maxid: 2159868xsize: 1 BEMS U00:%x[-2,0]U01:%x[-1,0]U02:%x[0,0]U03:%x[1,0]U04:%x[2,0]U05:%x[-2,0]/%x[-1,0]/%x[0,0]U06:%x[-1,0]/%x[0,0]/%x[1,0]U07:%x[0,0]/%x[1,0]/%x[2,0]U08:%x[-1,0]/%x[0,0]U09:%x[0,0]/%x[1,0]B 0 B16 U00:-20 U00:024 U00:128 U00:232 U00:336 U00:440 U00:544 U00:648 U00:752 U00:856 U00:960 U00:_B-164 U00:_B-2……17404 U01:厨17408 U01:去17412 U01:县17416 U01:参17420 U01:又17424 U01:叉17428 U01:及17432 U01:友17436 U01:双17440 U01:反17444 U01:发17448 U01:叔17452 U01:取17456 U01:受……77800 U05:_B-1/一/个107540 U05:一/方/面107544 U05:一/无/所107548 U05:一/日/三107552 U05:一/日/为107556 U05:一/日/之……566536 U06:万/吨/_B+1……2159864 U09:ｖ/ｅ -8.53540175259997199.04914538141489017.0388286231971700-7.25455581640930095.2799470769112835-8.5333633546653758-5.35491907356069335.2575182675282477-5.4259109736696054 综述 [笔记] 综述 A Survey on Deep Learning for Named Entity Recognition http://pelhans.com/2019/09/23/kg_paper-note4/ 配合论文一起研究 流水的NLP铁打的NER：命名实体识别实践与探索 https://zhuanlan.zhihu.com/p/166496466 工具：Kashgari github地址 https://github.com/BrikerMan/Kashgari 语料15个免费数据集和命名实体识别NER语料库 https://inforscan.com/ 命名实体识别的语料和代码 参考 word2vec 独热编码One-Hot 【NLP】基于CRF条件随机场的命名实体识别原理详解 Introduction to Conditional Random Fields🔼 CRF++/CRF/条件随机场的特征函数模板🔼 CRF++模型格式说明🔼 命名实体识别（一）[笔记] 综述 A Survey on Deep Learning for Named Entity Recognition🔼 命名实体识别总结🔼","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"http://yoursite.com/tags/nlp/"}],"author":"Dufy"},{"title":"⭐️机器学习知识点","slug":"机器学习知识点","date":"2020-05-23T16:00:00.000Z","updated":"2021-02-27T06:27:00.592Z","comments":true,"path":"2020/05/24/机器学习知识点/","link":"","permalink":"http://yoursite.com/2020/05/24/机器学习知识点/","excerpt":"[TOC]","text":"[TOC] AI 发展史 机器学习解决问题的思路 机器学习问题，是要从有限的样例中通过算法总结出一般性的规律，并可以应用到新的未知数据上。 机器学习类型比较 处理流程对比 花书💐：【注意可“学习”的部分越来越多】 基于规则的系统 经典机器学习 表示学习 静态计算图 vs 动态计算图都9102年了还不懂动态图吗？一文带你了解飞桨动态图 目前深度学习框架主要有声明式编程和命令式编程两种编程方式。声明式编程，代码先描述要做的事情但不立即执行，对深度学习任务建模，需要事先定义神经网络的结构，然后再执行整个图结构，这一般称为静态图模式。而命令式编程对应的动态图模式，代码直接返回运算的结果，神经网络结构的定义和执行同步。通常来说，静态图模式能够对整体性做编译优化，更有利于性能的提升，而动态图则非常便于用户对程序进行调试。 PyTorch的动态计算图体现在什么地方？ 训练需要的样本量 As of 2016, a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category, and will match or exceed human performance when trained with a dataset containing at least 10 million labeled examples. Working successfully with datasets smaller than this is an important research area, focusing in particular on how we can take advantage of large quantities of unlabeled examples, with unsupervised or semi-supervised learning. 对于监督深度学习算法，每类的样本标注量 5000，总共需至少 1000万的标注样本 提前终止依靠验证集判断，防止过拟合 此外，关于验证集作用，还可以用来模型选择和调参 当通过训练集训练出多个模型后，为了能找出效果最佳的模型，使用各个模型对验证集数据进行预测，并记录模型准确率。选出效果最佳的模型所对应的参数，即用来调整模型参数。如svm中的参数c和核函数等。 训练集、验证集、测试集（附：分割方法+交叉验证） 偏差与方差注意是不同训练集 组合情况 图2.7给出了机器学习模型的期 望错误、偏差和方差随复杂度的变化情况，其中红色虚线表示最优模型.最优模 型并不一定是偏差曲线和方差曲线的交点. 评价指标对于分类问题，评价标准有： 准确率 精确率P 召回率R F值 其中，P/R是针对每个类进行的性能估计 为了计算所有类别的总体P,R，有两种方法： 宏平均：每一类的算数平均值 微平均 一般 Micro 方法更容易受到样本不均衡的影响, 容易使得表现较好的大数样本掩盖表现不好的小数据量类别. 逻辑回归可以看成是只有一层的神经网络，【参见】⏬ 万字干货|逻辑回归最详尽解释 损失函数由来 决策🌲决策树的判断过程类似于if-else 有不同的决策准则 信息增益准则：对取值数目较多的属性偏好 增益率准则：对取值数目较少的属性偏好 基尼指数 ID3 树的构建原则：信息增益变大 信息熵的公式，其实也很简单： Pk表示的是：当前样本集合D中第k类样本所占的比例为Pk。 C4.5 定义增益率 其中： ，为属性a的’固有值‘ 在使用时候，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。 CART 属性a 的基尼指数为：【选择其值最小的属性】 其中，Gini(D)为基尼值，衡量数据集D的纯度 剪枝 目的是用来对付’过拟合‘ 分为两种 预剪枝：基于’贪心‘ 在确定好属性后，根据验证集来判断进一步的划分是否提高了验证集的准确率，来据此最终确定划分的进行与否。 后剪枝，一般比预剪枝有更多的分支。性能上优于预剪枝，时间复杂度高 是在树构建好后，自底向上对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。 随机🌲🌲🌲RF 是Bagging 的一个扩展变体 RF在以决策树为基学习器构建Bagging 集成的基础上，进一步在决策树的训练过程中引入随机属性选择 拓展–【基分类器的特点】： 最好是不稳定的分类器，即对样本分布较为敏感为好 除了决策树，神经网络也适合作为基分类器 神经网络分类图4.6给出了前馈网络、记忆网络和图网络的网络结构示例，其中圆形节点 表示一个神经元，方形节点表示一组神经元. 前馈神经网络的理解对于分类的前馈神经网络 = 特征抽取 + 分类器（Logistic 、Softmax等） 可以看出，神经网络的一个重要作用就是 特征的抽取变换，即将样本的 原始特征向量 𝒙 转换到更有效的特征向量 𝜙(𝒙)，这个过程叫作特征抽取. 也就是说，Logistic回归或 Softmax 回 归 也可以看作只有一层的神经网络.返回 CNN 常用的卷积网络整体结构： 感受野用来表示网络内部的不同位置的神经元对原图像的感受范围的大小【2】： 典型神经网络 提出时间线： LeNet-5 AlexNet2012年，AlexNet横空出世。这个模型的名字来源于论文第一作者的姓名Alex Krizhevsky [1]。AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。 ! 对于全连接处，需要一个flatten()的操作： import numpy as npa = np.array([[1,2],[3,4]])a.flatten()——&gt;array([1, 2, 3, 4]) VGG16VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。 不得不说VGG是个很可爱的神经网络结构，它的成功说明了网络结构还是深的好。它使用的卷积全部为3x3，Pad=1，步长为1，也就是说，卷积不会改变输出大小，而改变输出大小这件事就交给了2x2，步长为2 的max pool，也就是说每通过一个 max pool，卷积的尺寸都会折半。 GoogLeNet–含并行连结的网络 2014年对于计算机视觉领域是一个丰收的一年，在这一年的ImageNet图像识别挑战赛(ILSVRC,ImageNet Large Scale Visual Recognition Challenge)中出现了两个经典、影响至深的卷积神经网络模型，其中第一名是GoogLeNet、第二名是VGG。 GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。与上一节介绍的NiN块相比，这个基础块在结构上更加复杂，如图5.8所示。 关于1*1卷积的使用，原因如下： ＧoogLeNet共有22层，原始输入数据的大小为224*224 *3。整体结构如下： 详细展开如下【1】： GoogLeNet网络模型参数变化如下图所示： 残差网络（ResNet）残差网络是为了解决深度神经网络（DNN）隐藏层过多时的网络退化问题而提出。退化（degradation）问题是指：当网络隐藏层变多时，网络的准确度达到饱和然后急剧退化，而且这个退化不是由于过拟合引起的。 下图中的Weight在卷积网络中是指卷积操作，addition是指单位加操作。详解残差网络 DenseNet RNN–循环神经网路 应用模式 LSTM 三个门的作用 遗忘门 𝒇𝑡 控制上一个时刻的内部状态 C𝑡−1 需要遗忘多少信息. 输入门𝒊𝑡 控制当前时刻的候选状态𝒄̃ 有多少信息需要保存. 输出门 𝒐𝑡 控制当前时刻的内部状态 𝒄𝑡 有多少信息需要输出给外部状态𝒉𝑡. 公式计算： GRU 网络 深层RNN 递归神经网络递归神经网络(Recursive Neural Network，RecNN)是循环神经网络在有向无循环图上的扩展 [Pollack, 1990].递归神经网络的一般结构为树状的层次结构。 当递归神经网络的结构退化为线性序列结构(见图6.11b)时，递归神经网络就等价于简单循环网络. 神经网络优化方法 概率图模型 概率图模型体系：HMM、MEMM、CRF 生成模型 vs 判别模型谈谈序列标注三大模型HMM、MEMM、CRF 生成模型 判别模型 学习联合概率p(x,y)，–&gt;p(y|x) 直接学习p(y|x)，得到分类边界 朴素贝叶斯（NB）、HMM 神经网络、SVM、CRF HMM隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。 隐马尔可夫模型由初始状态概率向量π、状态转移概率矩阵A和观测概率矩阵B决定。因此，隐马尔可夫模型可以写成λ=(A,B,π) HMM的3个基本问题： 学习算法 分两种情况： 预测 求隐藏状态序列的过程 采用维特比算法，根据动态规划求概率最大的路径. 采用了DP思想，可以减少计算量，即每一次直接引用前一个时刻的计算结果以避免重复计算。 CRF 参考 ＧoogLeNet清晰图🔼 深度神经网络中的感受野(Receptive Field)🔼 《神经网络与深度学习》 邱锡鹏著 https://github.com/nndl/nndl.github.io","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}],"author":"Dufy"},{"title":"⭐️计算机理解","slug":"计算机理解","date":"2020-05-06T16:00:00.000Z","updated":"2021-05-01T13:52:59.509Z","comments":true,"path":"2020/05/07/计算机理解/","link":"","permalink":"http://yoursite.com/2020/05/07/计算机理解/","excerpt":"","text":"晶体管晶体管的导通： 内存存储设备种类： 参考：程序员需要了解的硬核知识之内存 冯-诺依曼体系结构“冯·诺依曼结构”有两个关键点： 一是指出要将存储设备与中央处理器分开 二是提出了将数据以二进制方式编码 代码如何控制硬件参见知乎：代码是如何控制硬件的？ 参考文章计算机是如何工作的（最简单透彻的解释），整理如下： 视频：https://baike.baidu.com/item/%E6%99%B6%E4%BD%93%E7%AE%A1/569042?fromtitle=transistor&amp;fromid=8858808 值得好好观看👆 来自百度词条【晶体管】 字符编码 在python 程序中，当我们的程序运行起来后，内存中存储的字符串默认使用的是unicode。目的是 Unicode定长，好处理。但是，如果涉及到字符串的存储和传输，就必须进行编码，编码成utf-8 或 gbk 进行传递，原因是unicode 太浪费空间了。 12345678s = '很好' # 内存中使用unicode# 编码bs = s.encode('utf-8') print(bs) # b'\\xe5\\xbe\\x88\\xe5\\xa5\\xbd'print(type(bs)) # &lt;class 'bytes'&gt;bytes 数据类型# 在utf-8中，一个汉字是3个字节，每个\\x代表一个字节print(s.encode('gbk')) # 4字节：b'\\xba\\xdc\\xba\\xc3' 解码和bytes 数据类型转换： 12345678# 解码bs = b'\\xe5\\xbe\\x88\\xe5\\xa5\\xbd'print(bs.decode('utf8'))# gbk--&gt;&gt;utf8字节bs = b'\\xba\\xdc\\xba\\xc3's = bs.decode('gbk')print(s.encode('utf8')) # b'\\xe5\\xbe\\x88\\xe5\\xa5\\xbd' 字符编码笔记：ASCII，Unicode 和 UTF-8 全网最适合Python小白的Python基础课（纯干货，超详细） 1024 or 1000 ？ 编程范式再谈编程范式—程序语言背后的思想 网络互联网是分布式的 HTTP HTTP 是基于 TCP/IP 协议的应用层协议。它不涉及数据包（packet）传输，主要规定了客户端和服务器之间的通信格式，默认使用80端口。【2】 RPC RPC 指的是远程函数调用，是服务器通信的常见方法。本文介绍 RPC 的概念，然后用 C 语言演示一个简单的例子。 从头开始编写 RPC（英文） 参考 B站：计算机内部部件是如何工作的 阮一峰老师的《HTTP协议入门》","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"底层","slug":"底层","permalink":"http://yoursite.com/tags/底层/"}],"author":"Dufy"},{"title":"⭐️算法刷题","slug":"算法刷题","date":"2020-05-01T16:00:00.000Z","updated":"2021-05-11T15:56:04.965Z","comments":true,"path":"2020/05/02/算法刷题/","link":"","permalink":"http://yoursite.com/2020/05/02/算法刷题/","excerpt":"数据结构是工具，算法是通过合适的工具解决特定问题的方法。","text":"数据结构是工具，算法是通过合适的工具解决特定问题的方法。 刷题顺序规则： 按分类刷；每个分类从 Easy 到 Medium 顺序刷； 优先刷 树、链表、二分查找、DFS、BFS 等面试常考类型； 优先刷题号靠前的题目； 优先刷点赞较多的题目； LeetCode 现在将近 2000 道题，基本没有人能够全部刷完，而且对于参加面试者来说也没有必要刷特别多的题。许多人在面试前刷了 200 道题，基本够了；准备更充分的人，大概会刷 400 道题；能刷 600 道题目以上的，基本上国内公司的 Offer 都能收获到一大堆。 框架二分查找1234567891011121314151617def binarySearch(lis, val): &apos;二分查找&apos; pass left = 0 right = len(lis)-1 while left&lt;=right: # 候选区域有值 mid = (left+right) // 2 if lis[mid]==val: return mid elif lis[mid]&gt;val: # 待查找的值在mid 左边 right = mid-1 else: # 待查找的值在mid 右边 left = mid+1 return Noneli = [1,2,3,4,5,6,7,8,9]print(binarySearch(li, 3)) 应用：力扣【35】 队列双端队列 123456789101112import collections# 创建队列d = collections.deque()# 入队d.append(1) #从队尾d.appendleft(2) #从队头# 出队d.pop() #从队尾d.popleft() #从队头 数组27. 移除元素给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素，并返回移除后数组的新长度。 不要使用额外的数组空间，你必须仅使用 O(1) 额外空间并 原地 修改输入数组。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 说明: 为什么返回数值是整数，但输出的答案是数组呢? 请注意，输入数组是以「引用」方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。 你可以想象内部操作如下: 12345678// nums 是以“引用”方式传递的。也就是说，不对实参作任何拷贝int len = removeElement(nums, val);// 在函数里修改输入数组对于调用者是可见的。// 根据你的函数返回的长度, 它会打印出数组中 该长度范围内 的所有元素。for (int i = 0; i &lt; len; i++) &#123; print(nums[i]);&#125; 示例 1： 123输入：nums = [3,2,2,3], val = 3输出：2, nums = [2,2]解释：函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。你不需要考虑数组中超出新长度后面的元素。例如，函数返回的新长度为 2 ，而 nums = [2,2,3,3] 或 nums = [2,2,0,0]，也会被视作正确答案。 示例 2： 123输入：nums = [0,1,2,2,3,0,4,2], val = 2输出：5, nums = [0,1,4,0,3]解释：函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。注意这五个元素可为任意顺序。你不需要考虑数组中超出新长度后面的元素。 答案： 快慢指针 动图参考：https://leetcode-cn.com/problems/remove-element/solution/xue-sheng-wu-de-nu-peng-you-du-neng-kan-nk7yy/ 12345678910111213141516171819class Solution: def removeElement(self, nums: List[int], val: int) -&gt; int: # --------------- # l = len(nums) # for i in range(l-1,-1,-1): # 倒序 # if nums[i]==val: # nums[i:] = nums[i+1:] # return len(nums) # --------------- fast = 0 slow = 0 while fast&lt;len(nums): if nums[fast]==val: fast += 1 else: nums[slow] = nums[fast] slow += 1 fast += 1 return slow 代码也可以改进下： 12345678910class Solution: def removeElement(self, nums: List[int], val: int) -&gt; int: fast = 0 slow = 0 while fast&lt;len(nums): if nums[fast]!=val: nums[slow] = nums[fast] slow += 1 fast += 1 return slow 35. 搜索插入位置 🔼给定一个排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。 你可以假设数组中无重复元素。 示例 1: 12输入: [1,3,5,6], 5输出: 2 示例 2: 12输入: [1,3,5,6], 2输出: 1 示例 3: 12输入: [1,3,5,6], 7输出: 4 示例 4: 12输入: [1,3,5,6], 0输出: 0 1234567891011121314class Solution: def searchInsert(self, nums: List[int], target: int) -&gt; int: left = 0 right = len(nums)-1 while left&lt;=right: if nums[right]&lt;target: return right+1 elif nums[left]&gt;target: return left mid_index = (left+right)//2 if nums[mid_index]==target: # 先将等于置前 return mid_index elif nums[mid_index]&lt;target: left = mid_index+1 else: right = mid_index-1 解析：二分查找的改进 209. 长度最小的子数组给定一个含有 n 个正整数的数组和一个正整数 target 。 找出该数组中满足其和 ≥ target 的长度最小的 连续子数组 [numsl, numsl+1, ..., numsr-1, numsr] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。 示例 1： 123输入：target = 7, nums = [2,3,1,2,4,3]输出：2解释：子数组 [4,3] 是该条件下的长度最小的子数组。 示例 2： 12输入：target = 4, nums = [1,4,4]输出：1 示例 3： 12输入：target = 11, nums = [1,1,1,1,1,1,1,1]输出：0 可以采用队列，参考：https://leetcode-cn.com/problems/minimum-size-subarray-sum/solution/javade-jie-fa-ji-bai-liao-9985de-yong-hu-by-sdwwld/ 12345678910111213141516class Solution: def minSubArrayLen(self, target: int, nums: List[int]) -&gt; int: from collections import deque result = float('inf') d = deque() for i in nums: d.append(i) # 入队 if sum(d)&lt;target: continue while sum(d)&gt;=target: d.popleft() if result&gt; len(d)+1: result = len(d)+1 print(d) return result if result !=float('inf') else 0 这个结果远谈不上好： 15. 三数之和给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，*使得 *a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。 注意：答案中不可以包含重复的三元组。 示例 1： 12输入：nums = [-1,0,1,2,-1,-4]输出：[[-1,-1,2],[-1,0,1]] 示例 2： 12输入：nums = []输出：[] 采用左右指针： 123456789101112131415161718192021222324252627class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: nums.sort() # 从小到大排序 [-4,-1,-1,0,1,2] res = [] leng = len(nums) for ind, val in enumerate(nums): if val&gt;0: break elif ind&gt;0 and val==nums[ind-1]: # val 重复情况 continue left = ind +1 right = leng-1 while left&lt;right: sum1 = val+nums[left]+nums[right] if sum1==0: res.append([val,nums[left],nums[right]]) left += 1 while nums[left]==nums[left-1] and left&lt;len(nums)-1: left += 1 right -= 1 while nums[right]==nums[right+1] and right&gt;1: right -= 1 elif sum1&gt;0: right -= 1 else: left += 1 return res 参考：漫画：三数之和 和 N数之和 链表203. 移除链表元素删除链表中等于给定值 val\\ 的所有节点。 示例: 12输入: 1-&gt;2-&gt;6-&gt;3-&gt;4-&gt;5-&gt;6, val = 6输出: 1-&gt;2-&gt;3-&gt;4-&gt;5 思路： 需要两个游标，其中一个记录父节点，方可删除 这里用 p 表示父节点。这里tmp 的出现也很有必要 12345678910111213141516171819# Definition for singly-linked list.# class ListNode:# def __init__(self, val=0, next=None):# self.val = val# self.next = nextclass Solution: def removeElements(self, head: ListNode, val: int) -&gt; ListNode: tmp = ListNode() tmp.next = head p = tmp q = p.next while q != None: if q.val == val: p.next = q.next q = q.next else: p = p.next # 或者p=q q = p.next return tmp.next 206. 反转链表反转一个单链表。 示例: 12输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 链表：听说过两天反转链表又写不出来了？ 关键是注意，p,q没关系最初 1234567891011121314# Definition for singly-linked list.# class ListNode:# def __init__(self, val=0, next=None):# self.val = val# self.next = nextclass Solution: def reverseList(self, head: ListNode) -&gt; ListNode: p = None q = head while q != None: tmp = q.next q.next = p p, q = q, tmp return p 141. 环形链表给定一个链表，判断链表中是否有环。 如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。注意：pos 不作为参数进行传递，仅仅是为了标识链表的实际情况。 如果链表中存在环，则返回 true 。 否则，返回 false 。 进阶： 你能用 O(1)（即，常量）内存解决此问题吗？ 示例 1： 123输入：head = [3,2,0,-4], pos = 1输出：true解释：链表中有一个环，其尾部连接到第二个节点。 示例 2： 123输入：head = [1,2], pos = 0输出：true解释：链表中有一个环，其尾部连接到第一个节点。 示例 3： 123输入：head = [1], pos = -1输出：false解释：链表中没有环。 采用快慢指针 12345678910111213141516# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def hasCycle(self, head: ListNode) -&gt; bool: slow = fast = head while fast and fast.next: # 终止条件 slow = slow.next fast = fast.next.next if slow==fast: return True return False 234. 回文链表请判断一个链表是否为回文链表。 示例 1: 12输入: 1-&gt;2输出: false 示例 2: 12输入: 1-&gt;2-&gt;2-&gt;1输出: true 通过画图，可知，无论奇数、还是偶数个节点，得到中间节点，也就是待翻转的链表后半段，均可 通过代码：r = s.next # 待翻转 的链表 实现 123456789101112131415161718192021222324252627282930313233343536# Definition for singly-linked list.# class ListNode:# def __init__(self, val=0, next=None):# self.val = val# self.next = nextclass Solution: def isPalindrome(self, head: ListNode) -&gt; bool: # 1. 采用快慢指针，得到中间节点 s = head f = head while f.next is not None and f.next.next is not None: s = s.next f = f.next.next r = s.next # 待翻转 的链表 # 2. 翻转 r1 = self.reverseList(r) # 3.遍历，比较 before = head after = r1 while after: if after.val != before.val: return False after = after.next before = before.next return True def reverseList(self, head: ListNode) -&gt; ListNode: p = None q = head while q != None: tmp = q.next q.next = p p, q = q, tmp return p 哈希表242. 有效的字母异位词给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。 示例 1: 12输入: s = &quot;anagram&quot;, t = &quot;nagaram&quot;输出: true 示例 2: 12输入: s = &quot;rat&quot;, t = &quot;car&quot;输出: false 123456789101112131415class Solution: def isAnagram(self, s: str, t: str) -&gt; bool: count = &#123;&#125; # &#123;'a':1, 'b':2&#125; for i in s: # 先统计出现次数 count[i] = count.get(i, 0) + 1 for i in t: # 再逐个减 if i not in count: return False else: count[i] -= 1 for i in count: if count[i]!=0: return False return True 参考：https://leetcode-cn.com/problems/valid-anagram/solution/ha-xi-biao-fang-fa-ke-gua-ying-geng-da-gui-mo-zi-f/ 349. 两个数组的交集给定两个数组，编写一个函数来计算它们的交集。 示例 1： 12输入：nums1 = [1,2,2,1], nums2 = [2,2]输出：[2] 示例 2： 12输入：nums1 = [4,9,5], nums2 = [9,4,9,8,4]输出：[9,4] 123456class Solution: def intersection(self, nums1: List[int], nums2: List[int]) -&gt; List[int]: # return set(nums1)&amp;set(nums2) s1 = set(nums1) s2 = set(nums2) return [i for i in s1 if i in s2] set 的 union， intersection，difference 操作要比 list 的迭代要快。因此如果涉及到求 list 交集，并集或者差的问题可以转换为 set 来操作。 1. 两数之和给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素不能使用两遍。 你可以按任意顺序返回答案。 示例 1： 123输入：nums = [2,7,11,15], target = 9输出：[0,1]解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: dic = &#123;&#125; # 备查，&#123;val:index&#125; for ind,val in enumerate(nums): search_val = target-val # 要查询的目标值 if search_val in dic: return [ind, dic[search_val]] dic[val] = ind 栈20. 有效的括号给定一个只包括 &#39;(&#39;，&#39;)&#39;，&#39;{&#39;，&#39;}&#39;，&#39;[&#39;，&#39;]&#39; 的字符串 s ，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 示例 1： 12输入：s = &quot;()&quot;输出：true 示例 2： 12输入：s = &quot;()[]&#123;&#125;&quot;输出：true 通过 append(), pop()方法实现栈 12345678910111213class Solution: def isValid(self, s: str) -&gt; bool: l = [] dic = &#123;'(':')', '&#123;':'&#125;', '[':']'&#125; for i in s: if not l: # 空 l.append(i) # (, &#123;, [ elif dic.get(l[-1],None)==i: l.pop() # 出栈 else: l.append(i) # (, &#123;, [ return not l ？？239. 滑动窗口最大值栈与队列：滑动窗口里求最大值引出一个重要数据结构 给你一个整数数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。 返回滑动窗口中的最大值。 示例 1： 1234567891011输入：nums = [1,3,-1,-3,5,3,6,7], k = 3输出：[3,3,5,5,6,7]解释：滑动窗口的位置 最大值--------------- -----[1 3 -1] -3 5 3 6 7 3 1 [3 -1 -3] 5 3 6 7 3 1 3 [-1 -3 5] 3 6 7 5 1 3 -1 [-3 5 3] 6 7 5 1 3 -1 -3 [5 3 6] 7 6 1 3 -1 -3 5 [3 6 7] 7 示例 2： 12输入：nums = [1], k = 1输出：[1] 堆347. 前 K 个高频元素给定一个非空的整数数组，返回其中出现频率前 k\\ 高的元素。 示例 1: 12输入: nums = [1,1,1,2,2,3], k = 2输出: [1,2] 示例 2: 12输入: nums = [1], k = 1输出: [1] 采用小根堆 123456789class Solution: def topKFrequent(self, nums: List[int], k: int) -&gt; List[int]: counter = collections.Counter(nums) h = [] for key, val in counter.items(): heapq.heappush(h, (val, key)) if len(h) &gt; k: heapq.heappop(h) return [x[1] for x in h] 二叉树 只要涉及到递归，都可以抽象成二叉树的问题 二叉树的重要性 举个例子，比如说我们的经典算法「快速排序」和「归并排序」，对于这两个算法，你有什么理解？如果你告诉我，快速排序就是个二叉树的前序遍历，归并排序就是个二叉树的后续遍历，那么我就知道你是个算法高手了。 手把手带你刷二叉树（第一期） 遍历 144. 二叉树的前序遍历难度中等530 给你二叉树的根节点 root ，返回它节点值的 前序 遍历。 示例 1： 12输入：root = [1,null,2,3]输出：[1,2,3] 示例 2： 12输入：root = []输出：[] 12345678910111213141516171819202122232425262728293031323334353637# Definition for a binary tree node.# class TreeNode:# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution: def preorderTraversal(self, root: TreeNode) -&gt; List[int]: # 前序：中左右# =============================迭代--使用栈============================================= if root is None: return [] stack, res = [root], [] while stack: root = stack.pop() if root is not None: res.append(root.val) if root.right is not None: stack.append(root.right) if root.left is not None: stack.append(root.left) return res# 作者：JamLeon# 链接：https://leetcode-cn.com/problems/binary-tree-preorder-traversal/solution/yong-zhan-shi-xian-si-lu-jian-dan-xing-neng-gao-xi/# 来源：力扣（LeetCode）# 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 # =============================递归============================================== res = [] def helper(root): if not root: return res.append(root.val) helper(root.left) helper(root.right) helper(root) return res 中序遍历迭代方法： 写递归代码的关键，就是看能不能写出递推公式，而写递推公式的关键就是，如果要解决问题 A，就假 设子问题 B、C 已经解决，然后再来看如何利用 B、C 来解决 A。 102. 二叉树的层序遍历给你一个二叉树，请你返回其按 层序遍历 得到的节点值。 （即逐层地，从左到右访问所有节点）。 示例：二叉树：[3,9,20,null,null,15,7], 12345 3 / \\9 20 / \\ 15 7 返回其层序遍历结果： 12345[ [3], [9,20], [15,7]] 采用队列求解 12345678910111213141516171819202122232425# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: if not root: return [] res= [] queue = [root] while queue: tmp = [] for _ in range(len(queue)): node = queue.pop(0) tmp.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) res.append(tmp) return res 层序遍历的应用：判断对称二叉树 101. 对称二叉树12345678910111213141516101. 对称二叉树给定一个二叉树，检查它是否是镜像对称的。例如，二叉树 [1,2,2,3,4,4,3] 是对称的。 1 / \\ 2 2 / \\ / \\3 4 4 3但是下面这个 [1,2,2,null,3,null,3] 则不是镜像对称的: 1 / \\ 2 2 \\ \\ 3 3 对于层序的结果输出，需添加一个占位符，将不存在子节点的补上数值 123[1][2, 2]['/', 3, '/', 3] 123456789101112131415161718192021222324252627282930313233343536373839# Definition for a binary tree node.# class TreeNode:# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution: def isSymmetric(self, root: TreeNode) -&gt; bool: # 思路：层序遍历, 然后对每一层进行对称判断（采用左右指针逼近） que = [] que.append(root) placeholder = '/' while que: tmp = [] for _ in range(len(que)): node = que.pop(0) if node==placeholder: tmp.append(placeholder) else: tmp.append(node.val) if node.left: que.append(node.left) else: que.append(placeholder) if node.right: que.append(node.right) else: que.append(placeholder) # 进行对称判断 # print(tmp) i = 0 j = len(tmp)-1 while i&lt;=j: if tmp[i]==tmp[j]: i += 1 j -= 1 continue else: return False return True 104. 二叉树的最大深度123456789101112131415给定一个二叉树，找出其最大深度。二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。说明: 叶子节点是指没有子节点的节点。示例：给定二叉树 [3,9,20,null,null,15,7]， 3 / \\ 9 20 / \\ 15 7返回它的最大深度 3 。 12345678910111213141516171819202122232425# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def maxDepth(self, root: TreeNode) -&gt; int: # if not root: # return 0 # return 1+ max(self.maxDepth(root.left), self.maxDepth(root.right)) # ==============# 层序遍历============== if not root: return 0 deque = [root] depth = 0 while deque: depth +=1 for _ in range(len(deque)): node = deque.pop(0) if node.left: deque.append(node.left) if node.right: deque.append(node.right) return depth 构造满足要求的二叉树, return root 形式 涉及到树的构建或者返回改变后的树，可采用如下的结构形式： 1234567def buileTree(*args): ..... root = TreeNode(val) # 构造一棵树 root.left = buileTree(*args) # 递归调用 root.right = buileTree(*args) # 递归调用 return root 如，BST中插入一个数 450. 删除二叉搜索树中的节点12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode:# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution: def deleteNode(self, root: TreeNode, key: int) -&gt; TreeNode: if not root: return None if root.val==key: # 分为3种情况 if not root.left: # 左子树为空 root = root.right elif not root.right: # 右子树为空 root = root.left else: p = root.right while p.left: p = p.left p.left = root.left root = root.right elif root.val &lt; key: root.right = self.deleteNode(root.right, key) else: # 往左找，替换 root.left = self.deleteNode(root.left, key) return root 654. 最大二叉树1234567891011121314151617181920给定一个不含重复元素的整数数组。一个以此数组构建的最大二叉树定义如下：二叉树的根是数组中的最大元素。左子树是通过数组中最大值左边部分构造出的最大二叉树。右子树是通过数组中最大值右边部分构造出的最大二叉树。通过给定的数组构建最大二叉树，并且输出这个树的根节点。示例 ：输入：[3,2,1,6,0,5]输出：返回下面这棵树的根节点： 6 / \\ 3 5 \\ / 2 0 \\ 1 分析： 依据框架思维，首先要清楚root节点要做的是把自己构造出来，然后对数组左右部分递归调用函数 12345678910111213141516# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def constructMaximumBinaryTree(self, nums: List[int]) -&gt; TreeNode: if not nums: return None max_val = max(nums) max_ind = nums.index(max_val) # 找最大值得索引 root = TreeNode(max_val) root.left = self.constructMaximumBinaryTree(nums[:max_ind]) root.right = self.constructMaximumBinaryTree(nums[max_ind+1:]) return root 相类似的，有： 105. 从前序与中序遍历序列构造二叉树123456789101112131415# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; TreeNode: if not preorder or not inorder: return root = TreeNode(preorder[0]) index = inorder.index(preorder[0]) root.left = self.buildTree(preorder[1:1+index],inorder[:index]) root.right = self.buildTree(preorder[index+1:], inorder[index+1:]) return root 面试题 04.05. 合法二叉搜索树注意，辅助函数的使用 12345678910111213141516# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def isValidBST(self, root: TreeNode) -&gt; bool: def helper(root, min_, max_): if not root: return True if root.val&lt;=min_:return False # 只能判 False的情况 if root.val&gt;=max_:return False return helper(root.left, min_, root.val) and helper(root.right,root.val,max_) return helper(root, -float(\"inf\"), float(\"inf\")) 参考：《labuladong的算法小抄》二叉搜索树操作集锦 114. 二叉树展开为链表 注意，这里仍旧是考虑二叉树递归 给定一个二叉树，原地将它展开为一个单链表。 例如，给定二叉树 12345 1 / \\ 2 5 / \\ \\3 4 6 将其展开为： 12345678910111 \\ 2 \\ 3 \\ 4 \\ 5 \\ 6 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode:# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution: def flatten(self, root: TreeNode) -&gt; None: &quot;&quot;&quot; Do not return anything, modify root in-place instead. &quot;&quot;&quot; if not root:return # 后序遍历 self.flatten(root.left) self.flatten(root.right) if root.left: # 如果存在左节点 p = root.left while p.right: p = p.right p.right = root.right root.right = root.left root.left=None 226. 翻转二叉树翻转一棵二叉树。 示例： 输入： 12345 4 / \\ 2 7 / \\ / \\1 3 6 9 输出： 12345 4 / \\ 7 2 / \\ / \\9 6 3 1 123456789101112131415# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def invertTree(self, root: TreeNode) -&gt; TreeNode: if not root: return root.left, root.right = root.right, root.left =====# 以下位置可以互换==== self.invertTree(root.right) self.invertTree(root.left) return root 回溯 模板 77. 组合给定两个整数 n 和 k，返回 1 … n 中所有可能的 k 个数的组合。 示例: 12345678910输入: n = 4, k = 2输出:[ [2,4], [3,4], [2,3], [1,2], [1,3], [1,4],] 包括剪枝前后的对比 12345678910111213141516171819class Solution: def combine(self, n: int, k: int) -&gt; List[List[int]]: path = [] res = [] def back_tracking(n,k,start_index): # print(path) if len(path)==k: res.append(path.copy()) # 要注意这里是浅拷贝 return # for i in range(start_index,n+1): for i in range(start_index,(n-(k-len(path)))+2): # 剪枝操作 path.append(i) back_tracking(n,k,i+1) path.pop() back_tracking(n,k,1) return res 剪枝后速度提升惊人： 46. 全排列给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。 示例 1： 12输入：nums = [1,2,3]输出：[[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]] 示例 2： 12输入：nums = [0,1]输出：[[0,1],[1,0]] 示例 3： 12输入：nums = [1]输出：[[1]] 1234567891011121314151617181920class Solution: def permute(self, nums: List[int]) -&gt; List[List[int]]: # -------------回溯 path = [] res = [] def back_track(nums): pass if not nums: res.append(path.copy()) return for i in range(len(nums)): # 0,1,2,... path.append(nums[i]) tmp = nums[0:i]+nums[i+1:] # 除去nums[i] back_track(tmp) path.pop() back_track(nums) return res 131. 分割回文串给你一个字符串 s，请你将 s 分割成一些子串，使每个子串都是 回文串 。返回 s 所有可能的分割方案。 回文串 是正着读和反着读都一样的字符串。 示例 1： 12输入：s = &quot;aab&quot;输出：[[&quot;a&quot;,&quot;a&quot;,&quot;b&quot;],[&quot;aa&quot;,&quot;b&quot;]] 示例 2： 12输入：s = &quot;a&quot;输出：[[&quot;a&quot;]] 12345678910111213141516171819202122class Solution: def partition(self, s: str) -&gt; List[List[str]]: # 回溯算法====== path = [] res= [] def is_palindrome(s): return s==s[::-1] def back_traking(s): if s=='': res.append(path[:]) return for i in range(len(s)): # 处理节点 edge = s[:i+1] path.append(edge) # 递归 if is_palindrome(edge): back_traking(s[i+1:]) # 回溯 path.pop() back_traking(s) return res 动态规划 动态规划与贪心区别 动态规划是由前一个状态推导出来的，而贪心是直接局部选最优 解题步骤（动规五部曲） 1) 确定dp数组（dp table）以及下标的含义 2) 确定递推公式 3) dp数组如何初始化 4) 确定遍历顺序 5) 举例推导dp数组 509. 斐波那契数斐波那契数，通常用 F(n) 表示，形成的序列称为 斐波那契数列 。该数列由 0 和 1 开始，后面的每一项数字都是前面两项数字的和。也就是： 12F(0) = 0，F(1) = 1F(n) = F(n - 1) + F(n - 2)，其中 n &gt; 1 给你 n ，请计算 F(n) 。 示例 1： 123输入：2输出：1解释：F(2) = F(1) + F(0) = 1 + 0 = 1 示例 2： 123输入：3输出：2解释：F(3) = F(2) + F(1) = 1 + 1 = 2 1234567891011121314151617class Solution: def fib(self, n: int) -&gt; int: \"\"\" - 解题步骤（动规五部曲） &gt;1) 确定dp数组（dp table）以及下标的含义 &gt;2) 确定递推公式 &gt;3) dp数组如何初始化 &gt;4) 确定遍历顺序 &gt;5) 举例推导dp数组 \"\"\" if n&lt;=1: return n dp = [0]*(n+1) # dp[i]表示第 i 个数的斐波那契值 dp[0] = 0 dp[1] = 1 for i in range(2,n+1): dp[i] = dp[i-1] + dp[i-2] return dp[n] 优化： 主要是在空间复杂度上 12345678910class Solution: def fib(self, n: int) -&gt; int: if n&lt;=1: return n dp = [0]*(2) # dp[i]表示第 i 个数的斐波那契值 dp[0] = 0 dp[1] = 1 for i in range(2,n+1): tmp = dp[0]+dp[1] dp[0], dp[1] = dp[1], tmp return dp[1] 62. 机器人不同路径一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。 问总共有多少条不同的路径？ 示例 1： 12输入：m = 3, n = 7输出：28 12345678910111213141516171819class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: \"\"\" - 解题步骤（动规五部曲） &gt;1) 确定dp含义: dp[m][n]: 到达（m，n）的路径数 &gt;2) 确定递推公式 dp[m][n] = dp[m][n-1] + dp[m-1][n] &gt;3) dp数组如何初始化 &gt;4) 确定遍历顺序 &gt;5) 举例推导dp数组 \"\"\" # dp = [[0]*n]*m 注意，不可这样创建二维数组 dp = [[0 for i in range(n)] for j in range(m)] for i in range(n): dp[0][i]=1 for j in range(m): dp[j][0]=1 for i in range(1,n): for j in range(1,m): dp[j][i] = dp[j-1][i] + dp[j][i-1] return dp[m-1][n-1] 解题思路： 322. 零钱兑换给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。 你可以认为每种硬币的数量是无限的。 示例 1： 123输入：coins = [1, 2, 5], amount = 11输出：3 解释：11 = 5 + 5 + 1 示例 2： 12输入：coins = [2], amount = 3输出：-1 示例 3： 12输入：coins = [1], amount = 0输出：0 示例 4： 12输入：coins = [1], amount = 1输出：1 解析：为求最值型动态规划 123456789101112class Solution: def coinChange(self, coins: List[int], amount: int) -&gt; int: res = [0] * (amount+1) for i in range(1, amount+1): # i= 0,1,2...., amount res[i] = float('inf') for coin in coins: if i&gt;=coin: res[i] = min(res[i], res[i-coin]+1) # print(res) return res[amount] if res[amount]!= float('inf') else -1 参考","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"力扣","slug":"力扣","permalink":"http://yoursite.com/tags/力扣/"}],"author":"Dufy"},{"title":"⭐️算法笔记","slug":"算法笔记","date":"2020-04-05T16:00:00.000Z","updated":"2021-05-16T03:52:41.328Z","comments":true,"path":"2020/04/06/算法笔记/","link":"","permalink":"http://yoursite.com/2020/04/06/算法笔记/","excerpt":"[TOC] 数据结构是工具，算法是通过合适的工具解决特定问题的方法。","text":"[TOC] 数据结构是工具，算法是通过合适的工具解决特定问题的方法。 学习一个算法可分为三步 先了解算法本身解决什么问题 然后学习它的解决策略 最后了解相似算法之间的联系 程序员如何准备面试中的算法https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/00.01.html 备战面试中算法的五个步骤 对于立志进一线互联网公司，同时不满足于一辈子干纯业务应用开发，希望在后端做点事情的同学来说，备战面试中的算法，分为哪几个步骤呢？如下： 1、掌握一门编程语言首先你得确保你已掌握好一门编程语言： C的话，推荐Dennis M. Ritchie &amp; Brian W. Kernighan合著的《C程序设计语言》，和《C和指针》； C++ 则推荐《C++ Primer》，《深度探索C++对象模型》，《Effective C++》； Java推荐《Thinking in Java》，《Core Java》，《Effictive Java》，《深入理解Java虚拟机》。 掌握一门语言并不容易，不是翻完一两本书即可了事，语言的细枝末节需要在平日不断的编程练习中加以熟练。 2、过一遍微软面试100题系列我从2010年起开始整理微软面试100题系列，见过的题目不可谓不多，但不管题目怎般变化，依然是那些常见的题型和考察点，当然，不考察任何知识点，纯粹考察编程能力的题目也屡见不鲜。故不管面试题千变万化，始终不离两点：①看你基本知识点的掌握情况；②编程基本功。 而当你看了一遍微软面试100题之后(不要求做完)，你自会意识到：数据结构和算法在笔试面试中的重要性。 3、苦补数据结构基础如果学数据结构，可以看我们在大学里学的任一本数据结构教材都行，如果你觉得实在不够上档次，那么可以再看看《STL源码剖析》。 然后，你会发现：大部分的面试题都在围绕一个点：基于各种数据结构上的增删改查。如字符串的查找翻转，链表的查找遍历合并删除，树和图的查找遍历，后来为了更好的查找，我们想到了排序，排序仍然不够，我们有了贪心、动态规划，再后来东西多了，于是有了海量数据处理，资源有限导致人们彼此竞争，出现了博弈组合概率。 4、看算法导论《算法导论》上的前大部分的章节都在阐述一些经典常用的数据结构和典型算法（如二分查找，快速排序、Hash表），以及一些高级数据结构（诸如红黑树、B树），如果你已经学完了一本数据结构教材，那么建议你着重看贪心、动态规划、图论等内容，这3个议题每一个议题都大有题目可出。同时，熟悉常用算法的时间复杂度。 如果算法导论看不懂，你可以参看本博客。 5、刷leetcode或cc150或编程艺术系列 如主要在国外找工作，推荐两个面试编程网站：一个是http://leetcode.com/ ，leetcode是国外一网站，它上面有不少编程题；另外一个是http://www.careercup.com/ ，而后这个网站的创始人写了本书，叫《careercup cracking coding interview》，最终这本英文书被图灵教育翻译出版为《程序员面试金典》。 若如果是国内找工作，则郑重推荐我编写的《程序员编程艺术》，有编程艺术博客版，以及在博客版本基础上精简优化的编程艺术github版。除此之外，还可看看《编程之美》，与《剑指offer》。 而不论是准备国内还是国外的海量数据处理面试题，此文必看：教你如何迅速秒杀掉：99%的海量数据处理面试题。 此外，多看看优秀的开源代码，如nginx或redis，多做几个项目加以实践之，尽早实习（在一线互联网公司实习3个月可能胜过你自个黑灯瞎火摸爬滚打一年）。 当然，如果你是准备社招，且已经具备了上文所说的语言 &amp; 数据结构 &amp; 算法基础，可以直接跳到本第五步骤，开始刷leetcode或cc150或编程艺术系列。 后记学习最忌心浮气躁，急功近利，即便练习了算法，也不一定代表能万无一失通过笔试面试关，因为总体说来，在一般的笔试面试中，70%基础+ 30%coding能力(含算法)，故如果做到了上文中的5个步骤，还远远不够，最后，我推荐一份非算法的书单，以此为大家查漏补缺(不必全部看完，欢迎大家补充)： 《深入理解计算机系统》 W.Richard Stevens著的《TCP/IP详解三卷》，《UNIX网络编程二卷》，《UNIX环境高级编程：第2版》，详见此[豆瓣页面](http://book.douban.com/search/W.Richard Stevens)； 你如果要面机器学习一类的岗位，建议看看相关的算法（如支持向量机通俗导论（理解SVM的三层境界）），及老老实实补补数学基础，包括微积分、线性代数、概率论与数理统计（除了教材，推荐一本《数理统计学简史》）、矩阵论（推荐《矩阵分析与应用》）等.. 最后望大家循序渐进，踏实前进，若实在觉得算法 &amp; 编程太难，转产品、运营、测试、运维、前端、设计都是不错的选择，因为虽然编程有趣，但不一定人人适合编程。 算法与数据结构算法 渐进记号 时间复杂度： 复杂度 包括时间复杂度和空间复杂度 大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。 从时间复杂度的概念来看，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。 时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。 参考：Know Thy Complexities! 数据结构数据结构是计算机存储、组织数据的方式。对于特定的数据结构(比如数组)，有些操作效率很高(读某个数组元素)，有些操作的效率很低(删除某个数组元素)。程序员的目标是为当前的问题选择最优的数据结构。 数据结构无非就是数组、链表为骨架的一些特定操作而已；每个数据结构实现的功能无非增删查改罢了。【算法学习之路】 代码面试需要知道的8种数据结构(附面试题及答案链接) 知乎：Python列表：初学者应该懂得操作和内部实现 二者联系算法是一系列程序指令，用于解决特定的运算和逻辑问题； 数据结构是数据的组织、管理和存储形式，使用目的是为了高效地 访问和修改数据。 数据结构是算法的基石。如果把算法比喻为舞者，数据结构则是舞者脚下的舞台。不同的算法会选择不同的数据结构 当我们遇到一个实际问题时，首先需要解决两件事：数据结构为什么那么难？每个面试官都会问的问题！ （1）如何将数据存储在计算机中； （2）用什么方法和策略解决问题。 前者是数据结构，后者是算法。只有数据结构没有算法，相当于只把数据存储到计算机中，而没有有效的方法去处理，就像一幢只有框架的烂尾楼；若只有算法，没有数据结构，就像沙漠里的海市蜃楼，只不过是空中楼阁罢了。 数据是一切能输入计算机中的信息的总和，结构是指数据之间的关系。数据结构就是将数据及其之间的关系有效地存储在计算机中并进行基本操作。算法是对特定问题求解步骤的一种描述，通俗讲就是解决问题的方法和策略。 在遇到一个实际问题时，要充分利用自己所学的数据结构，将数据及其之间的关系有效地存储在计算机中，然后选择合适的算法策略，并用程序高效地实现。这就是Niklaus Wirth教授所说的：“数据结构+算法＝程序”。 性能优化改进算法，选择合适的数据结构【5】 一个良好的算法能够对性能起到关键作用，因此性能改进的首要点是对算法的改进。比如，O(1)就比O(n)要好 数据结构 字典 (dictionary) 与列表 (list) Python 字典中使用了 hash table，因此查找操作的复杂度为 O(1)，而 list 实际是个数组，在 list 中，查找需要遍历整个 list，其复杂度为 O(n)，因此对成员的查找访问等操作字典要比 list 更快。 12345678910111213from time import timet = time()list = ['a','b','is','python','jason','hello','hill','with','phone','test','dfdf','apple','pddf','ind','basic','none','baecr','var','bana','dd','wrd']# list = dict.fromkeys(list,'0')print(list)filter = []for i in range (1000000): for find in ['is','hat','new','list','old','.']: if find not in list: filter.append(find)print(\"total run time:\")print(time()-t) [‘a’, ‘b’, ‘is’, ‘python’, ‘jason’, ‘hello’, ‘hill’, ‘with’, ‘phone’, ‘test’, ‘dfdf’, ‘apple’, ‘pddf’, ‘ind’, ‘basic’, ‘none’, ‘baecr’, ‘var’, ‘bana’, ‘dd’, ‘wrd’]total run time:1.9940822124481201 {‘a’: ‘0’, ‘b’: ‘0’, ‘is’: ‘0’, ‘python’: ‘0’, ‘jason’: ‘0’, ‘hello’: ‘0’, ‘hill’: ‘0’, ‘with’: ‘0’, ‘phone’: ‘0’, ‘test’: ‘0’, ‘dfdf’: ‘0’, ‘apple’: ‘0’, ‘pddf’: ‘0’, ‘ind’: ‘0’, ‘basic’: ‘0’, ‘none’: ‘0’, ‘baecr’: ‘0’, ‘var’: ‘0’, ‘bana’: ‘0’, ‘dd’: ‘0’, ‘wrd’: ‘0’}total run time:1.015928030014038 将 list 转换为字典之后再运行，效率大概提高了一半。因此在需要多数据成员进行频繁的查找或者访问的时候，使用 dict 而不是 list 是一个较好的选择。 集合 (set) 与列表 (list) set 的 union， intersection，difference 操作要比 list 的迭代要快。因此如果涉及到求 list 交集，并集或者差的问题可以转换为 set 来操作 set 常见用法 语法 操作 说明 set(list1) | set(list2) union 包含 list1 和 list2 所有数据的新集合 set(list1) &amp; set(list2) intersection 包含 list1 和 list2 中共同元素的新集合 set(list1) - set(list2) difference 在 list1 中出现但不在 list2 中出现的元素的集 对循环的优化 对循环的优化所遵循的原则是尽量减少循环过程中的计算量，有多重循环的尽量将内层的计算提到上一层。 12345678910from time import time t = time() lista = [1,2,3,4,5,6,7,8,9,10] listb =[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.01] for i in range (1000000): for a in range(len(lista)): for b in range(len(listb)): x=lista[a]+listb[b]print(\"total run time:\")print(time()-t) total run time:18.201731204986572 将长度计算提到循环外，同时将第三层的计算 lista[a] 提到循环的第二层。 1234567891011from time import time t = time() lista = [1,2,3,4,5,6,7,8,9,10] listb =[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.01] for i in range(1000000): for a in range(len(lista)): temp = lista[a] for b in range(len(listb)): x=temp+listb[b]print(\"total run time:\")print(time()-t) total run time:17.083163022994995 区别： 充分利用 Lazy if-evaluation 的特性 python 中条件表达式是 lazy evaluation 的，也就是说如果存在条件表达式 if x and y，在 x 为 false 的情况下 y 表达式的值将不再计算。因此可以利用该特性在一定程度上提高程序效率。 12345678910from time import time t = time() abbreviations = ['cf.', 'e.g.', 'ex.', 'etc.', 'fig.', 'i.e.', 'Mr.', 'vs.']for i in range (10000000): for w in ('Mr.', 'Hat', 'is', 'chasing', 'the', 'black', 'cat', '.'): # if w in abbreviations: if w[-1] == '.' and w in abbreviations: passprint(\"total run time:\")print(time()-t) 在未进行优化之前程序的运行时间大概为 12.42，如果使用注释行代替第一个 if，运行的时间大概为 11.1。 字符串优化 1）字符串拼接尽量使用 join()，而不是 “+” 2）当对字符串可以使用正则表达式或者内置函数来处理的时候，选择内置函数。如 str.isalpha()，str.isdigit()，str.startswith((‘x’, ‘yz’))，str.endswith((‘x’, ‘yz’)) 3）对字符进行格式化比直接串联读取要快 123456head= &apos;aa&apos;prologue = &apos;ww..&apos;query = &apos;vv//v&apos;out = &quot;&lt;html&gt;%s%s%s&lt;/html&gt;&quot; % (head, prologue, query) # 推荐out1 = &quot;&lt;html&gt;&quot; + head + prologue + query + &quot;&lt;/html&gt;&quot;out, out1 1(&apos;&lt;html&gt;aaww..vv//v&lt;/html&gt;&apos;, &apos;&lt;html&gt;aaww..vv//v&lt;/html&gt;&apos;) 4) 使用列表解析（list comprehension）和生成器表达式（generator expression） a = [w for w in list] 5) if done is not None 比语句 if done != None 更快 6) while 1 要比 while True 更快（当然后者的可读性更好）； Python内置list数组的两种实现方式，考虑到能容纳任意的元素，且在更换存储区域时，list 对象的标识ID不变，只能采用分离式实现技术 元素外置顺序表如下，可以使得存储的元素大小不一，且有统一公式计算: $$Loc(e_i)=l_0+c*i$$ 链表 首先建立Node： code: 12345678910111213141516class Node: def __init__(self, initdata): self.data = initdata self.next = None def getData(self): return self.data def getNext(self): return self.next def setData(self, new_data): self.data = new_data def setNext(self, new_next): self.next = new_next 12temp = Node(93)temp.getData() # 93 建立UNordered list： 123class UnorderedList(): def __init__(self): self.head = None 添加元素： 1234def add(self,item): temp = Node(item) temp.setNext(self.head) self.head = temp 获取链表元素个数 12345678def size(self): current = self.head count = 0 while current != None: count = count + 1 current = current.getNext() return count 搜索 1234567def search(self, item): current = self.head while current.getData() != item: current = current.getNext() if current == None: return False return True 删除 123456789101112131415def remove(self, item): current = self.head previous = None found = False while not found: if current.getData() == item: found = True else: previous = current current = current.getNext() if previous == None: self.head = current.getNext() else: previous.setNext(current.getNext()) 完整代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class Node: def __init__(self, initdata): self.data = initdata self.next = None def getData(self): return self.data def getNext(self): return self.next def setData(self, new_data): self.data = new_data def setNext(self, new_next): self.next = new_nextclass UnorderedList(): def __init__(self): self.head = None def isEmpty(self, item): return self.head ==None def add(self, data): temp = Node(data) temp.setNext(self.head) self.head =temp def size(self): current = self.head count = 0 while current != None: count += 1 current = current.getNext() return count def search(self, item): current = self.head while current.getData() != item: current = current.getNext() if current == None: return False return True def remove(self, item): current = self.head previous = None found = False while not found: if current.getData() == item: found = True else: previous = current current = current.getNext() if previous == None: self.head = current.getNext() else: previous.setNext(current.getNext()) 创建链表 12345678910111213141516def createLinkHead(li): &apos;头插法&apos; head = Node(li[0]) for ele in li[1:]: node = Node(ele) node.next = head head = node return headdef printLinklist(lk): while lk: print(lk.item, end = &apos;,&apos;) lk = lk.nextlk = createLinkHead([1,2,3])printLinklist(lk) 栈.. 1234567891011121314151617181920212223242526272829class Stack: \"\"\" 栈的实现 \"\"\" def __init__(self): self.stack = [] def pop(self): return self.stack.pop() def push(self, element): self.stack.append(element) def getTop(self): if len(self.stack)&gt;0: return self.stack[-1] else: return None def isEmpty(self): return len(self.stack) == 0stack = Stack()stack.push(1)stack.push(2)stack.push(4)print(stack.pop())print(stack.pop()) 栈的应用 括号匹配问题 12345678910111213141516171819202122def braceMatch(s): &apos;括号匹配问题&apos; match = &#123;&apos;&#125;&apos;:&apos;&#123;&apos;, &apos;]&apos;:&apos;[&apos;, &apos;)&apos;:&apos;(&apos;&#125; stack = Stack() for char in s: if char in [&apos;&#123;&apos;,&apos;(&apos;,&apos;[&apos;]: stack.push(char) else: # ch in [&apos;&#125;&apos;, &apos;]&apos;, &apos;)&apos;] if stack.isEmpty(): return False elif stack.getTop() == match[char]: stack.pop() else: # 栈顶元素不匹配 return False if stack.isEmpty(): return True else: return Falseprint(braceMatch(&apos;[[()]]&#123;&#125;&apos;)) 队列 内置模块 双端队列，即可实现栈，也可实现队列。 12345678910import collections# 创建队列d = collections.deque()d.append(1) #从队尾入队d.popleft() #从队头出队d.appendleft(2) #从队头入队d.pop() #从队尾出队 使用 deque(maxlen=N) 构造函数会新建一个固定大小的队列。当新的元素加入并且这个队列已满的时候， 最老的元素会自动被移除掉。 1234567891011from collections import dequeq = deque(maxlen=3)q.append(12)q.append(1)q.append(142) #---&gt; deque([12， 1, 142])q.append(-19)print(q, '-------')q.popleft()q 12deque([1, 142, -19], maxlen=3) -------deque([142, -19]) 应用： 在某种意义上，这种混合线性结构提供了单个数据结构中的栈和队列的所有能力 e.g. 解决回文问题，如 ‘radar’, ‘toot’字符判断 环形队列 123456789101112131415161718192021222324252627282930313233343536373839class Queue: def __init__(self, size =100): self.queue = [0 for _ in range(size)] self.size = size self.rear = 0 # 队尾指针 self.front = 0 # 队首指针 def push(self, element): if not self.isFull(): self.rear = (self.rear+1) % self.size self.queue[self.rear] = element else: raise IndexError('Queue is filled.') def pop(self): if not self.isEmpty(): self.front = (self.front+1) % self.size return self.queue[self.front] else: raise IndexError('Queue is empty.') def isEmpty(self): '判断队空' return self.rear == self.front def isFull(self): '判断队满' return (self.rear+1) % self.size == self.frontq = Queue(5)for i in range(4): q.push(i)print(q.isFull())print(q.pop())print(q.pop())print(q.pop())print(q.pop()) True0123 哈希表 应用 树 链表构造形式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class BinaryTree: def __init__(self,rootObj): self.key = rootObj self.leftChild = None self.rightChild = None def insertLeft(self,newNode): if self.leftChild == None: self.leftChild = BinaryTree(newNode) else: t = BinaryTree(newNode) t.leftChild = self.leftChild self.leftChild = t def insertRight(self,newNode): if self.rightChild == None: self.rightChild = BinaryTree(newNode) else: t = BinaryTree(newNode) t.rightChild = self.rightChild self.rightChild = t def getRightChild(self): return self.rightChild def getLeftChild(self): return self.leftChild def setRootVal(self,obj): self.key = obj def getRootVal(self): return self.keyr = BinaryTree('a')print(r.getRootVal())print(r.getLeftChild())r.insertLeft('b')print(r.getLeftChild())print(r.getLeftChild().getRootVal())r.insertRight('c')print(r.getRightChild())print(r.getRightChild().getRootVal())r.getRightChild().setRootVal('hello')print(r.getRightChild().getRootVal())r.insertLeft('qq')print(r.getLeftChild())print(r.getLeftChild().getRootVal()) # qqr_left = r.getLeftChild().getLeftChild()print(r_left.getRootVal()) aNone&lt;main.BinaryTree object at 0x101745390&gt;b&lt;main.BinaryTree object at 0x1017453c8&gt;chello&lt;main.BinaryTree object at 0x101745400&gt;qqb #此处 b 移到了插入的qq的左子节点位置 注意,在进行insert插入时候，如果子节点存在内容，则默认移除到下方左子节点，图示如下： 遍历 前序遍历 12345def preorder(tree): if tree: print(tree.getRootVal()) preorder(tree.getLeftChild()) preorder(tree.getRightChild()) 12print(&apos;-&apos;*30)preorder(r) 目录构建 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Node: def __init__(self, name, type = &apos;dir&apos;): self.name = name self.type = type self.children = [] self.parent = None def __repr__(self): return self.nameclass FileSystemTree: def __init__(self): self.root = Node(&apos;/&apos;) self.now = self.root def mkdir(self, name): # name 以/ 结尾 if name[-1] != &apos;/&apos;: name += &apos;/&apos; node = Node(name) self.now.children.append(node) node.parent = self.now def ls(self): return self.now.children def cd(self, name): if name[-1] != &apos;/&apos;: name += &apos;/&apos; for child in self.now.children: if child.name == name: self.now = child return raise ValueError(&apos;invalid dir&apos;)# n = Node(&apos;hell0&apos;)# n2 =Node(&apos;world&apos;)# n.children.append(n2)# n2.parent = ntree = FileSystemTree()tree.mkdir(&apos;var/&apos;)tree.mkdir(&apos;bin/&apos;)tree.mkdir(&apos;usr/&apos;)print(tree.ls())tree.cd(&apos;bin/&apos;)tree.mkdir(&apos;python/&apos;)print(tree.ls()) [var/, bin/, usr/][python/] 遍历 12345678910111213141516171819202122232425262728293031323334353637def pre_order(root): '前序遍历' if root: print(root.data, end= ',') pre_order(root.lchild) pre_order(root.rchild)def in_order(root): '中序遍历' if root: in_order(root.lchild) print(root.data, end= ',') in_order(root.rchild)def post_order(root): '后序遍历' if root: post_order(root.lchild) post_order(root.rchild) print(root.data, end= ',')from collections import dequedef level_order(root): '层次遍历，用队列实现' queue = deque() queue.append(root) while len(queue)&gt;0: #只要队不空 node = queue.popleft() print(node.data, end=',') if node.lchild: queue.append(node.lchild) if node.rchild: queue.append(node.rchild)level_order(root) 二叉搜索树 插入 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273class BST: '二叉搜索树' def __init__(self, li=None): self.root = None if li: for val in li: self.insert_no_rec(val) def insert(self, node, val): '插入操作' if not node: node = BiTreeNode(val) elif val&lt;node.data: self.insert(node.lchild, val) node.lchild.parent = node elif val&gt;node.data: self.insert(node.rchild, val) node.rchild.parent = node else: # 相等情况 pass return node def insert_no_rec(self, val): '非递归形式' p = self.root if not p: # 空树 self.root = BiTreeNode(val) return while True: if val&lt;p.data: if p.lchild: p = p.lchild else: # 左孩子不存在 p.lchild = BiTreeNode(val) p.lchild.parent = p return elif val &gt; p.data: if p.rchild: p = p.rchild else: # p.rchild = BiTreeNode(val) p.rchild.parent = p return else: return def pre_order(self, root): '前序遍历' if root: print(root.data, end=',') pre_order(root.lchild) pre_order(root.rchild) def in_order(self, root): '中序遍历' if root: in_order(root.lchild) print(root.data, end=',') in_order(root.rchild) def post_order(self, root): '后序遍历' if root: post_order(root.lchild) post_order(root.rchild) print(root.data, end=',')print()tree = BST([4,6,7,9,2,1,3,5,8])tree.pre_order(tree.root)print()tree.in_order(tree.root) # 输出有序的print()tree.post_order(tree.root) 4,2,1,3,6,5,7,9,8,1,2,3,4,5,6,7,8,9,1,3,2,5,8,9,7,6,4, AVL树 复杂，目前还没看懂！！https://www.bilibili.com/video/BV1mp4y1D7UP?p=78 B树 堆Python 内置 heapq，建小根堆 12345678# heapq模块可以接受元组对象，默认元组的第一个元素作为priority，即按照元组的第一个元素构成小根堆nums = [2, 3, 5, 1, 54, 23, 132]res = []for num in nums: heapq.heappush(res, num) # 加入堆print(res) # [1, 2, 5, 3, 54, 23, 132][heapq.heappop(res) for _ in range(len(res))] # 堆排序结果 [1, 2, 3, 5, 23, 54, 132] 关于传参元组情况，参见 堆–347 查找最大或最小的 N 个元素 1234import heapqnums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]print(heapq.nlargest(3, nums)) # Prints [42, 37, 23]print(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2] 两个函数都能接受一个关键字参数，用于更复杂的数据结构中： 1234567891011portfolio = [ &#123;'name': 'IBM', 'shares': 100, 'price': 91.1&#125;, &#123;'name': 'AAPL', 'shares': 50, 'price': 543.22&#125;, &#123;'name': 'FB', 'shares': 200, 'price': 21.09&#125;, &#123;'name': 'HPQ', 'shares': 35, 'price': 31.75&#125;, &#123;'name': 'YHOO', 'shares': 45, 'price': 16.35&#125;, &#123;'name': 'ACME', 'shares': 75, 'price': 115.65&#125;]cheap = heapq.nsmallest(3, portfolio, key=lambda s: s['price'])expensive = heapq.nlargest(3, portfolio, key=lambda s: s['price'])print(cheap, expensive) 1[&#123;&apos;name&apos;: &apos;YHOO&apos;, &apos;shares&apos;: 45, &apos;price&apos;: 16.35&#125;, &#123;&apos;name&apos;: &apos;FB&apos;, &apos;shares&apos;: 200, &apos;price&apos;: 21.09&#125;, &#123;&apos;name&apos;: &apos;HPQ&apos;, &apos;shares&apos;: 35, &apos;price&apos;: 31.75&#125;] [&#123;&apos;name&apos;: &apos;AAPL&apos;, &apos;shares&apos;: 50, &apos;price&apos;: 543.22&#125;, &#123;&apos;name&apos;: &apos;ACME&apos;, &apos;shares&apos;: 75, &apos;price&apos;: 115.65&#125;, &#123;&apos;name&apos;: &apos;IBM&apos;, &apos;shares&apos;: 100, &apos;price&apos;: 91.1&#125;] 递归 递归三定律： 递归终止条件 向终止条件迭代靠近 调用自身 e.g. 计算列表和 12345678def sumList(list1): if len(list1)==1: return list1[0] else: return list1[0] + sumList(list1[1:]) list_temp = [1,4,6,8,2]sumList(list_temp) e.g. 比较如下两个递归的区别 12345def fun1(x): if x&gt;0: print(x) fun1(x-1)fun1(4) 汉诺塔问题 123456789def hanoi(n, a, b, c): &quot;将n 个盘子，从a经过b 移到c&quot; pass if n&gt;0: hanoi(n-1, a, c,b) print(f&apos;&#123;a&#125;---&gt;&#123;c&#125;&apos;) hanoi(n-1, b, a, c)hanoi(3,&apos;a&apos;, &apos;b&apos;, &apos;c&apos;) a—&gt;ca—&gt;bc—&gt;ba—&gt;cb—&gt;ab—&gt;ca—&gt;c 递归时间复杂度 利用递归树进行算法复杂度分析： 二分查找 1234567891011121314151617def binarySearch(lis, val): '二分查找' pass left = 0 right = len(lis)-1 while left&lt;=right: # 候选区域有值 mid = (left+right) // 2 if lis[mid]==val: return mid elif lis[mid]&gt;val: # 待查找的值在mid 左边 right = mid-1 else: # 待查找的值在mid 右边 left = mid+1 return Noneli = [1,2,3,4,5,6,7,8,9]print(binarySearch(li, 3)) 时间复杂度：O(lgN) 二分查找与顺序查找的时间比较 123456789101112131415161718192021222324252627282930from cal_time import cal_time@cal_timedef linearSeach(lis, val): \"线性查找\" for index,v in enumerate(lis): if v==val: return index return None@cal_timedef binarySearch(lis, val): '二分查找，在有序的前提下' pass left = 0 right = len(lis)-1 while left&lt;=right: # 候选区域有值 mid = (left+right) // 2 if lis[mid]==val: return mid elif lis[mid]&gt;val: # 待查找的值在mid 左边 right = mid-1 else: # 待查找的值在mid 右边 left = mid+1 return Noneli = list(range(100000000))print(linearSeach(li, 30000000))print(binarySearch(li, 30000000)) cal_time.py 12345678910import timedef cal_time(func): def wrapper(*args, **kwargs): t1 = time.time() result = func(*args, **kwargs) t2 = time.time() print(f&apos;&#123;func.__name__&#125; running time：&#123;t2-t1&#125;secs&apos;) return result return wrapper 排序 不同排序复杂度 十大经典排序算法动画与解析，看我就够了！（配代码完全版） Know Thy Complexities!： 稳定排序与不稳定排序： 数值相同的元素在排序后仍然保持着排序前的顺序，则这样的排序算法是稳定排序； 否则为不稳定排序 冒泡排序 1234567891011121314import randomdef bubbleSort(li): for i in range(len(li)-1): # 趟数，第i趟 for j in range(len(li)-i-1): if li[j]&gt;li[j+1]: li[j], li[j+1] = li[j+1], li[j]li = [random.randint(0,100) for i in range(10)]print(li)bubbleSort(li)print(li)----------------[86, 92, 58, 73, 25, 28, 30, 34, 72, 91][25, 28, 30, 34, 58, 72, 73, 86, 91, 92] 优化 123456789101112def bubbleSort(li): '冒泡排序优化' for i in range(len(li)-1): # 趟数，第i趟 exchange =False for j in range(len(li)-i-1): if li[j]&gt;li[j+1]: li[j], li[j+1] = li[j+1], li[j] exchange=True print(li) if not exchange: return 如果冒泡排序某趟排序没有发生交换，说明列表已经有序，可以结束算法 选择排序 每次外循环i 找寻当前最值，然后与 i 交换 注意，是在第二个 for 循环结束再交换，注意与冒泡比较 123456789101112131415161718192021222324def selectSort(li): '选择排序' for i in range(len(li)-1): # 趟数，第i趟 min_loc = i # min_tag 记录最小位置，进行交换 for j in range(i+1, len(li)): if li[j]&lt;li[min_loc]: min_loc = j li[min_loc], li[i] = li[i], li[min_loc] print(li)li = [1,2,5,4,7,-10, 3]print(li)selectSort(li)print(li)--------------[1, 2, 5, 4, 7, -10, 3][-10, 2, 5, 4, 7, 1, 3][-10, 1, 5, 4, 7, 2, 3][-10, 1, 2, 4, 7, 5, 3][-10, 1, 2, 3, 7, 5, 4][-10, 1, 2, 3, 4, 5, 7][-10, 1, 2, 3, 4, 5, 7][-10, 1, 2, 3, 4, 5, 7] 插入排序 注意:黑色为待排序数字，其前面的数组已经排好序 123456789101112131415161718192021222324def insertSort(li): '插入排序， 类比扑克牌' pass for i in range(1, len(li)): # i表示摸到牌 的下标 tmp = li[i] #交换值使用 j = i-1 # 手里牌的最后一个 while j&gt;=0 and li[j]&gt;tmp: li[j+1] = li[j] # 后移一位 j -= 1 li[j+1] = tmp print(li)li = [1,2,5,4,7,-10, 3]print(li)insertSort(li)print(li)--------------[1, 2, 5, 4, 7, -10, 3][1, 2, 5, 4, 7, -10, 3][1, 2, 5, 4, 7, -10, 3][1, 2, 4, 5, 7, -10, 3][1, 2, 4, 5, 7, -10, 3][-10, 1, 2, 4, 5, 7, 3][-10, 1, 2, 3, 4, 5, 7][-10, 1, 2, 3, 4, 5, 7] 希尔排序希尔排序为插入排序 的变种【1】 1234567891011121314151617181920'''希尔排序'''def shell_sort(lis1): gap = len(lis1) // 2 while gap &gt; 0: for i in range(gap, len(lis1)): j = i while j &gt; 0: if lis1[j - gap] &gt; lis1[j]: lis1[j - gap], lis1[j] = lis1[j], lis1[j - gap] j -= gap else: break print(lis1, '........') # 缩短gap步长 gap //= 2lis1 = [5, 2, 4, 6, 1, 3]shell_sort(lis1)print(lis1) 实现版本2 123456789101112131415161718192021def insertSortGap(li, gap): '插入排序改进， 类比扑克牌' pass for i in range(gap, len(li)): # i表示摸到牌 的下标 tmp = li[i] #交换值使用 j = i-gap # 手里牌的最后一个 while j&gt;=0 and li[j]&gt;tmp: li[j+gap] = li[j] # 后移一位 j -= gap li[j+gap] = tmpdef shellSort(li): d = len(li)//2 while d&gt;=1: insertSortGap(li, d) d //=2li = list(range(1000))import randomrandom.shuffle(li)shellSort(li)print(li) 归并排序【2】 图片：讨厌算法的程序员 7 - 归并排序的时间复杂度分析 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import timestart = time.clock()import mathdef resolve(lis): n = math.ceil(len(lis) / 2) if n &gt; 1: lis = [lis[0:n], lis[n:]] print(lis, '===') temp1 = resolve(lis[0]) temp2 = resolve(lis[1]) if len(temp1) == 2: if temp1[0] &gt; temp1[1]: temp1[0], temp1[1] = temp1[1], temp1[0] if len(temp2) == 2: if temp2[0] &gt; temp2[1]: temp2[0], temp2[1] = temp2[1], temp2[0] lis = merge(temp1, temp2) # lis = temp1 + temp2 # lis = insert_sort(lis) print(lis, '------------') return lisdef merge(left, right): \"\"\"合并两个已排序好的列表，产生一个新的已排序好的列表\"\"\" result = [] # 新的已排序好的列表 i = 0 # 下标 j = 0 # 对两个列表中的元素 两两对比。 # 将最小的元素，放到result中，并对当前列表下标加1 while i &lt; len(left) and j &lt; len(right): if left[i] &lt;= right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 result += left[i:] result += right[j:] return resultdef merge_sort(lis1): lis2 = resolve(lis1) print(lis2)# merge_sort([90, 30, -3, 10, 15, 2, 9, 12, 34, 6, 0, 13, 1, -1, -2, -3, -4, -9])import randomimport time# 产生随机数组all_num = 200000000num = 100000 # 1万排序数组lis1 = random.sample(range(0, all_num), num)merge_sort(lis1)print(time.process_time()) 这里有一个关键点： 在进行归并排序时，通过递归进行拆解，然后通过merge()函数进行融合，merge()函数的书写也成为项目的关键，参考：python归并排序–递归实现 在递归 中调用另一个函数，就复杂度而言，是自己写过的 最复杂的函数 merge()函数理解如下： 123456789101112131415161718192021def merge(left, right): &quot;&quot;&quot;合并两个已排序好的列表，产生一个新的已排序好的列表&quot;&quot;&quot; result = [] # 新的已排序好的列表 i = 0 # 下标 j = 0 # 对两个列表中的元素 两两对比。 # 将最小的元素，放到result中，并对当前列表下标加1 while i &lt; len(left) and j &lt; len(right): if left[i] &lt;= right[j]: result.append(left[i]) i += 1 print(&apos;result:&#123;&#125;, i:&#123;&#125;, j:&#123;&#125;&apos;.format(result,i,j)) else: result.append(right[j]) j += 1 print(&apos;result:&#123;&#125;, i:&#123;&#125;, j:&#123;&#125;&apos;.format(result,i,j)) result += left[i:] result += right[j:] return resultmerge([-3,1,8],[4, 10, 15, 20]) 版本2 归并实现 123456789101112131415161718192021def merge(li, low, mid, high): pass i = low j = mid+1 ltmp =[] while i&lt;=mid and j&lt;=high: if li[i]&lt;li[j]: ltmp.append(li[i]) i += 1 else: ltmp.append(li[j]) j += 1 # while 执行完肯定有一部分没数了 while i &lt;= mid: ltmp.append(li[i]) i += 1 while j &lt;= high: ltmp.append(li[j]) j += 1 li[low:high+1] = ltmp 1234li = [2,4,5,7,1,3,6,8]merge(li,0,3,7)print(li)[1, 2, 3, 4, 5, 6, 7, 8] 程序实现 123456789101112131415def mergeSort(li, low, high): if low &lt; high: # 至少有两个元素，递归 mid = (low+high)//2 mergeSort(li, low, mid) mergeSort(li, mid+1, high) # print(li[low:high+1]) merge(li,low, mid, high) # print(li[low:high+1])li = list(range(1000))import randomrandom.shuffle(li)print(li)mergeSort(li, 0 ,len(li)-1)print(li) 1234567891011121314151617181920[4, 9, 2, 5, 1, 7, 3, 0, 8, 6][4, 9][4, 9][4, 9, 2][2, 4, 9][5, 1][1, 5][2, 4, 9, 1, 5][1, 2, 4, 5, 9][7, 3][3, 7][3, 7, 0][0, 3, 7][8, 6][6, 8][0, 3, 7, 6, 8][0, 3, 6, 7, 8][1, 2, 4, 5, 9, 0, 3, 6, 7, 8][0, 1, 2, 3, 4, 5, 6, 7, 8, 9][0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 快速排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def quickSort(list1): \"\"\" 快排 :param list1: :return: \"\"\" # print(list1) if len(list1)&lt;3: if len(list1) ==2: if list1[0] &gt; list1[1]: list1[0], list1[1] = list1[1], list1[0] return list1 else: pivot = 0 left = 1 right = len(list1)-1 right_stay_tag = True left_stay_tag = True while left != right: while list1[right] &gt;= list1[pivot] and left &lt; right: # 优先移动右 right -= 1 right_stay_tag = False while list1[left] &lt; list1[pivot] and left &lt; right: left += 1 left_stay_tag = False list1[left], list1[right] = list1[right], list1[left] if not right_stay_tag and not left_stay_tag: list1 = quickSort(list1[1:left+1]) + [list1[0]] + quickSort(list1[left+1:]) elif left_stay_tag: #针对left 不移动情况 if [list1[1]] &lt; [list1[0]]: list1 = [list1[1]]+[list1[0]] + quickSort(list1[2:]) else: list1 = [list1[0]] + quickSort(list1[1:]) else: list1 = quickSort(list1[1:]) + [list1[0]] return list1import randomall_num = 20000num = 10000 #1万排序数组list1=random.sample(range(0,all_num),num)# list1 = [8,1,3,5,0]print(list1)a = quickSort(list1)print(a,'@') 方案二，更 简洁 空位、移动指针 12345678910111213141516171819202122232425262728def partition(li, left, right): '获取mid值，同时移动左右指针' tmp = li[left] # 存储第一个位置 while left&lt;right: while li[right]&gt;=tmp and left&lt;right: # 从右边找，比tmp小的数 right -= 1 # 往左移一步 li[left] = li[right] # 将right指针处值 写入到左边空位上 print(li, '&lt;---') while li[left]&lt;= tmp and left&lt;right: left += 1 li[right] = li[left] print(li, '---&gt;') li[left] = tmp # 左右相遇 ----------------li = [5,7,4,6,3,1,2,9,8]print(li)partition(li, 0, len(li)-1)print(li)[5, 7, 4, 6, 3, 1, 2, 9, 8][2, 7, 4, 6, 3, 1, 2, 9, 8] &lt;---[2, 7, 4, 6, 3, 1, 7, 9, 8] ---&gt;[2, 1, 4, 6, 3, 1, 7, 9, 8] &lt;---[2, 1, 4, 6, 3, 6, 7, 9, 8] ---&gt;[2, 1, 4, 3, 3, 6, 7, 9, 8] &lt;---[2, 1, 4, 3, 3, 6, 7, 9, 8] ---&gt;[2, 1, 4, 3, 5, 6, 7, 9, 8] 12345678910111213141516171819202122232425262728293031323334353637383940414243def quickSort(li, left, right): if left&lt;right: # 至少两个元素 mid = partition(li, left, right) quickSort(li, left, mid-1) quickSort(li, mid+1, right)def partition(li, left, right): '获取mid值，同时移动左右指针' tmp = li[left] # 存储第一个位置 while left&lt;right: while li[right]&gt;=tmp and left&lt;right: # 从右边找，比tmp小的数 right -= 1 # 往左移一步 li[left] = li[right] # 将right指针处值 写入到左边空位上 while li[left]&lt;= tmp and left&lt;right: left += 1 li[right] = li[left] li[left] = tmp return left -----------------------------------li = [5,7,4,6,3,1,2,9,8]print(li)quickSort(li, 0, len(li)-1)print(li)[5, 7, 4, 6, 3, 1, 2, 9, 8][2, 7, 4, 6, 3, 1, 2, 9, 8] &lt;---[2, 7, 4, 6, 3, 1, 7, 9, 8] ---&gt;[2, 1, 4, 6, 3, 1, 7, 9, 8] &lt;---[2, 1, 4, 6, 3, 6, 7, 9, 8] ---&gt;[2, 1, 4, 3, 3, 6, 7, 9, 8] &lt;---[2, 1, 4, 3, 3, 6, 7, 9, 8] ---&gt;[1, 1, 4, 3, 5, 6, 7, 9, 8] &lt;---[1, 1, 4, 3, 5, 6, 7, 9, 8] ---&gt;[1, 2, 3, 3, 5, 6, 7, 9, 8] &lt;---[1, 2, 3, 3, 5, 6, 7, 9, 8] ---&gt;[1, 2, 3, 4, 5, 6, 7, 9, 8] &lt;---[1, 2, 3, 4, 5, 6, 7, 9, 8] ---&gt;[1, 2, 3, 4, 5, 6, 7, 9, 8] &lt;---[1, 2, 3, 4, 5, 6, 7, 9, 8] ---&gt;[1, 2, 3, 4, 5, 6, 7, 8, 8] &lt;---[1, 2, 3, 4, 5, 6, 7, 8, 8] ---&gt;[1, 2, 3, 4, 5, 6, 7, 8, 9] 快排与其他排序比较 123456789101112131415161718@cal_timedef _quickSort(li): pass quickSort(li, 0, len(li)-1)import randomimport copyli = list(range(10000))random.shuffle(li)li1 = copy.deepcopy(li)li2 = copy.deepcopy(li)_quickSort(li1)bubbleSort(li2)print(li1)print(li2) 快排缺点 最坏情况：O(n^2） 比如，遇到倒排的列表。这时候，每次第一个位置均是最值，破坏了二分结构，增加了递归的深度—-&gt;n 123li = list(range(10000, 0, -1))_quickSort(li1)print(li1) 报错：RecursionError: maximum recursion depth exceeded in comparison 12345import syssys.setrecursionlimit(100000)_quickSort(li1)_quickSort running time：5.158224105834961secs 程序修改 只需增加三行代码： 12345678910111213141516171819def partition(li, left, right): &apos;获取mid值，同时移动左右指针&apos; li1 = list(range(left, right + 1)) i = random.choice(li1) li[i], li[left] = li[left], li[i] tmp = li[left] # 存储第一个位置 while left&lt;right: while li[right]&gt;=tmp and left&lt;right: # 从右边找，比tmp小的数 right -= 1 # 往左移一步 li[left] = li[right] # 将right指针处值 写入到左边空位上 # print(li, &apos;&lt;---&apos;) while li[left]&lt;= tmp and left&lt;right: left += 1 li[right] = li[left] # print(li, &apos;---&gt;&apos;) li[left] = tmp return left 堆排序 建立堆，“农村包围城市” 123456789101112131415161718192021222324252627282930313233343536373839404142434445def sift(li, low, high): \"\"\"将堆顶元素，调整到合适位置。 假设：节点的左右子树都是堆，但自身不是堆 向下调整 不断与孩子节点最大的比较 :param li: 列表 :param low: 堆的根节点位置 :param high: 堆的最后一个元素位置 :return: \"\"\" i =low # 最开始指向根节点 j = 2*i+1 # j开始是左孩子 tmp = li[low] # 把堆顶存起来 while j&lt;=high: # 只要j 位置有数 if j+1&lt;=high and li[j+1]&gt;li[j]: # 如果右孩子有且较大 j = j+1 # 指向右孩子，此步在于选择最大孩子节点 if li[j]&gt; tmp: li[i] = li[j] i = j # 往↓看一层 j = 2*i+1 else: # tmp更大，直接放到i位置上 li[i]=tmp break else: li[i] = tmp # 把tmp 放到叶子节点上def heapSort(li): n=len(li) for i in range((n-2)//2, -1,-1): # i 表示建堆的时候调整部分的根的下标 sift(li, i, n-1) # 建堆完成 for i in range(n-1,-1,-1): # i指向当前堆的最后一个元素 li[0], li[i] = li[i], li[0] sift(li, 0, i-1) # i-1是新的highli = [i for i in range(100)]import randomrandom.shuffle(li)print(li)heapSort(li)print(li) 堆的内置模块 1234567891011121314import heapq # q--&gt;queue,优先队列import randomli = list(range(10))random.shuffle(li)print(li)heapq.heapify(li) # 建堆print(li)n= len(li)for i in range(n): print(heapq.heappop(li), end=',')print('')print(li) [0, 7, 6, 1, 5, 8, 9, 3, 2, 4][0, 1, 6, 2, 4, 8, 9, 3, 7, 5]0,1,2,3,4,5,6,7,8,9,[] 堆排序—top-k 建立小根堆 先找出前K个数，建堆 然后遍历后面（n-k），比较，出数 12345678910111213141516171819202122232425def topk(li, k): # 建堆 for i in range((k-2)//2, -1,-1): # i 表示建堆的时候调整部分的根的下标 sift(li, i, k-1) # 遍历 for i in range(k,len(li)): if li[i]&gt; li[0]: li[0] = li[i] sift(li,0,k-1) # 出数 for i in range(k - 1, -1, -1): # i指向当前堆的最后一个元素 li[0], li[i] = li[i], li[0] sift(li, 0, i - 1) # i-1是新的high print(li[0:k])li = [i for i in range(1000)]import randomrandom.shuffle(li)print(li)# heapSort(li)topk(li, 10)print(li) 排序总结 计数排序需要先知道待排序列表的数值大小范围 123456789def countSort(li, max_count = 100): &apos;计数排序&apos; count = [0 for _ in range(max_count+1)] for val in li: count[val] += 1 # 得到个数统计 li.clear() for ind,val in enumerate(count): for i in range(val): li.append(ind) 桶排序12345678910111213141516171819def bucketSort(li, n=100, max_num=10000): # n：桶个数， max_num：最大元素值 buckets= [[] for _ in range(n)] # 二维列表，创建桶 for var in li: i = min(var//(max_num//n), n-1) # i：需要放的桶序号，取min 是为了 最大值10000考虑 buckets[i].append(var) # 保持桶内的顺序 for j in range(len(buckets[i])-1, 0, -1): if buckets[i][j]&lt; buckets[i][j-1]: buckets[i][j], buckets[i][j - 1] = buckets[i][j-1], buckets[i][j] else: break sorted_li = [] for buc in buckets: sorted_li.extend(buc) return sorted_li 基数排序 动态规划 参考：五大基本算法之动态规划算法 DP 递归自顶向下，而动态规划则是自底向上 以斐波那契为例： 123456789101112import time# 递归求解def fab(n): if n==1 or n==2: return n else: return fab(n-1) + fab(n-2)time1 = time.time()print(fab(35))print(f'耗时： &#123;time.time()-time1&#125;') 1214930352耗时： 2.4594528675079346 动态规划 12345678910111213import time# 动态规划，自底向上def fab(n): list_temp = [0]*(n+1) list_temp[0] = 1 list_temp[1] = 1 for i in range(2, n+1): list_temp[i] = list_temp[i-1] + list_temp[i-2] return list_temp[n]time1 = time.time()print(fab(35))print(f'耗时： &#123;time.time()-time1&#125;') 1214930352耗时： 0.0003566741943359375 二者耗时是巨大的！ 贪心算法 算法策略 找零问题 分数背包 12345678910111213141516171819goods = [(100,20), (60,10), (120,30)] # (价格，重量)goods.sort(key=lambda x: x[0] / x[1], reverse=True) # 排序def fra(goods,w): money = 0 m = [0 for _ in range(len(goods))] for i, (prize,weight) in enumerate(goods): if weight &lt;= w: m[i] =1 w -= weight money += prize else: m[i] = w/weight money += m[i]*prize break return money, mprint(fra(goods, 50)) # 重量限制 50 (240.0, [1, 1, 0.6666666666666666]) 拼接最大数字 贪心： 比较 a+b之后 12345678910111213141516from functools import cmp_to_keydef xy_cmp(x,y): if x+y &gt; y+x: return 1 elif x+y &lt; y+x: return -1 else: return 0li = [32,94, 128, 1286,6,71]def number_join(li): li = list(map(str, li)) li.sort(key=cmp_to_key(xy_cmp), reverse=True) return ''.join(li)print(number_join(li)) 94716321286128 活动选择问题 贪心策略： 所以，只需一直寻找最先结束的活动即可 12345678910111213141516activities = [(1,4),(3,5), (0,6), (5,7), (3,9), (5,9), (6,10),(8,11), (8,12), (2,14), (12,16)]# 要保证活动是按照结束时间拍好序的activities.sort(key=lambda x:x[1])def activitySelection(a): res = [a[0]] for i in range(1,len(a)): if a[i][0]&gt;= res[-1][1]: # 当前活动开始时间&lt;=最后一个入选活动的结束时间 #不冲突 res.append(a[i]) return resprint(activitySelection(activities)) [(1, 4), (5, 7), (8, 11), (12, 16)] KMP算法七分钟了解什么是 KMP算法 帮你把KMP算法学个通透！（理论篇）B站视频 参考 数据结构与算法（Python语言描述）——完整顺序版🔼 自己知乎–排序🔼 https://facert.gitbooks.io/python-data-structure-cn/content/ 哔哩哔哩视频：清华计算机博士带你学习Python算法+数据结构 Python 代码性能优化技巧","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"底层","slug":"底层","permalink":"http://yoursite.com/tags/底层/"}],"author":"Dufy"},{"title":"⭐️有深度的文章汇总","slug":"有深度的文章汇总","date":"2020-04-04T16:00:00.000Z","updated":"2021-04-06T15:46:03.625Z","comments":true,"path":"2020/04/05/有深度的文章汇总/","link":"","permalink":"http://yoursite.com/2020/04/05/有深度的文章汇总/","excerpt":"[TOC] 思辨","text":"[TOC] 思辨 重读《盐铁论》：国营垄断与民营经济，历代争斗的结局都是怎样？CTO 干什么？（英文）作者原本是一个6人小公司的工程师，后来因为公司业务快速扩张，他被提拔成工程总监（CTO）。他花了三年时间，才搞明白什么是 CTO 的职责，分成产品、流程、人员三部分。 CTO 干什么？: https://www.hashtagcoder.dev/blog/director-of-engineering 工业软件——这是中国与西方差距最大的一个行业，重要程度堪比芯片！为什么被忽略了？https://zhuanlan.zhihu.com/p/94093479 未来的电池，可能是用病毒做的 | 近未来㉘https://www.ifanr.com/1315222 14亿人的战争：中国人用了30年望见计算力的珠峰https://mp.weixin.qq.com/s/gJ_xfexfhIMXr4Cz7sonnw 中国为什么一定要搞新基建？到2030年你就都明白了！https://mp.weixin.qq.com/s/mb0YFqqjQNkJ58D9r60r8w 诸葛越：关于算法工程师职业发展的思考https://mp.weixin.qq.com/s/mV-oXdWRe3Jcs9OJnXbEbw wj第072封信 为什么对“巧合”要保持警惕？划重点 1、如果你理解了巧合背后的规则，就会发现有些我们以为是“巧合”的事情，背后其实有着必然性。 2、在对事情做出判断的时候，最重要的其实还不是你给出的判断本身，而是你所使用的判断方法。 3、在做判断时，事实要优先于我们的猜测。但在得不到事实之前，我们要依靠理性和常识去思考和做出判断。 4、面对那些好得难以置信的事情，记住要想一想，好运气背后是否另有原因。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230设为首页收藏本站开启辅助访问切换到窄版终生学习社用户名自动登录 找回密码密码 登录 立即注册快捷导航论坛请输入搜索内容帖子搜索 热搜:活动交友discuz终生学习社»论坛›订阅专栏›吴军·硅谷来信 3›wj第072封信 为什么对“巧合”要保持警惕？ ...返回列表发新帖查看: 331|回复: 0打印 上一主题 下一主题wj第072封信 为什么对“巧合”要保持警惕？ [复制链接]admin1万主题 1万帖子 4万积分管理员Rank: 9Rank: 9Rank: 9积分46302发消息 电梯直达跳转到指定楼层楼主 发表于 2021-1-25 20:06:17 | 只看该作者 回帖奖励播放 加速 减速当前速度:这里是吴军的《硅谷来信》第3季，这封信我们今天来谈一谈巧合这件事。在线版唯一充值微信：web699250。网络上有一个段子，说明太祖朱元璋是一位化学巨匠，因为明朝的皇室，很多人的名字中都包含一些今天的人才知道的化学物质。比如，明朝有个王爷叫朱慎镭，镭字就是居里夫人发现的那个放射性元素镭；还有一个王爷叫朱均钚，这个钚是核武器的重要原料，也是20世纪才发现的化学元素；另外还有个王爷叫朱悦烯，烯就是近两年热度很高的那种新材料，石墨烯的烯。如果你去翻一翻明代藩王世系表，还能找到很多这样的名字。可是这些元素和物质，大部分都是20世纪才被发现，才有了名称，明朝的皇室们是怎么未卜先知，找到这些字来做名字的呢？这就和朱元璋留下的一条规矩有关。不是巧合的巧合朱元璋是一个特别细致、特别爱替子孙后代操心的人。朱元璋小时候出身穷困，甚至做过乞丐，所以特别担心子孙后代的生计，于是立了一个规矩，规定明朝皇室后代，都不必从事生产劳动养活自己，每年从朝廷领生活费就好。另外，朱元璋的本名叫做朱重八，他父亲名字叫朱五四，爷爷叫朱初一，这些名字都是从出生日期取出来的，显示着朱元璋的家庭处于平民中的最下层。登基做了皇帝之后，朱元璋自然不能再允许自己的后代中有可能再出现这样的名字，于是他又立了一条规矩，把子孙后代名字该怎么起都给规定好了。朱元璋先是给每个儿子甚至侄子都写了一首诗，孙辈往后的名字，名字中的第二个字要从这首诗里面取。第三个字不好定死，但朱元璋也立了规矩，第三个字的偏旁要按金木水火土五行的相生关系来起。比如朱元璋的儿子这一辈，名字都是木字旁，比如太子朱标和燕王朱棣；那么到孙子辈，因为五行木生火，第三个字就应该是火字旁。比如太子的儿子，建文帝朱允炆，炆字就是火字旁；朱棣的儿子，明仁宗朱高炽，炽字也是火字旁。但是，朱元璋没有考虑到一个问题，就是子孙后辈的人数增长的速度非常快，是一种指数增长。到了嘉靖年间，朱氏子孙就已经有接近两万人了。这么多人，都要按朱元璋定的规矩取名字，字是不够用的。特别是中国人还有一条传统，叫做避讳，自己祖辈名字中用过的字，后辈是不能再用的。于是到了明朝中期，包含金木水火土的汉字基本都给朱家用光了。再往后，朱家的王爷们只好自己造字，发明了一堆极其生僻的汉字，除了给自己做名字用，再也没有别的用处了。那么这些字又是怎么变成新发现的化学元素和化学物质的呢？这就要提到清朝末年的一位中国化学家和翻译家，徐寿。当时他和在江南制造局工作的一个英国人傅兰雅一起，翻译了很多西方科技书籍。当翻译到化学著作时，徐寿遇到了一个难点，就是很多化学元素和物质的学名，中国过去大多是没有的，需要起名字。为了规范化，徐寿就定了三条标准：第一，中国古代已经有的名称继续使用，比如金银铜铁等等，再比如水银，古文中叫做汞，也属于这一类。第二，前人翻译西方著作时发明的外来词，合适的可以继续使用，但根据徐寿后面第三条翻译规则做出一些修订。比如之前，氧气、氮气和氢气这三种物质，在翻译中根据它们的特性名字起了名字：氧气原本是“养气”，养是滋养的养；氮气原本是淡气，淡是寡淡的淡，因为氮气在大气中含量高，“冲淡”了氧气的含量；氢气原本是“轻气”，轻重的轻。徐寿沿用了这三个名字，同时根据它们是气体的特性，把字改成了气字头的三个字“氧氮氢”。第三条翻译规则，就是根据元素的的性质和拉丁文读音寻找合适的汉字。例如，金属元素就使用金字旁的字，常态为液体的元素可以使用三点水偏旁的字，非金属元素和物质则使用石字旁的字。于是，在徐寿的规则下，很多化学元素的名字和物质名就和明朝朱氏子孙的名字重叠了。所以，并不是朱元璋和明朝皇室未卜先知，而是他们起名在先，徐寿等人使用这些字在后。表面上是一个巧合，背后其实自有原因。而且如果我们深入一步思考，就会发现这甚至称不上是一个巧合，可以说有一定的必然性。原因很简单，化学元素目前有一百多个，加上一些专有名词，大概需要用到一百五六十个汉字，其中一大半都和金属有关。这里面只有金银铜铁等一小部分是已经有了对应汉字的，其他都要新找一个字来命名。而汉字中，金字旁的字总共也就一百多个，其中很多都是明朝皇室在起名时造出来的生僻字。因此，化学元素名称和朱家人的名字高度重合，可以说是一种必然。可见，如果你理解了巧合背后的规则，就会发现有些我们以为是“巧合”的事情，背后其实有着必然性。用概率来判断巧合与必然通常我们在认知中，怎么判断一件事是巧合还是必然呢？最好用的工具就是概率论。比如抛硬币。假如你把一个硬币抛了10次，全是正面朝上。这是巧合吗？我们怎么来判断这样一件事呢？这时，重要的其实不是你给出的回答，而是你所使用的判断方法。有的人回答说，肯定有问题，但他的依据只是自己的直觉或者日常经验。这个回答就是没有意义的。有的人会回答说，很正常啊，每次硬币朝上朝下的概率都是1/2，十次正面和五次正面五次反面都一样啊，没什么区别，很正常。这里好像比用直觉更进了一步，但仍然是错误的思考。为什么这么说？因为这个回答其实混淆了一个概念，就是究竟什么叫做“一种结果”。实际上，在这个问题中我们所说“一种结果”，在最准确的意义上指的是概率中的“原子事件”，也就是不可以再分的最小的随机事件。比如，十次硬币全都是正面，这是一种结果，也确实是一个“原子事件”。但五次正面五次反面，却并不是一个原子事件，因为其中还包含了“1-5次是正面，6-10次是反面”“单数次是正面，双数次是反面”等等许多种情况，具体来说，里面其实包含了252种原子事件。所以十次正面朝上和五次正面五次反面并不一样，十次正面朝上的概率是1/1024，而五次正面五次反面的概率则是252/1024。相比之下，10次正面朝上确实是一个小概率事件，如果它发生了，这并不能看作是一种完全“正常”的情况。这时候我们就要想一想，背后恐怕有一种力量促使它在发生，而不是简单地将它归结为偶然性。我们可以进一步地通过数学方法去尝试判断，这个硬币有问题的可能性是多大。简单来说就是做两个假设。第一个假设是硬币本身没有问题，是均匀的；第二个假设是硬币是有问题的，怎么扔都是正面朝上。接下来就是根据观察的结果来验证哪一个假设是对的，这在统计中有一套专门的方法。更细节的过程这里就不展开了，我们可以给出一个结论：一枚硬币抛10次，如果连续10次都是正面朝上，这个硬币作了假的概率是99.7%，没有作假的概率只有0.3%。也就是说，对于硬币抛10次10次都是正面这个问题，经过理性的思考后的回答应该是，有99.7%的概率，这不是一个巧合，而是硬币本身有问题。这里特别需要提醒的是，这是我们在“不知道硬币有没有问题”的前提之下，作出的假设和验证，也就是说我们并不知道事实如何。如果在此之前，我们就确认了事实是这枚硬币就是正常硬币，那么我们就要承认，即使只有0.3%的可能性，但这个巧合确实发生了。也就是说，在做判断时，事实要优先于我们的猜测。但在得不到事实之前，我们要依靠理性和常识去思考和做出判断。对巧合保持警惕理性的判断在抛硬币的例子里已经讲得很清楚了，常识的判断又是什么样呢？我之前和你讲过一个故事，有人为了吸引他人投资，同时给几千人发不同的邮件，预测股票的走势，第一次预测之后，他再继续给收到了正确预测邮件的人发第二次预测。这样下来，几千个人里面，总会有少数几个人收到的邮件连续10次预测都是对的。这少数的几个人恐怕会把发邮件的人看成是股神。但实际上，如果真的遇到连续10次都正确这样的小概率事件，我们首先应该想到的是，极大概率背后一定是有什么原因。我们不妨做两个假设，一个是此人真的是股神，第二个假设是对方是骗子。这时候常识就可以派得上用场，你应该想到两点：第一，对方会给你发，也就会给别人发，实际上你不知道他失败了多少次。第二，如果真的是股神，有赚大钱的本事，为什么不自己去投资发财，要花时间来说服你呢？有了常识，我们就足够识破骗子的把戏。今天，人们接触到的信息更丰富，骗子的手法也越来越花样百出。很多人讲，现在的骗子防不胜防。其实我们只要记住一点：面对那些好得难以置信的事情，想一想好运气背后是否另有原因。收藏家马未都先生讲过这样一个故事，一个古董爱好者，有一天让人带着到乡下去淘古董。走过一片田地看见几个人在挖地，正巧一个人一锹土掀到他脚下，随着土挖出一个汝窑瓷器。汝窑瓷器是今天世界上最值钱的瓷器之一，一共只有60多件传世，每一件的来历都清清楚楚，绝大多数都在大博物馆手里。这位老兄运气实在是太好，恰巧从那里路过，恰巧几个农民就挖出一个汝窑瓷器，甚至就掀到了他脚下。这位老兄想买下这件汝窑瓷器，和几个农民讨价还价，最后用自己一辈子全部的积蓄买了下来。回去后他找马未都先生鉴定，马先生怕他太伤心，不敢说是假货，只和他说这东西“不真”。这位老兄怎么也不肯相信自己上了当，明明是亲眼看见那农民碰巧从地里挖出来的。马先生讲，汝窑瓷器全世界60多件，绝大多数都是从古代收藏家手里一代代留下来的，只有个别几件是考古出土的，出土的地方也往往是旧宫殿遗址这一类的地方，不会是田间地头。很多大收藏家花了一辈子时间也不可得的东西，怎么正好有一件，就在你当时当日经过那个地方的时候被挖了出来。显然，这样的巧合背后，极大概率另有原因。相比之下，这个古董爱好者被人盯上，被特意做了个局蒙骗的概率，要远远大于他在田间地头碰巧就得到一件稀世珍品的概率。小结说到这里，你大概也能理解，为什么这封信的标题讲“面对巧合要保持警惕”。面对这类情况，也就是所谓的“超级好运气”，我自己通常的做法是只当它没有发生，我没有看见。不去肖想额外的所得，也就不会有所失，我只需要获得我应得的就好。反过来，对于一些超级坏运气，也不能简单归结为运气坏，有很大概率背后是另有原因，我们需要找到这个原因，才能在之后避免“坏运气”。好了，这封信就说到这里，我们下一封信再见。划重点1、如果你理解了巧合背后的规则，就会发现有些我们以为是“巧合”的事情，背后其实有着必然性。 2、在对事情做出判断的时候，最重要的其实还不是你给出的判断本身，而是你所使用的判断方法。 3、在做判断时，事实要优先于我们的猜测。但在得不到事实之前，我们要依靠理性和常识去思考和做出判断。 4、面对那些好得难以置信的事情，记住要想一想，好运气背后是否另有原因。用户留言甜小姐116 赞1， 人性。 我看很多香港那些讲千术的电影，那些人是利用所谓的“巧合”来达到骗人和获取钱财的目的，更准确地说，是利用人性。有坏的人性，比如“贪婪”，马路上突然出现一笔横财，又很巧地出现了失主来寻找，还很巧地出现了另一个见者有份的人，于是就开始捡钱分钱骗钱的千局，各种调包，真钱假钱等等，利用的就是当事人想占小便宜的心态。当有利益可占时，就算他平时头脑清醒，知道可能会有诈，此刻他也会相信这是真的的。人倾向于相信他希望是真的的事情。 还有的利用是好的人性，比如“情感、义气”。我们以前有个行长都上过当，对方假扮老朋友，恰巧我们行长正好有个朋友也姓王（这种一般就是利用大概率数据行骗，比如王是个很常见的姓），然后我们行长一听朋友有难，又碍于面子不好意思核对真实，非常仗义就一笔钱汇出去了。骗子也知道你这样的职位人有钱，不差这几千块钱。但是还是很难以想象的，因为是一个资深的金融工作从业者。所以受骗的，不只是那些糊涂的老年人，骗子对人性的把控相当准确。 面对贪婪，始终要记得的是，天下没有免费的馅饼，也没有那么多巧合降临在我们身上，背后必定有诈。踏踏实实挣来的，才是让人花的安心的钱财。面对情感，再深，也不能冲动，不能慌了手脚，要镇定要核实清楚。我有时碰到骗子，其实我一开始不知道他是骗子，我以为是真的，但是我是个好奇的人，我的思维习惯就是要弄明白怎么回事。于是，我就问了一大堆问题，骗子被我问得烦死了，编不下去了，最后就把电话挂了。我还纳闷呢，这不是还没解决，怎么就不说了，然后再细细一想，原来是骗子。 2， 认知。除了这些人为制造的巧合，还有很多我们的认知为我们制造的巧合。比如类似于“幸存者偏见”。当你处于某种状态的时候，你就会特别关注某个和自己相关联的现象，然后发现到处都是，从而误以为是巧合。其实平时也一直有，只是你没有注意。比如，你在想某个人的时候，他正好打电话过来，是巧合吗？那是你忽略了更多次数的“”你在想他，他没有打电话过来的”场景，还有“他打电话过来，但是你没在想他的场景”。特殊性往往会集中人们的注意力，从而忽略了其他的普遍现象，从而得出巧合的结论。 看到过一句话， 你赚的每一分钱都是你对世界的认知； 你亏的每一分钱都是你对世界认知的不足； 你永远不可能真正赚到超出你的认知以外的钱。6小时前作者回复谢谢你的分享6小时前左星星84 赞这节课让我想到了前段时间的一条新闻，一位六十多岁的大妈，日子过得不错，只是感情生活很平淡。有一次她在网上遇到了演员靳东，两人聊得挺好，大妈感到自己坠入了爱河，决心跟丈夫离婚，非靳东不嫁…… 那个“靳东”当然是假的，大妈的可悲之处并不在于被骗，而在于她为什么居然能相信，靳东应该娶她呢？ 其实我们不应该去笑话大妈，我们每个人或许都有『奇迹思维』。这不是说这个世界不存在奇迹，世界上有时候就是会发生奇迹，比如买彩票中了亿万大奖。我说的是人们总是不能很好地评估和珍惜奇迹的稀有程度。 爷爷奶奶那一辈人有一个特别朴素的观点，你得有付出才能有回报。而考虑到能量转化过程中的损耗，回报常常会小于付出。特别好的东西需要巨大的付出。 物理学中有个能量守恒定律，这个世界不会凭空“多”出来一个什么东西。每一件事物都一定是从别处移动过来、或者从别的东西转化而成的。 你这里多一个，别处就得少一个，你用了，别人就没有了。 你面对突然天降的横财，就要用能量守恒定律去分析一下，你付出了什么？哪里是不是少了一点什么？这一切符合自然规律吗？ 此前，有个朋友向我推荐一款理财产品，说是投资一个鳄鱼养殖场，鳄鱼皮专供奢侈品公司生产名贵挎包，保证年化收益率10%，还可以签订保底合同。 我想了想还是拒绝了，我想到了罗辑思维里的一段话：“你永远赚不到超出你认知范围外的钱，除非你靠运气。但是，靠运气赚到的钱，最后往往又会靠实力亏掉。”8小时前作者回复四十年前中国有关假张俞事件，情况类似。6小时前李博典42 赞吴军老师好： 感谢老师今日来信，用多个案例与理性思考去更细致解释“巧合”。面对“巧合”，如何判断偶然性的巧合与必然性的巧合很重要。极小概率下的事实，我们必须要承认，这种巧合，确实是存在，也确实是可遇不可求。但更多的是无法确定事实的“巧合”，就需要依靠理性与常识去思考与判断。 老师已用多个案例分析阐明大多数的的“巧合”都是有一定的危机，我们面对这些“巧合”应该保持警惕。老师说的：不去肖想，只取自己应得部分就好。我感觉说得很对，这种思考也算是一种巧合，也是必然的思考模式。面对现实生活中的方方面面，首先要对自己做到不去贪心，你就可以避免很多诈骗。这个思想从小都会被灌输，后来从影视剧，各类书籍，都能感受到其中教导我们的都是这种思考模式。 记得20年前的深圳，很多地方都还是处于待开发状态，到处都是鱼龙混杂的人。那时候寒暑假来到深圳与父母团聚，父母都会告诫我们，不是自己的东西不能要，即使在路上看到钱都别去捡，外面的世界太多太多骗子，没有足够的判断能力，你们能做的就是不能贪心，对陌生人始终保持警惕，不可轻信外人的话。 记得当时父亲也说过一个案例，他朋友和一个陌生人在路边同时看到一块手表，同时想捡起来，后来磋商一块分了，因为是手表，不知道价格几何，就辗转多次最后把手表卖了，两人各分了几百块钱。但后来又因为他的贪心，不知道怎么的最后还是被骗了几千块钱（具体怎么被骗忘了）。他们骗子的套路是一套一套的，我们这些不能识别判断的人，首先要做到的就是不去贪心，不是自己的不要就是了，起码没损失。 祝老师安康。 学生：李博典 2021.01.25 01:309小时前作者回复谢谢你的分享6小时前海绵38 赞我们可以通过渡边淳一著的《钝感力》这本书，来谈谈“巧合”的概念。 其实，我们可以观察自己周围那些取得成功的人，不一定是能力非常卓越的人，但大多是情绪比较稳定的人，他（她）们当然有才能，但在这些才能背后，一定隐藏着有益的钝感力（这个“钝感”指的是“不敏感”）。 ○善于情绪管理 对外界的批评保持钝感而不气馁、还能保持镇定的情绪，相比较遇事急躁、动不动就发脾气的人，肯定前者的升职几率更大，没有一个管理者会希望自己的下属是一个不定时的炸弹。 ○勇于坚持的力量 孤独奋斗的人经常会陷入一时的失败情绪而不能自拔，怀疑自己、怀疑一切甚至怀疑到运气上，但钝感的人会有不断试错的决心，在实践中继续找寻正确的方向。 事实证明，努力找到方向并在一个行业勤耕的人，最终都会有所收获，这是勇于坚持的力量。 所以说，我们看为什么有的人一直那么“巧合”的顺风顺水，不能只关注表面风光，要挖掘其背后具有哪些值得我们学习的品质。 如果说，才能是最基本的必备指标，那么有益的“钝感力”，可能就是能够让才能发扬光大的催化剂。8小时前作者回复渡边淳一著的《钝感力》这本书，～～～这本书最近很热6小时前涂立文27 赞信中提到一个古董爱好者购买一个假汝窑瓷器，除了出现在田地间地点不对之外，还有两点可疑，一是汝窑瓷器在这田地里怎么得以完整保存，以及何以农民随意挖掘都没有损坏。二是普通农民哪知汝窑瓷器价值连城，大概率能卖几千上万他也觉得是飞来横财了，不至于真要古董爱好者花一生积蓄才出手。 《水浒传》里，有人跟在林冲背后叫卖宝刀（这是真宝刀），正常来说也不可能普通人在普通闹市街头卖真宝刀，还冲你叫卖，但林冲也是上了“巧合”的当，因此改变了人生。 面对巧合要保持警惕： 1.天上不会掉馅饼，掉了自己脑袋太小也接不住。 2.不要有贪念。 3.重点：学会换位思考。 例如，有人向你推销价值10万元的古董，说急需用钱5000元贱卖给你。 你站在卖家角度想一想：他什么事真的急着现在卖这点钱？不能等等或没有别的方式了吗？如果还有别的卖高价的可能性，就是有问题。 谢谢吴老师！10小时前作者回复林冲的故事很典型6小时前Allelujah 朱磊35 赞这是一段我哥和银行理财产品经理之间的对话： “你好，我们机构有年化利率XX%的保底理财产品，请问您要购买多少金额的呢？” “没钱。” “不需要很大额的资金，最低投入金额只需要XXXX元就可以了，这个收益率真的很难得。” “真的没钱。要不这样吧，你借钱给我买，赚了算我的，亏了算你的。” “滴滴滴…”10小时前Scenery30 赞万维钢老师曾经把迷信总结为四句话：在没有道理的地方寻找道理，在没有意义的地方找到意义，在没有规律的地方发现规律，在没有因果的地方强加因果。这样的迷信何尝不是『巧合事件』本身。 世上的万事万物都有联系，一次巧合或许是巧合，两次以上的巧合就可能是预谋。当人们对事情了解的越少，就越想要创造出一个模式或者编造一个故事来进行解释，而且还会觉得，如果自己这样做了，就更容易成功。 面对巧合 ①对自己严格要求，避免“巧合事件”。比如在公司，遇到一些倒霉事，有些员工会说，“为什么偏偏发生在我的身上”，觉得这是一种巧合，就给自己找解释来说明本是自己的错误。但很多时候我们反过来想，是啊，为什么每次出现问题都是你呢。正面问题，那一定是自己做的不够好，应该反思自己，避免以后再出现这样的『巧合』。 ②对外界严格警惕，永远相信没有什么事是无缘无故发生的，天下也不会有掉馅饼的事，但会有“碰瓷”。一些高级的碰瓷者先自残式的给自己造成伤害，然后趁司机不注意制造自己被车辆碾压或碰撞的假象，并对“目标司机”进行勒索。面对这种巧合，我们一定多留点心眼，实在不行直接报警。3小时前舒适蜗蜗26 赞谢谢吴老师分享和启发！ 今天的讲授让我从理性角度重新认识了“巧合”这件事儿，天上不会掉馅饼幸好砸中了我。记得吴老师原来也讲过，运气是个常量。我确实也相信这样的说法。我觉得如果好运气就是这么巧合地选中了我，比如我中了彩票，估计我会很不踏实，我得拿出来做一份对社会有益的事业，回馈社会，才能抵消已经获得的这份好运，从而给未来保留获得新的好运的机会。 关于巧合，我原来有件事儿一直没想通，后来慢慢总结出了其中的奥妙，不知对不对！我举个例子:常言道，这人啊，经不起念叨，一念叨就见面。我偶尔也亲身经历这种事儿，特别是有一次，我脑子里正在想一位几年都没见面的人，想着想着，瞬间我的手机响了，天啊，是这个人给我打的电话！我真是惊讶到不行。那几天，我经常和别人讲这个神奇的经历。 后来，我渐渐发现，是不是我过于大惊小怪了。我经常都会在脑海里考虑到某些事儿、想到某些人，而我脑子里想到的人，在接下来的时刻都没有和我遇见或给我打电话啊。而我这样不停地想张三、李四、王五等人，早晚会发生一次这样的“巧合”，所以这应该是个大概率事件，而这种“巧合”不幸被我歪曲成了一个神奇的事！ 所以我还是坚信:天上不会掉馅饼，脚踏实地靠自己，他买他的比特币，我就傻傻做宽基。 祝吴老师和同学们新的一周顺利！9小时前作者回复很赞赏你的心态6小时前小熊25 赞太喜欢吴军老师这封信啦，尤其是汝窑瓷器的故事，越嚼越百感交集。虽然为例子里的人难过，但是自己又不厚道的笑出声来，惭愧惭愧。 而关于“好运气”的思考角度，吴军老师提出的“概率论”概念，绝对是要无时无刻回荡在心中，保护自己不受骗的不二法宝。在这个基础上，还有两个思考，可以在自己心里出现“要不要赌一把，万一我真的好运气呢”的想法时，作为思考的参考： 首先，从吴军老师67封信聊到的“风险”角度上看，花太大成本去赌“好运气”，会让自己进入无法控制的风险区。毕竟“好运气”是极小概率事件，如果穷尽所能去赌这个“好运气”，没赌上，可能会完全失去过去很多年的积累。从规避风险的角度上看，如果一个“赌局”的成本高于可承受范围内，即使“看起来还挺可能的”，也还是不要参与的好。 其次，从刘润老师商学院课程中聊到的“心理账户”角度上看，我们即使遇到了“好运气”，因为并没有在追求中作出任何有实质性的努力，这部分的钱，恐怕会被归入自己和身边人“意外所得”这个心理账户：自己和身边人对这个钱没有很强的所有权感，钱因此很容易被花掉（或者被某些人要走/借了不还/帮忙花干净）。也因此，在美国跟踪中奖者的研究中，因为花钱习惯和人际关系被这个”好运气“打乱，鲜少有人最后有善终。这么说来，即使“好运气”不是骗局，也不是每个人，都有处理这个意外的能力的。 结合二者来说，与其汲汲追求小概率的“好运气”，还不如让自己的资源和精力，集中到提升自己能力、赚“自己可以妥善安排的钱”。7小时前作者回复你理解了我的意思～～～ 结合二者来说，与其汲汲追求小概率的“好运气”，还不如让自己的资源和精力，集中到提升自己能力、赚“自己可以妥善安排的钱”。6小时前凡五24 赞打从疫情以来，留学生群体内的“杀猪盘”猖獗了不少，有些“杀猪盘”类似于PUA，被骗几十万的都有。 “杀猪盘”是放长线钓大鱼型，他们会和你聊很久，也很懂人心，恨不得每个都是心理学大师，一点点瓦解你的防备。 有不少人觉得自己很幸运，能在网上找到这样的“知己”，殊不知这只是一套组合拳里的一部分。有的“杀猪盘”甚至把别墅豪车什么的，一条龙全给你备好，入世未深的学生/总想着自己可以撞大运的人们，就这么上当了。 而且这个“行业”发展还特快，“分支”可多了，人设从高富帅白富美到平平无奇普通人。 我有一个同学，偶然认识一个“网友”，聊得那是一拍即合，真的把对方当做了好朋友，聊了几个月，那人突然来了一句 — 你需要代写吗？7小时前作者回复偶然认识一个“网友”，聊得那是一拍即合，～～～这样的巧合就有问题6小时前李岳洋19 赞#为什么对“巧合”要保持警惕# 因为近似的正确好过精确的错误。 模糊的正确，好过正确的模糊。 为什么呢？ 因为一个正确的方法得到一个错误的结果，比用一个错误的方法得到一个正确的结果要好得多。 每个人遇到问题的时候，因为解决问题的“高频方式”不同，所以也有了不同的“高频问题”。 比如：牛市总结出来的经验通常可以在熊市帮自己亏更多钱。 看书，学习，思考都是在提升成功的概率，提升运气的运气，而好过一直平庸的轮回。 ●如何提升运气的运气？我来个例子。 2020年参加公司组织“1024程序员欢乐节”，我和一个健身的哥们参加“二人三足”绑腿跑的节目。 赛前，我们开始商讨战略： 1、不第一组参加，作为第二组在旁边先演练下，做好准备。 2、先迈出绑的脚为1，未被绑的脚为2，121212，拉齐认知。 3、把绳子绑高一些，快到膝盖处，这样被限制的程度小一些。 4、拆解如何转弯最快——最后以我未被绑的脚为中心，他一步跨过来。 5、站在起跑线的时候，我机智的想到：把被绑住的脚后撤一步，这样可以比别人提前“半步”。 最终我们赢得了第一名，奖品“榨汁机”。 为什么我们得奖了，因为我们的目标就不一样，有的是参与，有的是为了得奖，我们是为了奔着第一名的奖品去的，并且通过一个一个小步骤提升“运气的运气”，做成了。4小时前Hanskeng 尹桂政18 赞用理性的思维去思考问题，需要一定的理论指导，不过在真实生活中却是非常的好用，就比如说公司地下停车场停车的问题，如果公司有十个车位，在八点上班，在8点5分的时候，你能拿到车位的概率是多少？ 考虑这种问题，需要我们了解泊松概率，然后根据小概率事件发生的期望和分布图去推断特定的场景。 真实生活中，我们可以修炼把决策树思维用得如同呼吸、吃饭一样平常，用以指导自己的行为。这样我们每一步做事都是理性的，不会偏差太多的。但在某些事情上，我倒不推荐什么事情都去用概率指导行动，毕竟，一些艺术性的抉择不是靠概率得出的。11小时前伪装15 赞小的时候最喜欢听那些“巧合”的故事，无巧不成书嘛，但是越长大就越是明白，这个世界没有白来的好处，所有的“巧合”背后都有一个代价，即便是这个巧合没有欺骗，就是你运气好，但是你依旧要为此付出代价。 还在上学的时候，我在走了没有一千遍，也有八百遍的宿舍楼门口捡了五百块钱，没有等到失主，就自己昧下了，过了一个月的好生活，那时候我一个月的生活费才两百就够了，一下翻了三倍多，真是有点花天酒地的感觉。 代价很快就来了，接下来的一年里，我不得不面对月月花超生活费的后果，父母虽然多给了一些，但是也没多多少，又因为思想保守不准我出去打工，那一年里总是被“钱不够”折磨，直到我想明白了，钱不是自己的千万不要去碰这个道理，而这也成为了我此后坚持原则。6小时前作者回复谢谢你分享自己亲身经历6小时前日光微夏15 赞天上不会掉馅饼，当一切都发生得太过巧合，就有可能是套路，我们要用常识去分辨其中的真伪，若非机缘，就是预谋。 我很喜欢苏轼的一句话：且夫天地之间，物各有主，苟非吾之所有，虽一毫而莫取。 任何东西都是有成本的，我们想要的得靠我们自己去争取！8小时前林染13 赞如果你理解了巧合背后的规则，就会发现有些我们以为是“巧合”的事情，背后其实有着必然性。 重要的其实不是你给出的回答，而是你所使用的判断方法。 在做判断时，事实要优先于我们的猜测。但在得不到事实之前，我们要依靠理性和常识去思考和做出判断。 面对那些好得难以置信的事情，想一想好运气背后是否另有原因。 不去肖想额外的所得，也就不会有所失，我只需要获得我应得的就好。9小时前KING13 赞保持一个合理的警惕性，其实骗子的招式再多；你只要终止和控制自己的贪欲和贪婪的心，任由他巧舌如簧我自岿然不动。记住天上不会掉馅饼，反而有可能会掉铅球；自己自然而然就会警惕多了。10小时前Masir12 赞感谢老师来信~ 天下没有免费的午餐！ 正如老师所讲，每次有好运来临的时候都需要警惕。因为上帝的公平的，它给了你一面，必然会拿走另一面。 也许你的好运最后要用同等的痛苦来抵消！ 最好的态度是通过自己的努力换来同等的收获，这才是人生的意义！10小时前任勇11 赞吴军老师，您好： 2019年春节期间，“疟疾抗癌”事件在网络上爆红，中科院广州生物医药与健康研究院的陈小平研究员，在中科院SELF论坛一场公开演讲里介绍了他的研究工作：利用疟原虫成功治疗晚期癌症患者。这项成果起源于他早年学习期间，偶然发现世界范围内疟疾发病率和癌症死亡率的地理分布图，似乎存在负相关的关系，萌生了“疟疾也许能治癌”的初步想法。 他展示的疟疾发病率与全球肿瘤死亡率的分布图中，非洲，南亚，东南亚等疟疾高发地区的癌症总死亡率具有相关性，但是在他的演讲中对两个问题没有很好的答案，一个是这些疟疾高发地区，是全球的贫困落后地区，经济落后，生活环境恶劣，医疗水平低，人均寿命非常低，这些地区癌症总死亡率低是不是因为平均寿命低呢？另外一个问题是，在疟疾高发地区，人民普遍服用抗疟药，以青蒿素、奎宁为主的这些治疗疟疾的药物，是不是也有一定的治疗癌症作用呢？这个巧合的背后，如果还存在很多未解的疑问，因为这个巧合得出的结论，肯定是一个不正确不严谨的结论。 今天来信得到： 1，如果你了解巧合背后的规则，就会发现有些我们以为是“巧合”的事情，背后其实有着必然性。 2，判断一个事情是不是巧合，重要的不是你给出的答案，而是所使用的的判断方法，概率论是最好用的工具。 3，在做判断时，事实要优先于我们的猜测，但在得不到事实之前，我们要依靠理性和常识去思考和做出判断。 4，面对那些好的难以置信的事情，想一想好运气背后是否另有原因； 5，对于一些超级坏运气，不能简单的归结为运气坏，有很大概率是另有原因，我们需要找到这个原因，才能在之后避免“坏运气”。 感谢吴军老师！ 祝： 冬安！ 任勇9小时前作者回复谢谢你的分享，这和过去吃鲨鱼软骨6小时前伪装10 赞“巧合”其实无处不在，我们每一个人的出生都是一个堪称奇迹的“巧合”，但这是一个“自然巧合”，是这个世界演化过程中的偶然，不必然出现，但是出现了就是必然的结果。 我们是把“自然巧合”当成这个世界的一些基础认知来看的，毕竟我们每个人出生都不必去追溯那亿万分之一的概率，刮风下雨只看结果就行，天气预报也未必就能准了。 但是“人为巧合”却是最能够迷惑我们，甚至是我们最需要警惕的东西，一旦你面对极度有诱惑的“人为巧合”时，你最需要的是保持平常心，问问自己何德何能，能够遇到这等好事，后面是不是有一个大坑在等你去跳？ 面对“人为巧合”，只要你有一点概率思维，有点数据思维，都可以以最快的速度发现其中的问题，甚至可以辨明哪些是真的机会，哪些是披着机会外衣的危险。6小时前作者回复刮风下雨只看结果就行，天气预报也未必就能准了。～～有些事确实只看结果就行6小时前日辰之龙9 赞说一个反过来的案例，就是对于一些超级坏运气，也不能简单归结为运气坏的情况。这个反面案例也非常值得在做投资的同学记住，这是本人花“学费”交出来的。 我在一个朋友的股权基金里投过4个项目，其中3个都是不成功的，另一个还在“孵化”中。对于项目失败的原因，表面上看：一个是企业投资后挪用了资金，一个是碰上疫情经营受挫，一个是股东矛盾影响发展。在和管理人反复了解沟通后，给了我这么一个解释：“股权投资本来就是高风险、高收益的事，投项目10个里能成功1～2个，这是行业是普遍现象。所以，你的项目不成功也是正常的，因为你的运气不好，刚好在2%的成功概率之外。” 乍一听，特别有道理，一点毛病都没有。其它一起投资的朋友都点头数说：“嗯，看来是我运气不好，那要再多投几个项目，比如10个，这样就能有成功的概率，一个成功后就能把其他成本都收回来了。” 正如老师所说，有很大概率背后是另有原因，我们需要找到这个原因，才能在之后避免“坏运气”。 当时我总是觉得这个听起来“很有道理的”理由，总有哪里不对劲，就在得到学习了大量的投资课后，终于理清了思路。 1、管理人的2%成功项目的成功原因是“CEO的个人资源好”，有朋友的关系牵线+管理人的敏锐度分析。 2、至于基金的后期管理，那基本是没有的。因为第一不是大股东没有说话权；第二因为投资金额对公司总股本来说没那么大，连一个董事席位也没有；第三没有产业对接，仅靠一些人脉资源的介绍，对企业只能是锦上添花，还是没有说话权。 3、如果主要是“我运气不好”的因素，那买彩票做投资就好了，为什么还要找人“管理”？ 4、继续投10个项目一定有2%成功的概率吗？我想大概率是“没有可能”，所以就停止一切这类的投资行为，理由很简单： ①管理人个人资源好的项目，那在他心里会有较大把握，这类项目一定是给那些对他关系重要的人，而这个人肯定不是我。 ②后期管理形同虚设，是目前股权投资的普遍现象，还美其名约“不能限制创业者的创业激情”。搞得人性的贪婪在资本市场里都能治愈一样，这就是一个掩耳盗铃的逻辑。 ③既然是投资就是股东，但是股东的作用是不一样的，一类是“财务投资”，一类是“产业投资”。前者就是“只出钱不出力”，后者是“又出钱又出力”，所以前者失败的概率一定比后者高。 ④如果项目失败了，管理人获得了经验，但是学费却是我交的………那还不如自己做这类投资，即使交了学费还学习了知识。6小时前作者回复谢谢你分享自己的情况6小时前以上留言由 作者 筛选显收藏收藏回复举报返回列表发新帖高级模式BColorImageLinkQuoteCodeSmilies您需要登录后才可以回帖 登录 | 立即注册本版积分规则发表回复 回帖后跳转到最后一页终生学习社GMT+8, 2021-2-17 21:00 , Processed in 0.069306 second(s), 17 queries .Powered by Discuz! X3.4© 2001-2017 Comsenz Inc. IPU芯片–飞越舒适区布局未来（IPU for AI）http://news.modernweekly.com/lead/35601 关于 IPU 我们想制造一台超高性能的计算机，利用低精度来处理数据 也就是说，Graphcore正在为计算机开发的大脑将像人类一样处理信息，而不是通过大量的数字运算来模拟人类 与成熟的CPU（中央处理器）、GPU（图形处理器）相比，人工智能应该需要一块专属芯片——这个想法让全球创业企业看到了挑战巨头的机会。 计算的极限https://fwjmath.wordpress.com/recommended-list/ 参考 我的语雀","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[{"name":"阅读","slug":"阅读","permalink":"http://yoursite.com/tags/阅读/"}],"author":"Dufy"},{"title":"⭐️宗成庆《统计自然语言处理》","slug":"宗成庆《统计自然语言处理》","date":"2020-03-12T16:00:00.000Z","updated":"2021-02-27T06:22:43.462Z","comments":true,"path":"2020/03/13/宗成庆《统计自然语言处理》/","link":"","permalink":"http://yoursite.com/2020/03/13/宗成庆《统计自然语言处理》/","excerpt":"[TOC]","text":"[TOC] 书籍章节解读 NLP定义 nlp研究方法：规则与统计在nlp中存在两种研究思路：基于规则和基于统计的方法。 二者有其相应的哲学基础。 主义关系的问题 出发点不同 规则 什么是基于规则？ 代表人物是乔姆斯基，《语言学理论的逻辑结构》 代表技术 统计 优缺点比较 如何看待？ 所以，统计自然语言处理的经验主义方法只是NLP的一个方面，除此还存在基于规则的理性主义方法。我们应全面学习，取长补短，相得益彰。 nlp面临的困难 歧义消解 鲁棒性 概率论 概率 最大似然估计 条件概率 贝叶斯法则 P(A)的计算： e.g. “e.g.和i.e.是英语中两个容易混淆的拉丁语缩写。前者是exempli gratia的缩写，意思是”for example”（举例），后者是id est的缩写，意思是”that is”（即）。” 二项分布 1234567891011121314151617#-*- coding:utf-8 -*-import numpy as npimport matplotlib.pyplot as pltimport mathfrom scipy import statsn = 20p = 0.3k = np.arange(0,n+1)binomial = stats.binom.pmf(k,n,p)print(binomial)plt.plot(k, binomial, 'o-')plt.title('binomial:n=%i,p=%.2f (www.jb51.net)'%(n,p),fontsize=15)plt.xlabel('number of success（脚本之家测试）',fontproperties='SimHei')plt.ylabel('probalility of success', fontsize=15)plt.grid(True)plt.show() 信息论 熵, 是不确定性的测度 e.g. 噪声信道模型【1】 补充：自适应模型的提出 自适应模型包括： 基于缓存的语言模型 基于混合方法的语言模型 基于最大熵的语言模型 n元语法 数据平滑 注意与👆例子进行比较 HMM–隐马尔科夫模型 参考 噪声信道模型zz","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"http://yoursite.com/tags/nlp/"}],"author":"Dufy"},{"title":"⭐️编程规范","slug":"编程规范","date":"2020-02-22T16:00:00.000Z","updated":"2021-05-03T15:48:13.524Z","comments":true,"path":"2020/02/23/编程规范/","link":"","permalink":"http://yoursite.com/2020/02/23/编程规范/","excerpt":"[TOC] 任何一个傻瓜都能写出计算机可以理解的代码，惟有写出人类容易理解的代码，才是优秀的程序员。—–Bob叔 在代码整洁的道路上要不断修行！","text":"[TOC] 任何一个傻瓜都能写出计算机可以理解的代码，惟有写出人类容易理解的代码，才是优秀的程序员。—–Bob叔 在代码整洁的道路上要不断修行！ 编程大师Martin Fowler先生曾经说过：“代码有很多种坏味道，重复是最坏的一种！”，要写出高质量的代码首先要解决的就是重复代码的问题。 如何编写高质量代码高质量代码具有以下几个特点： 可读性高 结构清晰 可扩展（方便维护） 代码风格统一 低复杂性 简练 代码的马斯洛金字塔马斯洛金字塔是美国心理学家马斯洛提出的一个心理学模型，认为人类的心理需求从下往上分为5个层次，一旦实现了下层的需求，就会追求上一层的需求。 这五个层次依次是：生理需求、安全需求、社交需求、尊严需求、自我实现。 代码质量也可以用金字塔模型表示，从下往上有五个层次。 （1）第一层：正确（Correct） 代码是否执行预期的工作？是否考虑了边缘情况？是否经过充分测试？是否有可维护性？是否有可接受的性能？ （2）第二层：安全（Secure） 代码是否存在漏洞？数据是否安全存储？个人识别信息（PII）是否得到正确处理？是否对用户的输入进行了全面的验证？ （3）第三层：可读（Readable） 代码是否易于阅读和理解？测试足够简洁吗？变量、函数、类的名称是否适当？使用领域模型是否可以清晰地映射现实世界，以减少认知负担？是否使用一致的编码风格约定？ （4）第四层：优雅（Elegant） 代码是否利用了众所周知的编程模式？能否更简单简洁地实现所需的功能？你会为编写这些代码感到兴奋吗？你为这些代码感到骄傲吗？ （5）第五层：利他主义（Altruist） 别人能否从这些代码学到东西？这些代码是否会激励其他工程师进行改进？它是否会让世界变成一个更好的地方？ 编程规范 https://geosoft.no/ Google 开源项目风格指南 (中文版) https://python-web-guide.readthedocs.io/zh/latest/index.html https://www.zhihu.com/question/269154135 https://github.com/alibaba/p3c https://mp.weixin.qq.com/s/QytibJl3XZcC48gcGTZMhQ 编写 clean 代码 但是，我们如何从简单的编写代码过渡到编写整洁的代码？Martin认为，我们有两大途径持续重构和测试驱动开发（TDD），它们就像硬币的两面，具有相互作用。以下是一些定义： 重构(refactor)就是在不改变外部行为的情况下重构现有计算机代码的过程 测试驱动的开发是一个过程，在这个过程中，需求被转换成特定的测试用例，然后只编写使测试通过的功能代码。 python规范代码整洁之道-编写 Pythonic 代码 5分钟了解《代码整洁之道》精华 命名给变量（事实上应该是所有的标识符）命名时做到见名知意也是非常重要的。 变量名，使用下划线_ 连接，小写 123names = \"Python\" #变量名 namejob_title = \"Software Engineer\" #带有下划线的变量名populated_countries_list = [] #带有下划线的变量名 函数命名，小写+大写【参考】 12def getData(): pass 要依据最后返回的数据类型来确定函数起始命名 e.g. 返回bool，以 isFinish…..() 有数据返回：get….()开头 执行操作：以set…()【设置】, perform….()【执行】开头等 类，首字母全大写 1class AndGate(BinaryGate): 常量，使用大写 123TOTAL = 56TIMOUT = 6MAX_OVERFLOW = 7 代码表达式e.g.（注意异常情况的判断） 注意，下面的代码不推荐 users = [ {“first_name”:”Helen”, “age”:39}, {“first_name”:”Buck”, “age”:10}, {“first_name”:”anni”, “age”:9}]users = sorted(users, key=lambda user: user[“first_name”].lower())print(users) 原因： 不容易理解 异常情况的处理不完善 推荐： 12345678910111213141516171819users = [ &#123;\"first_name\":\"Helen\", \"age\":39&#125;, &#123;\"first_name\":\"Buck\", \"age\":10&#125;, &#123;\"first_name\":\"anni\", \"age\":9&#125;]def getUserName(users_element): \"\"\"Get name of the user in lower case\"\"\" return users_element[\"first_name\"].lower()def getSortedDictionary(users): \"\"\"Sort the nested dictionary\"\"\" for users_element in users: if not isinstance(users_element, dict): raise ValueError(\"Not a correct dictionary\") if not len(users): raise ValueError(\"Empty dictionary\") users_by_name = sorted(users, key=getUserName) return users_by_nameprint(getSortedDictionary(users)) [{‘first_name’: ‘anni’, ‘age’: 9}, {‘first_name’: ‘Buck’, ‘age’: 10}, {‘first_name’: ‘Helen’, ‘age’: 39}] 如您所见，此代码检查了所有可能的意外值，并且比起以前的单行代码更具可读性。 单行代码虽然看起来很酷节省了行，但是会给代码添加很多复杂性。 但是这并不意味着单行代码就不好 这里提出的一点是，如果你的单行代码使代码变得更难阅读，那么就请避免使用它，记住写代码不是为了炫酷的，尤其在项目组中。 注释 . 结尾 间隔一空行 123456789101112131415def call_weather_api(url, location): \"\"\"Get the weather of specific location. Calling weather api to check for weather by using weather api and location. Make sure you provide city name only, country and county names won't be accepted and will throw exception if not found the city name. :param url:URL of the api to get weather. :type url: str :param location:Location of the city to get the weather. :type location: str :return: Give the weather information of given location. :rtype: str \"\"\" 类型注解，进一步提高对函数的理解 敏捷开发敏捷开发入门教程 含义：迭代开发+增量开发 迭代开发将一个大任务，分解成多次连续的开发，本质就是逐步改进。开发者先快速发布一个有效但不完美的最简版本，然后不断迭代。每一次迭代都包含规划、设计、编码、测试、评估五个步骤，不断改进产品，添加新功能。通过频繁的发布，以及跟踪对前一次迭代的反馈，最终接近较完善的产品形态。 具体来说，每次迭代都必须依次完成以下五个步骤。 需求分析（requirements analysis） 设计（design） 编码（coding） 测试（testing） 部署和评估（deployment / evaluation） 所谓”增量开发”，指的是软件的每个版本，都会新增一个用户可以感知的完整功能。也就是说，按照新增功能来划分迭代。 敏捷开发的价值观 宣言链接：https://agilemanifesto.org/iso/zhchs/manifesto.html 《敏捷软件开发宣言》里面提到四个价值观。 程序员的主观能动性，以及程序员之间的互动，优于既定流程和工具。 软件能够运行，优于详尽的文档。 跟客户的密切协作，优于合同和谈判。 能够响应变化，优于遵循计划。 十二条原则 该宣言还提出十二条敏捷开发的原则。 通过早期和持续交付有价值的软件，实现客户满意度。 欢迎不断变化的需求，即使是在项目开发的后期。要善于利用需求变更，帮助客户获得竞争优势。 不断交付可用的软件，周期通常是几周，越短越好。 项目过程中，业务人员与开发人员必须在一起工作。 项目必须围绕那些有内在动力的个人而建立，他们应该受到信任。 面对面交谈是最好的沟通方式。 可用性是衡量进度的主要指标。 提倡可持续的开发，保持稳定的进展速度。 不断关注技术是否优秀，设计是否良好。 简单性至关重要，尽最大可能减少不必要的工作。 最好的架构、要求和设计，来自团队内部自发的认识。 团队要定期反思如何更有效，并相应地进行调整。 异常处理常见异常详细参见：python标准异常 异常名称 描述 ValueError 传入无效的参数 ZeroDivisionError 除(或取模)零 (所有数据类型) IOError 输入/输出操作失败 try–except 不会因为异常而中断，如下，‘done’依旧执行了【（Python）异常处理try…except、raise】 注意，except的三种表示 123456789101112a=10b=0try: c=a/b print(c)# except ZeroDivisionError as e:# except Exception as e:except (IOError, ZeroDivisionError) as err:# except句子可以同时处理多个异常，这些异常将被放在一个括号里成为一个元组， print('除法异常：',err)else: print('no error')print(\"done\") 12除法异常： division by zerodone raise 语句允许程序员强制发生指定的异常， 后面的将不会执行 123456a = 19if a&gt;5:# raise Exception('x 不能大于 5。x 的值为: &#123;&#125;'.format(x)) raise ValueError('x 不能大于 5。x 的值为: &#123;&#125;'.format(x)) print('--') 自定义异常类如上，raise语句，如何不影响后续的执行呢？ 可以考虑自定义异常类 如果这样写： 123456789a = 1try: if a &lt; 10: raise Exception('x 不能小于 10。x 的值为: &#123;&#125;'.format(x)) # raise ValueError('x 不能大于 5。x 的值为: &#123;&#125;'.format(x)) except: passprint('--') 并不能达到效果，异常未抛出： 正确是这样; 123456789101112class MyException(Exception): def __init__(self,message): Exception.__init__(self) self.message=message a =1if a&lt;10: try: raise MyException('x 不能小于 10。x 的值为: &#123;&#125;'.format(x)) except MyException as e: print(e.message)print(a-1, '--') 日志记录 相比 print， logging 的优势何在？ 我在上面说过，用 print 的话会产生大量的信息，从而很难从中找到真正有用的信息。而 logging 中将日志分成不同的级别以后，我们在大多数时间只保存级别比较高的日志信息，从而提高了日志的性能和分析速度，这样我们就可以很快速的从一个很大的日志文件里找到错误的信息。 「日志」是一个系统的重要组成部分，用来记录用户操作、系统运行状态和错误信息，它的好坏直接影响到系统出现问题时定位的速度，有日志记录，我们可以在服务崩溃的时候很快的通过查看日志来发现问题出现的地方，同样也可以通过对日志的观察和分析，提前发现系统可能存在的风险。 简单的说就是，排错，这对于大型系统来说很关键 参考","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"规范","slug":"规范","permalink":"http://yoursite.com/tags/规范/"}],"author":"Dufy"},{"title":"⭐️肺炎预测","slug":"肺炎预测","date":"2020-02-19T16:00:00.000Z","updated":"2021-02-27T06:25:41.141Z","comments":true,"path":"2020/02/20/肺炎预测/","link":"","permalink":"http://yoursite.com/2020/02/20/肺炎预测/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 模型建立https://github.com/flitdu/transfer/blob/master/wuhanfeiyan2019.ipynb 调参技巧预测记录知乎链接：多位专家预测出新型肺炎疫情拐点即将出现，什么情况下才能说疫情拐点出现？ 1-30至2-19预测情况统计（包括预测误差、实际确诊人数和预测峰值人数，2-1后对模型进行了修改，2-11后诊断标准改变），两条红色曲线越接近则疫情情况越好 ​ 诊断标准变之前统计 2020-01-30自己建立了一个模型，来对疫情数据进行分析（感谢大家的支持，仅供参考，不必太当真哈） 依据模型预测结果，今日（1-30号）后，肺炎确诊病例会有9463。实际情况，今晚24点后可以得知。 并且模型预测最终的确诊人数会在 14754 例，从今天算，需要10天时间达到顶峰，也就是到农历15-16号，元宵节前后，与之前网上钟南山院士的判断相符合。 这次疫情，来的突然，目前还在上行阶段。 应该来说，相比较SARS（下图），这次疫情高峰期会得早，去的也会早。 目前每日新增数据如下图。结合确诊预测模型，个人觉得新增数据未来不会有大的增加，加速上升的势头在国家的强有力措施下得到了遏制。 全国人民万众一心，对于武汉等重灾区也是八方支援，我们中国一定会很快打赢这场看不见敌人的战争。 今后一段时间还是要戴口罩，少出行，在家待着就是为国家做贡献。 还是多说一句，这次疫情，重蹈SARS覆辙，中间仅仅隔17年，本质上是没有处理好人与自然的关系，爱护野生动物，不可无限制向大自然索取。 两次教训不可谓不惨痛，唯有立法方可根除。 加油武汉！加油中国！ 随着新数据的出现，后续会持续更新预测情况 2020-01-30 附：【模型预测确诊病例数据】 1234567891011121314151617124.6687847 201.62960005 325.04447171 521.29406501 829.22254726 1302.40034933 2006.67755113 3006.67326849 4335.51835413 5954.00996685 7728.52041165 9463.2025961110978.82979291 12178.49680202 13055.95424192 13661.3491194214062.40273107 14320.98243352 14484.79988951 14587.4309832514651.28008706 14690.82904768 14715.25999013 14730.3267224714739.60890002 14745.32374667 14748.8408837 14751.0049440314752.33627199 14753.15522906 14753.6589765 14753.9688251214754.15940502 14754.27662427 14754.34872128 14754.3930651414754.42033911 14754.4371141 14754.44743164 14754.4537774814754.45768052 14754.46008109 14754.46155757 14754.4624656914754.46302423 14754.46336776 14754.46357905 14754.4637090114754.46378894 14754.4638381 14754.46386833 14754.4638869314754.46389837 14754.4639054 14754.46390973 14754.4639123914754.46391403 14754.46391503 14754.46391565 14754.4639160314754.46391627 14754.46391641 14754.4639165 14754.4639165614754.46391659 14754.46391661 2020-01-31模型验证情况： 预测30日9463，真实数字为9692，误差2.3% 预测31日确诊病例11147，最终确诊人数会在16000左右。 加油武汉！加油中国！ 2020-01-31 2020-02-01验证： 预测31日 11147，真实数字为11791，误差5.4% 可以看出预测比实际偏小，意味着最终的确诊人数会更高，真希望自己预测的会偏大点。 考虑到3%左右的致死率，不知道又会有多少人因此而夺命。 君不见钟院士，83岁，仍然挂帅出征在疫情一线；部分地方执政者，身肩抗击肺炎重任而“一问三不知”、无计可施，拿辖区民众生命开玩笑。 疫情的好转需要大家万众一心，共克时艰，做好各自本职工作。就在昨天，本次疫情已经升级为“国际关注的突发公共卫生事件”，背后的大国博弈必定激烈。 对于2月末结束战斗，目前保持乐观。 等疫情过后，立法根除已经变得刻不容缓!以往广东，如今湖北，再过段时间就可以换个别的省。 再次声明，所做只是一种出于对疫情关注产生的兴趣，不严谨之处还望轻喷。自己除了戴口罩、不外出，还能为疫情做的一点个人贡献就是对未来的肺炎趋势做出点判断，供需要的人参考。 预测2-1日确诊病例 12836，最终确诊人数会在17000左右。 加油武汉！加油中国！ 2020-02-01 2020-02-02验证情况： 2-1日预测 12836，真实数字为14380，误差10.7% -———————————- 对模型进行了修改 预测2-2日确诊病例 17625，最终确诊人数会在36000左右。（疫情的变化有点快） 加油武汉！加油中国！ 2020-02-02 2020-02-03验证情况： 2-2日预测 17625，真实数字为17209，误差2.4% -———————————- 预测2-3日确诊病例 20193，最终确诊人数会在34470左右 加油武汉！加油中国！ 2020-02-03 2020-02-04最新与SARS对比图： 03年SARS发展图： 可见此次疫情来之猛 感染新增趋势： 看来我们的吃这下更加世界知名了，不仅能吃出菜系，更可以吃出自然失衡、国际关注突发。 疫情终会过去，唯有立法，方可彻底解决当前困局。 今后必须加强卫生监管，各个省都存在爆发危险，如果不整治，昨日广东，现在湖北，明天会是哪个？ 别忘了，恩格斯在《自然辩证法》说过的那段警世名言： 不要过分陶醉于我们人类对自然界的胜利。对于每一次这样的胜利，自然界都对我们进行报复，我们最初的成果又消失了。 世界上有很多地方，如美索不达米亚、小亚细亚以及其他各地的居民，为了想得到耕地，把森林都砍完了，但他们梦想不到，这些地方今天正因此成为不毛之地。 类似此次疫情的突发情况，都可以看作是未来中国崛起路上对国家能力的检验，涉及到医学水平、执政者决策能力、地方管理者应急组织能力、调度协调能力、不同企业的随机应变能力、民众科学普及程度等等。 一次疫情，可以看作是一场大检验、大考核。中国要崛起，这些各方面的能力必须上来。等疫情过后，最重要的是总结经验、教训，这背后都是一个个鲜活的生命。 重大事件–&gt;应对–&gt;总结–&gt;新的重大事件….，缺一环而不可。 那些打不倒自己的，终将使自己强大。 验证情况： 2-3日预测 20193，真实数字为20440，误差1.2% -———————————- 预测2-4日确诊病例 23803 ，最终确诊人数会在42000左右 加油武汉！加油中国！ 2020-02-04 2020-02-05验证情况： 2-4日预测 23803，真实数字为24325，误差2.1% -———————————- 预测2-5日确诊病例 28382，最终确诊人数会在 56000 左右（自己有点惊讶，只能相信） 关于拐点，参照下图，预计会在这两三天会到来，果真如此，那么意味着疫情将得到有效控制。 目前，新增确诊人数仍在不断上升 一次疫情，可以看作是一场大检验、大考核。中国要崛起，这些各方面的能力必须上来。等疫情过后，最重要的是总结经验、教训，这背后都是一个个鲜活的生命换来的。 重大事件–&gt;应对–&gt;总结–&gt;新的重大事件….，缺一环而不可。 那些打不倒自己的，终将使自己强大。 加油武汉！加油中国！ 2020-02-05 2020-02-06验证情况： 2-5日预测 28382，真实数字为28018，误差1.3% -———————————- 预测2-6日确诊病例 31733，最终确诊人数会在 52460 左右 注意到，今天的新增确诊＜昨日，好消息！ 加油武汉！加油中国！ 2020-02-06 2020-02-07疫情“吹哨人”去世，这个代价太过沉重 “他们欠你一个道歉，我们欠你一句谢谢。李文亮医生，一路走好!” 验证情况： 2-6日预测 31733，真实数字为31161，误差1.8% -———————————- 预测2-7日确诊病例 34112，最终确诊人数会在 47154 左右 注意到，今天的新增确诊依旧＜昨日&lt;前天，好消息！ 加油武汉！加油中国！ 2020-02-07 2020-02-08今日返程，车站更新。 路上各个村口红旗飘扬，一些人在值守，处于“封村”状态。 国道上看不到车辆。 验证情况： 2-7日预测 34112，真实数字为34560，误差1.3% -———————————- 预测2-8日确诊病例 37391，最终确诊人数会在49619左右 注意到，昨日新增确诊有上升势头，但仍＜前两天的峰值，前景发展未可知。但是，只要今后的新增不超过前几天的峰值，就表明疫情得到了控制，后续发展看好。 加油武汉！加油中国！ 2020-02-08 2020-02-09验证情况： 2-8日预测 37391，真实数字为37202，误差 0.5% -———————————- 预测2-9日确诊病例 39505，最终确诊人数会在48449左右 注意到，昨日新增持续走低，为新增峰值以来最低值，好消息！目前来看，峰值确诊依旧稳定在5万左右。 加油武汉！加油中国！ 2020-02-09 2020-02-10验证情况： 2-9日预测 39505，真实数字为40260，误差 1.8% -———————————- 预测2-10日确诊42302病例 ，最终确诊人数会在51000左右 注意到，昨日新增确诊有上升势头，但仍＜历史峰值。但是，只要今后的新增不超过前几天的峰值，后续发展看好。峰值确诊依旧稳定在5万左右。 加油武汉！加油中国！ 2020-02-10 2020-02-11验证情况： 2-10日预测 42302，真实数字为42651，误差 0.8% -———————————- 预测2-11日确诊44184病例 ，最终确诊人数会在51000左右 注意到，昨日新增确为峰值以来最低值，好消息！目前新增正朝着稳步下降的趋势走，后续发展看好（更加确定）。峰值确诊依旧稳定在5万左右。 我想提醒的是，疫情过后，万不可急着庆祝而忘了溯源，今后如何彻底解决本次危机、杜绝再次发生（连着发生2次已经）？ 我们GDP已经是第二，2019年我国国内生产总值为99.0865万亿元，大概每天2700亿元，本次疫情带来的损失又是多少？平时，我们可否拿出一部分放在环境保护上或者环保研发上？ 人也是自然的一部分，我们与其它生物包括细菌，病毒，共同生活在这个星球，共同组成目前这个生态系统。当人类肆意品尝、污染环境（当今的塑料垃圾真的是有点惊人，何时解决！！？？），傲慢凌驾于自然之上时，真的觉得是占了便宜吗？ 既然一部分人，仍旧执迷不悟 ，拿所有人来冒险，为了我们的家园，为何不立法保证、强制实施呢？ 我想不到别的更好的办法…. 加油武汉！加油中国！ 2020-02-11 2020-02-12验证情况： 2-11日预测 44184，真实数字为44653，误差 1.0% -———————————- 预测2-12日确诊 45944 病例 ，最终确诊人数会在51000左右 注意到，昨日新增继续下降，好消息！后续发展看好。峰值确诊依旧稳定在5万左右。 加油武汉！加油中国！ 2020-02-12 2020-02-13今日新增过万，吓我一跳。看来此前估计，还是有点乐观了。 新增确诊图 之所以新增确诊激增，在于诊断标准变了，增加了“临床诊断病例”。 验证情况： 12日确诊 59804，这其中包含湖北临床诊断病例13332例， 如果诊断标准未变，那么今日确诊&lt;46472。2-12日预测 45944，误差&lt; 1.1% 当然，既然确诊数确实猛增，那么实际情况也确实如此，当日误差仍旧定为 23.2%。 -———————————- 预测2-13日确诊 69879 病例 ，最终确诊人数会在150000左右 加油武汉！加油中国！ 2020-02-13 2020-02-14验证情况： 2-13日预测 69879 ，真实数字为64894，误差 7.6 % -———————————- 预测2-14日确诊 72124 病例 ，最终确诊人数会在14 万左右 注意到，昨日新增下降，好消息！ 加油武汉！加油中国！ 2020-02-14 2020-02-15验证情况： 2-14日预测 72124 ，真实数字为66492，误差 8.4 % 目前还没到修改模型的地步，对后续情况继续保持跟踪 -———————————- 预测2-15日确诊 70745 病例 ，最终确诊人数会在10 万左右 注意到，昨日新增继续下降，好消息！ 加油武汉！加油中国！ 2020-02-15 2020-02-16来看一下我们的“敌人”长什么样： ​ 图片来源：美国卫生研究院（NIH）官网 ​ 顶级医学杂志《柳叶刀》(The Lancet)公布了新型冠状病毒(2019-nCoV/novel coronavirus)的彩色照片 验证情况： 2-15日预测 70745 ，真实数字为68501，误差 3.2 % -———————————- 预测2-16日确诊72587病例 ，最终确诊人数会在9.5万左右 注意到，昨日新增继续下降，好消息！ 加油武汉！加油中国！ 2020-02-16 2020-02-17验证情况： 2-16日预测 72587 ，真实数字为70548，误差 2.8 % -———————————- 预测2-17日确诊 73365 病例 ，最终确诊人数会在8.8 万左右 注意到，昨日新增略微反弹 必须立法禁止，对于果子狸，穿山甲等，只要与重大疾病相关的中间宿主，一定要从法律层面立法禁止。这不仅是保护自然，亦是保护人类自己 我们不应该讨论立不立法，而是讨论如何让医学家参与制定哪些野生动物明确要在禁止吃之列。 支持立法彻底解决! 加油武汉！加油中国！ 2020-02-17 2020-02-18 最近，联合国粮农组织正在巴基斯坦开会，谈到蝗灾时坦诚：“气候变化与史无前例的蝗虫危机之间存在不可分割的联系。” 2020年刚开始不到60天，我们就已经看到山火、蝗灾和疫情在世界肆虐，南极气温也突破了20摄氏度。 这个世界很美好，也很脆弱。整个自然界是相互联系的有机整体。善待自然，亦是尊重我们自己。 就像电影《流浪地球》开头的台词： “最初，没有人在意这场灾难。这不过是一场山火、一次旱灾、一个物种的灭绝、一座城市的消失，直到这场灾难和每个人息息相关。” 验证情况： 2-17日预测 73365 ，真实数字为72438，误差 1.2 % -———————————- 预测2-18日确诊 75102 病例 ，最终确诊人数会在 8.6 万左右 注意到，昨日新增继续下降，好消息！ 加油武汉！加油中国！ 2020-02-18 2020-02-19验证情况： 2-18日误差 1.2 % -———————————- 预测2-19日确诊75971病例 ，最终确诊人数会在 8.4 万左右 注意到，昨日新增继续下降，好消息！ 加油武汉！加油中国！ 2020-02-19 2020-02-20预测2-20日确诊75920病例 ，最终确诊人数会在 8.2 万左右 注意到，昨日新增有一个猛降，好消息！ 加油武汉！加油中国！ 局势已明朗，接下来就不更新啦，谢谢大家! 2020-02-20 21号后表格统计 日期 确诊预测 实际确诊 误差 最终峰值 2-20 75920 75465 0.006 82304 2-21 76564 76288 81589 2-22 76936 参考","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[],"author":"Dufy"},{"title":"⭐️爬虫demo","slug":"爬虫demo","date":"2020-02-12T16:00:00.000Z","updated":"2021-02-27T06:19:38.893Z","comments":true,"path":"2020/02/13/爬虫demo/","link":"","permalink":"http://yoursite.com/2020/02/13/爬虫demo/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… ☆GitHub爬虫教程https://github.com/wistbean/learn_python3_spider 豆瓣电影 Top 250【1】https://movie.douban.com/top250? 1234567891011121314151617181920212223242526272829303132333435363738394041import requestsfrom bs4 import BeautifulSoupimport xlwtfrom fake_useragent import UserAgentua = UserAgent()def request_douban(url): try: headers = &#123;'User-Agent': ua.chrome&#125; response = requests.get(url,headers=headers) if response.status_code == 200: return response.text except requests.RequestException: return Nonedef save_to_excel(soup): list = soup.find(class_='grid_view').find_all('li') for item in list: item_img = item.find('a').find('img').get('src') item_index = item.find(class_='').string # item_name = item.find(class_='title').string # item_score = item.find(class_='rating_num').string # item_author = item.find('p').text item_intr = item.find(class_ ='inq').string # print('爬取电影：' + item_index + ' | ' + item_name +' | ' + item_score +' | ' + item_intr)def main(page): url = 'https://movie.douban.com/top250?start='+ str(page*25)+'&amp;filter=' html = request_douban(url) soup = BeautifulSoup(html) save_to_excel(soup)if __name__ == '__main__': for i in range(0, 2): main(i) 123456789101112爬取电影：1 | 肖申克的救赎 | 9.7 | 希望让人自由。爬取电影：2 | 霸王别姬 | 9.6 | 风华绝代。爬取电影：3 | 阿甘正传 | 9.5 | 一部美国近现代史。爬取电影：4 | 这个杀手不太冷 | 9.4 | 怪蜀黍和小萝莉不得不说的故事。爬取电影：5 | 美丽人生 | 9.5 | 最美的谎言。爬取电影：6 | 泰坦尼克号 | 9.4 | 失去的才是永恒的。 爬取电影：7 | 千与千寻 | 9.3 | 最好的宫崎骏，最好的久石让。 爬取电影：8 | 辛德勒的名单 | 9.5 | 拯救一个人，就是拯救整个世界。爬取电影：9 | 盗梦空间 | 9.3 | 诺兰给了我们一场无法盗取的梦。爬取电影：10 | 忠犬八公的故事 | 9.4 | 永远都不能忘记你所爱的人。爬取电影：11 | 海上钢琴师 | 9.3 | 每个人都要走一条自己坚定了的路，....... 36Kr：24 小时热榜https://36kr.com/ 24 小时热榜 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import requestsfrom bs4 import BeautifulSoupfrom fake_useragent import UserAgentprint('from 36Kr...')ua = UserAgent()def request_douban(url): try: headers = &#123;'User-Agent': ua.chrome&#125; response = requests.get(url, headers=headers) if response.status_code == 200: # response.encoding = 'utf-8' # print(response.text) return response.text except requests.RequestException: return Nonedef save_to_excel(soup, url): # print(soup) list = soup.find(class_='hotlist-main').find_all('a') # print(list) i = 0 for item in list: # print(item) # item_name = item.find(class_='title').string # item_img = item.find('a').find('img').get('src') # item_index = item.find(class_='').string # item_score = item.find(class_='rating_num').string # item_author = item.find('p').text # item_intr = item.find(class_='inq').string # item_intr = item.find(class_ ='inq').string # item_url = item.get('href') item_index = item.text # # # print('爬取电影：' + item_index + ' | ' + item_name +' | ' + item_score +' | ' + item_intr) # print(item_index) if item_index: # 非空 i += 1 print('爬取&#123;&#125;: &#123;&#125;|&#123;&#125;'.format(i, item_index, url + item_url))def main(): url = 'https://36kr.com' html = request_douban(url) soup = BeautifulSoup(html) save_to_excel(soup, url)if __name__ == '__main__': main() 12345678910爬取1: 当年鄙视微商的，2020都活成了微商|https://36kr.com/p/5291516爬取2: 奥斯卡爆款《寄生虫》：你低估了穷人变穷的真相|https://36kr.com/p/5291536爬取3: 8点1氪丨贾跃亭方回应甘薇索偿40亿元：仍在处理，尚存异议；京东最新股权结构曝光：刘强东持股16.2%，投票权近八成；特斯拉在北美召回1.5万辆Model X|https://36kr.com/p/5291805爬取4: 最前线 | 疫情持续冲击线下商家，抖音快手鼓励它们上线卖货|https://36kr.com/p/5291683爬取5: 60 万线下教育机构“自救” | 36氪新风向|https://36kr.com/p/5291373爬取6: 36氪独家｜「凌笛科技」完成1亿元A+轮融资，服装行业线上协同需求剧增|https://36kr.com/p/5291392爬取7: 科技神回复 | 甘薇向贾跃亭索赔近40亿，左口袋到右口袋？|https://36kr.com/p/5291789爬取8: 老乡鸡董事长手撕联名信：至暗时刻，最见一个人的格局|https://36kr.com/p/5291511爬取9: 氪星晚报 | 新冠肺炎疫情全国新增确诊病例数下降48.2%；甘薇离婚索赔贾跃亭40亿；博瑞医药：今天生产瑞德西韦制剂|https://36kr.com/p/5291734爬取10: 愿景基金第一家倒闭公司：1亿美金全部打水漂|https://36kr.com/p/5291753 UserAgent是识别浏览器的一串字符串，相当于浏览器的身份证，在利用爬虫爬取网站数据时，频繁更换UserAgent可以避免触发相应的反爬机制。fake-useragent对频繁更换UserAgent提供了很好的支持，可谓防反扒利器。 新浪https://news.sina.com.cn/china/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import requestsfrom bs4 import BeautifulSoupimport xlwtfrom fake_useragent import UserAgentua = UserAgent()def request_douban(url): try: headers = &#123;'User-Agent': ua.chrome&#125; response = requests.get(url, headers=headers) if response.status_code == 200: response.encoding = 'utf-8' # print(response.text) return response.text except requests.RequestException: return Nonedef save_to_excel(soup): # print(soup) list1 = soup.find(class_='right-content').find_all('li') list2 = soup.find(class_='news-rank-conb1').find(class_='hot-news-ul') # print(list2.text) i = 0 for item in list1: i += 1 item_img = item.find('a').text item_url = item.find('a').get('href') # item_index = item.find(class_='rankNum').string # # item_name = item.find(class_='text').string # # item_score = item.find(class_='rating_num').string # # item_author = i # item_intr = item.find(class_ ='inq').string # # print('爬取电影：' + item_index + ' | ' + item_name +' | ' + item_score +' | ' + item_intr) print('爬取&#123;&#125;: &#123;&#125; &#123;&#125;'.format(i, item_img, item_url))def main(): url = 'https://news.sina.com.cn/china/' html = request_douban(url) soup = BeautifulSoup(html) print('from新浪...') save_to_excel(soup)if __name__ == '__main__': main() from新浪…爬取1: 中央赴湖北指导组：应收尽收 刻不容缓 https://news.sina.com.cn/c/2020-02-08/doc-iimxxste9863647.shtml爬取2: 王贺胜履新湖北省委常委前 已赴鄂工作多日 https://news.sina.com.cn/o/2020-02-08/doc-iimxxste9798545.shtml爬取3: 美媒落井下石翻车 新加坡总理夫妇先后发声挺中国 https://news.sina.com.cn/c/2020-02-08/doc-iimxxste9855111.shtml爬取4: 武汉之外疫….. 深圳卫健委每日疫情只爬取带“疫情情况”的信息目录： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import requestsfrom bs4 import BeautifulSoupfrom fake_useragent import UserAgentprint('from 深圳市卫生健康委员会...')ua = UserAgent()def request_douban(url): try: headers = &#123;'User-Agent': ua.chrome&#125; response = requests.get(url, headers=headers) if response.status_code == 200: response.encoding = 'utf-8' # print(response.text) return response.text except requests.RequestException: return Nonedef save_to_excel(soup, url): # print(soup) list = soup.find(class_='wendangListC').find_all('li') # print(list) i = 0 for item in list: # print(item) # item_name = item.find(class_='title').string # item_img = item.find('a').find('img').get('src') item_index0 = item.find(class_='').string # item_score = item.find(class_='rating_num').string # item_author = item.find('p').text # item_intr = item.find(class_='inq').string # item_intr = item.find(class_ ='inq').string # item_url = item.find('a').get('href') item_txt = item.find('a').text # # # print('爬取电影：' + item_index + ' | ' + item_name +' | ' + item_score +' | ' + item_intr) # print(item_index) if item_txt and '疫情情况' in item_txt: # 非空 and 过滤 i += 1 # print('爬取&#123;&#125;: &#123;&#125;|&#123;&#125;'.format(i, item_index, url + item_url)) print('爬取&#123;&#125;: &#123;&#125;|&#123;&#125;'.format(item_index0, item_txt, url[:26] + item_url))def main(i): if i &gt;0: url = 'http://wjw.sz.gov.cn/yqxx/' + 'index_&#123;&#125;.htm'.format(i) else: url = 'http://wjw.sz.gov.cn/yqxx/' html = request_douban(url) soup = BeautifulSoup(html) save_to_excel(soup, url)if __name__ == '__main__': for i in range(0,15): main(i) 1234567爬取4: 2020年2月16日深圳市新冠肺炎疫情情况|http://wjw.sz.gov.cn/yqxx/./202002/t20200216_19014139.htm爬取8: 2020年2月15日深圳市新冠肺炎疫情情况|http://wjw.sz.gov.cn/yqxx/./202002/t20200215_19013050.htm爬取16: 2020年2月14日深圳市新冠肺炎疫情情况|http://wjw.sz.gov.cn/yqxx/./202002/t20200214_19011706.htm爬取22: 2020年2月13日深圳市新冠肺炎疫情情况|http://wjw.sz.gov.cn/yqxx/./202002/t20200213_19010518.htm爬取28: 2020年2月12日深圳市新冠肺炎疫情情况|http://wjw.sz.gov.cn/yqxx/./202002/t20200212_19009020.htm爬取33: 2020年2月11日深圳市新冠肺炎疫情情况|http://wjw.sz.gov.cn/yqxx/./202002/t20200211_19007846.htm爬取37: 截至2020年2.... 书籍爬取https://github.com/xdlkc/EpubwSpider 参考 python爬虫教程从0到1:learn_python3_spider","categories":[{"name":"网页","slug":"网页","permalink":"http://yoursite.com/categories/网页/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}],"author":"Dufy"},{"title":"⭐️Git学习","slug":"Git学习","date":"2020-01-06T16:00:00.000Z","updated":"2021-01-12T15:43:25.298Z","comments":true,"path":"2020/01/07/Git学习/","link":"","permalink":"http://yoursite.com/2020/01/07/Git学习/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 状态转移图： 参考 四个命令玩转 Git 猴子都能懂的GIT入门传送门→https://backlog.com/git-tutorial/cn/ 书籍《Pro Git》传送门→https://git-scm.com/book/zh/v2 branch分支管理 远程建仓 clone 到本地 初始代码版本放入本地 push 到远程 远程建立 dev 分支 本地建立 dev 分支 查看当前分支对应 1git branch -vv 与远程分支建立对应 1git branch --all 会发现，远程的dev 未看到 首先， 12git fetch获取分支 再次查看，可以看到有远程的dev了 关联： 1234git checkout dev #切换到分支 &apos;dev&apos;git branch --set-upstream origin/dev #设置当前本地分支的默认远程分支。 下拉 1git pull 本地切换到dev开发 修改2次，每次均git add . git commit git push 远端，结果如下： 合并分支到master上【1】 假设已经开发好一个版本，可以考虑合并上传 切换到master—&gt;git pull origin master–&gt;git merge dev 再 git push 远端同步 git stashGit：什么时候应使用 “git stash” 12345$ git stash save$ git add -u$ git commit -m &apos;...&apos;$ git stash pop 参考 合并分支到master上","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}],"author":"Dufy"},{"title":"⭐️书单","slug":"书单","date":"2020-01-04T16:00:00.000Z","updated":"2021-03-07T08:55:57.348Z","comments":true,"path":"2020/01/05/书单/","link":"","permalink":"http://yoursite.com/2020/01/05/书单/","excerpt":"","text":"2020《时间简史》，史蒂芬・霍金，01.01-1.04 《创造自然–亚历山大·冯·洪堡的科学发现之旅》01.11-2.08 文笔优美，自然描写如身临其境，跟随洪堡踏上自然探寻之旅。洪堡的自然观，值得我们再次挖掘，尤其是在今天世界各国面临各种生态问题的背景下。 自然，包括有机和无机物，是一个相互联系的有机整体。 春节期间（1月-..）爆发的NCP(Novel coronavirus pneumonia)本质上是没有处理好人与自然的关系。敬畏自然，爱护自然，而不是采取凌驾于自然、肆意践踏其它生命。看看今天的疾病，气候变暖，水土流失，森林砍伐等，谁说洪堡200多年前的思想，我们真正的领会实践了呢？ 洪堡与歌德，达尔文，梭罗等 的关系真是让我很惊讶，他真的是一位思想渊博前卫的大家，给很多人以启发。 《病毒星球》，02.09-….细胞战场Battlefield Cell 参考","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[{"name":"阅读","slug":"阅读","permalink":"http://yoursite.com/tags/阅读/"}],"author":"Dufy"},{"title":"⭐️动手深度学习","slug":"《动手深度学习》","date":"2019-12-25T16:00:00.000Z","updated":"2021-02-27T06:26:24.895Z","comments":true,"path":"2019/12/26/《动手深度学习》/","link":"","permalink":"http://yoursite.com/2019/12/26/《动手深度学习》/","excerpt":"[TOC]","text":"[TOC] 总论具体来说，应用深度学习需要同时理解： 问题的动机和特点； 将大量不同类型神经网络层通过特定方式组合在一起的模型背后的数学原理； 在原始数据上拟合极复杂的深层模型的优化算法； 有效训练模型、避免数值计算陷阱以及充分利用硬件性能所需的工程技能； 为解决方案挑选合适的变量（超参数）组合的经验。 我们将在同一个地方阐述剖析问题所需的批判性思维、解决问题所需的数学知识，以及实现解决方案所需的工程技能。 深度学习简介事实上，这也是目前的机器学习和深度学习应用共同的核心思想：我们可以称其为“用数据编程”。 通俗来说，机器学习是一门讨论各式各样的适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。深度学习是指机器学习中的一类函数，它们的形式通常为多层神经网络。近年来，仰仗着大数据集和强大的硬件，深度学习已逐渐成为处理图像、文本语料和声音信号等复杂高维度数据的主要方法。 起源 机器学习和深度学习的关系 “经验”通常以“数据”形式存在 预备知识数据操作 从打印D时显示的属性&lt;NDArray 2*3 @cpu(0)&gt;可以看出，它是二维数组，且被创建在CPU使用的内存上。其中“@cpu(0)”里的0没有特别的意义，并不代表特定的核。 自动求梯度 123456789x = nd.arange(4).reshape((4, 1))print('x:&#123;&#125;'.format(x))print('以下为求关于x的梯度，注意以下为一个求梯度的整体表达')x.attach_grad()with autograd.record(): y = 2*nd.dot(x.T, x)y.backward()assert (x.grad - 4*x).norm().asscalar() == 0 x.grad 12345678910111213x:[[0.] [1.] [2.] [3.]]&lt;NDArray 4x1 @cpu(0)&gt;以下为求关于x的梯度[[ 0.] [ 4.] [ 8.] [12.]]&lt;NDArray 4x1 @cpu(0)&gt; Python3 assert（断言）断言可以在条件不满足程序运行的情况下直接返回错误，而不必等待程序运行后出现崩溃的情况 注意：以下两种写法结果是一样的 1234with autograd.record(): y = 2*nd.dot(x.T, x) or y = 2*x**2 123456789x = nd.arange(4).reshape((4, 1))print(&apos;x:&#123;&#125;&apos;.format(x))print(&apos;以下为求关于x的梯度，注意以下为一个求梯度的整体表达&apos;)x.attach_grad()with autograd.record(): y = 2*x**2y.backward()assert (x.grad - 4*x).norm().asscalar() == 0 x.grad 线性回归模型神经网路图表示线性回归和softmax回归都可看成单层神经网络。 这种看法很新颖 矢量计算表示 注意红框中的“分母” 从0实现参考【3】 123456789101112131415161718192021222324252627%matplotlib inlinefrom IPython import displayfrom matplotlib import pyplot as pltfrom mxnet import autograd, ndimport randomnum_inputs = 2num_examples = 1000true_w = [2, -3.4]true_b = 4.2features = nd.random.normal(scale=1, shape=(num_examples, num_inputs)) # 标注差为1labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_blabels += nd.random.normal(scale=0.01, shape=labels.shape)# 本函数已保存在d2lzh包中方便以后使用def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) # 样本的读取顺序是随机的 for i in range(0, num_examples, batch_size): j = nd.array(indices[i: min(i + batch_size, num_examples)]) yield features.take(j), labels.take(j) # take函数根据索引返回对应元素batch_size = 10for X, y in data_iter(batch_size, features, labels): print('1个batch_size:', X, y) break 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 初始化模型参数w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))b = nd.zeros(shape=(1,))print(w,b)w.attach_grad() #申请存储梯度所需要的内存b.attach_grad()def linreg(X, w, b): # 本函数已保存在d2lzh包中方便以后使用 return nd.dot(X, w) + b# 定义损失函数, 两种写法【参见自动求梯度一节】def squared_loss(y_hat, y): # 本函数已保存在d2lzh包中方便以后使用# print((y_hat - y.reshape(y_hat.shape)) ** 2 / 2,'-------------')# return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 #&lt;NDArray 10x1 @cpu(0)&gt; ------------- tep = y_hat - y.reshape(y_hat.shape) print(nd.dot(tep.T, tep),'============') return nd.dot(tep.T, tep) # 定义优化算法def sgd(params, lr, batch_size): # 本函数已保存在d2lzh包中方便以后使用 for param in params:# print(param.grad) param[:] = param - lr * param.grad / batch_sizelr = 0.03num_epochs = 5net = linregloss = squared_lossfor epoch in range(num_epochs): # 训练模型一共需要num_epochs个迭代周期 # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X # 和y分别是小批量样本的特征和标签 for X, y in data_iter(batch_size, features, labels): #每次取1个batch,得到X,y with autograd.record(): l = loss(net(X, w, b), y) # l是有关小批量X和y的损失,l:10*1 l.backward() # 小批量的损失对模型参数求梯度 sgd([w, b], lr, batch_size) # 使用小批量随机梯度下降迭代模型参数 train_l = loss(net(features, w, b), labels) #计算所有训练样本的损失，train_l:1000*1 print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy())) print(true_w, w)print(true_b, b)--------------------------------------------------[[0.00077456] [0.01180387]]&lt;NDArray 2x1 @cpu(0)&gt; [0.]&lt;NDArray 1 @cpu(0)&gt;epoch 1, loss 0.028056epoch 2, loss 0.000091epoch 3, loss 0.000050epoch 4, loss 0.000050epoch 5, loss 0.000050[2, -3.4] [[ 1.9994183] [-3.3996756]]&lt;NDArray 2x1 @cpu(0)&gt;4.2 [4.2005925]&lt;NDArray 1 @cpu(0)&gt; batch_size取值笔记 通过修改不同的batch_size，可以看出，小的batch_size可以在更少用更少的epoch收敛 关于data_iter(）解读 利用Gluon接口更简洁实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#=====================生成数据集from mxnet import autograd, ndnum_inputs = 2num_examples = 1000true_w = [2, -3.4]true_b = 4.2features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_blabels += nd.random.normal(scale=0.01, shape=labels.shape)#=====================读取数据from mxnet.gluon import data as gdatabatch_size = 10# 将训练数据的特征和标签组合dataset = gdata.ArrayDataset(features, labels)# 随机读取小批量data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)for X, y in data_iter: print(X, y) break#=====================定义模型from mxnet.gluon import nnnet = nn.Sequential()net.add(nn.Dense(1)) # 定义输出层个数为1# 初始化模型参数from mxnet import initnet.initialize(init.Normal(sigma=0.01))# 定义损失函数from mxnet.gluon import loss as glossloss = gloss.L2Loss() # 平方损失又称L2范数损失# 定义优化算法，小批量随机梯度下降（sgd）from mxnet import gluontrainer = gluon.Trainer(net.collect_params(), 'sgd', &#123;'learning_rate': 0.03&#125;)# =====================训练模型num_epochs = 3for epoch in range(1, num_epochs + 1): for X, y in data_iter: with autograd.record(): l = loss(net(X), y) l.backward() trainer.step(batch_size) l = loss(net(features), labels) print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy())) 1234567891011121314151617[[-0.9445115 1.9230403 ] [ 0.28609878 -0.5688993 ] [ 2.0842285 -1.3100709 ] [ 1.233935 -1.609952 ] [ 0.24503022 0.40995437] [-1.3501216 -0.04495556] [-1.4607683 -3.0632296 ] [-0.03292305 0.57150906] [ 0.29747176 -1.009977 ] [ 1.4822187 0.07581457]]&lt;NDArray 10x2 @cpu(0)&gt; [-4.2168045 6.709853 12.82297 12.135502 3.3031228 1.6529553 11.694708 2.186649 8.227948 6.9211607]&lt;NDArray 10 @cpu(0)&gt;epoch 1, loss: 0.041319epoch 2, loss: 0.000152epoch 3, loss: 0.000046 模型参数读取： 12345678910# =====================获取模型参数dense = net[0]true_w, dense.weight.data()([2, -3.4], [[ 1.999795 -3.399354]] &lt;NDArray 1x2 @cpu(0)&gt;)true_b, dense.bias.data()(4.2, [4.1996756] &lt;NDArray 1 @cpu(0)&gt;) 值得一提的是，在Gluon中我们无须指定每一层输入的形状，例如线性回归的输入个数。当模型得到数据时，例如后面执行net(X)时，模型将自动推断出每一层的输入个数。我们将在之后“深度学习计算”一章详细介绍这种机制。Gluon的这一设计为模型开发带来便利。【4】 softmax回归 对于分类问题，模型输出为（0，1）间的概率值。 损失函数形式 采用平方损失不再合适。 损失函数采用交叉熵： 实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667%matplotlib inlineimport d2lzh as d2lfrom mxnet import autograd, ndbatch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)num_inputs = 784num_outputs = 10W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs))b = nd.zeros(num_outputs)W.attach_grad()b.attach_grad()def softmax(X): X_exp = X.exp() partition = X_exp.sum(axis=1, keepdims=True) return X_exp / partition # 这里应用了广播机制def net(X): return softmax(nd.dot(X.reshape((-1, num_inputs)), W) + b)def cross_entropy(y_hat, y): return -nd.pick(y_hat, y).log()def accuracy(y_hat, y): return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()# 本函数已保存在d2lzh包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中# 描述def evaluate_accuracy(data_iter, net): acc_sum, n = 0.0, 0 for X, y in data_iter: y = y.astype('float32') acc_sum += (net(X).argmax(axis=1) == y).sum().asscalar() n += y.size return acc_sum / nnum_epochs, lr = 5, 0.1# 本函数已保存在d2lzh包中方便以后使用def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None): for epoch in range(num_epochs): train_l_sum, train_acc_sum, n = 0.0, 0.0, 0 for X, y in train_iter: with autograd.record(): y_hat = net(X) l = loss(y_hat, y).sum() l.backward() if trainer is None: d2l.sgd(params, lr, batch_size) else: trainer.step(batch_size) # “softmax回归的简洁实现”一节将用到 y = y.astype('float32') train_l_sum += l.asscalar() train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar() n += y.size test_acc = evaluate_accuracy(test_iter, net) print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f' % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr) 12345 epoch 1, loss 0.7875, train acc 0.748, test acc 0.804epoch 2, loss 0.5729, train acc 0.812, test acc 0.821epoch 3, loss 0.5287, train acc 0.825, test acc 0.831epoch 4, loss 0.5045, train acc 0.831, test acc 0.833epoch 5, loss 0.4898, train acc 0.834, test acc 0.839 简洁实现 12345678910111213141516171819%matplotlib inlineimport d2lzh as d2lfrom mxnet import gluon, initfrom mxnet.gluon import loss as gloss, nnbatch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)net = nn.Sequential()net.add(nn.Dense(10))net.initialize(init.Normal(sigma=0.01))loss = gloss.SoftmaxCrossEntropyLoss()trainer = gluon.Trainer(net.collect_params(), &apos;sgd&apos;, &#123;&apos;learning_rate&apos;: 0.1&#125;)num_epochs = 5d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer) 12345epoch 1, loss 0.7888, train acc 0.749, test acc 0.805epoch 2, loss 0.5746, train acc 0.810, test acc 0.825epoch 3, loss 0.5292, train acc 0.825, test acc 0.826epoch 4, loss 0.5054, train acc 0.831, test acc 0.837epoch 5, loss 0.4898, train acc 0.833, test acc 0.842 多层感知机 在图3.3所示的多层感知机中，输入和输出个数分别为4和3，中间的隐藏层中包含了5个隐藏单元（hidden unit）。由于输入层不涉及计算，图3.3中的多层感知机的层数为2。 激活函数的引入 为了 避免多层等效为单层NN，需要引入非线性变换，也就是激活函数 常用激活函数及其导数 从0实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445%matplotlib inlineimport d2lzh as d2lfrom mxnet import gluon, initfrom mxnet.gluon import loss as gloss, nnfrom mxnet import autograd, ndbatch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)# 初始化模型参数num_inputs, num_outputs, num_hiddens = 784, 10, 256W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))b1 = nd.zeros(num_hiddens)W2 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_outputs))b2 = nd.zeros(num_outputs)params = [W1, b1, W2, b2]for param in params: param.attach_grad() # 定义激活函数def relu(X): return nd.maximum(X, 0)# 定义模型def net(X): X = X.reshape((-1, num_inputs)) H = relu(nd.dot(X, W1) + b1) return nd.dot(H, W2) + b2# 定义损失函数loss = gloss.SoftmaxCrossEntropyLoss()# =====================训练模型num_epochs, lr = 5, 0.5d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr) -----------------------------epoch 1, loss 0.8098, train acc 0.697, test acc 0.743epoch 2, loss 0.4928, train acc 0.817, test acc 0.851epoch 3, loss 0.4290, train acc 0.843, test acc 0.851epoch 4, loss 0.3962, train acc 0.853, test acc 0.869epoch 5, loss 0.3736, train acc 0.862, test acc 0.869 简洁实现 12345678910111213141516171819202122232425262728293031%matplotlib inlineimport d2lzh as d2lfrom mxnet import gluon, initfrom mxnet.gluon import loss as gloss, nnfrom mxnet import autograd, nd# 定义模型net = nn.Sequential()net.add(nn.Dense(256, activation='relu'), #隐藏层单元个数为256 nn.Dense(10))net.initialize(init.Normal(sigma=0.01))# =====================训练模型batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)loss = gloss.SoftmaxCrossEntropyLoss()trainer = gluon.Trainer(net.collect_params(), 'sgd', &#123;'learning_rate': 0.5&#125;)num_epochs = 5d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer) -----------------------epoch 1, loss 0.7866, train acc 0.704, test acc 0.823epoch 2, loss 0.4899, train acc 0.820, test acc 0.825epoch 3, loss 0.4288, train acc 0.842, test acc 0.860epoch 4, loss 0.3978, train acc 0.853, test acc 0.868epoch 5, loss 0.3681, train acc 0.864, test acc 0.871 欠拟合和过拟合 训练误差和泛化误差 通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。 在机器学习里，我们通常假设训练数据集（训练题）和测试数据集（测试题）里的每一个样本都是从同一个概率分布中相互独立地生成的。 由于数据分布的差异，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。 机器学习模型应关注降低泛化误差。 模型复杂度的影响 给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。应对欠拟合和过拟合的一个办法是针对数据集选择合适复杂度的模型。 具体反映 多项式函数拟合实验【5】，注意其 表现形式 避免过拟合 权重衰减【6】 可见，L2范数正则化令权重w1和w2先自乘小于1的数，再减去不含惩罚项的梯度。因此，L2范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。 dropout 丢弃法不改变其输入的期望值，注意是对隐藏层的输出进行概率丢弃 丢弃概率是丢弃法的超参数。 实现 主要是dropout 的添加实现，其它与之前的一样。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import d2lzh as d2lfrom mxnet import autograd, gluon, init, ndfrom mxnet.gluon import loss as gloss, nndef dropout(X, drop_prob): assert 0 &lt;= drop_prob &lt;= 1 keep_prob = 1 - drop_prob # 这种情况下把全部元素都丢弃 if keep_prob == 0: return X.zeros_like() mask = nd.random.uniform(0, 1, X.shape) &lt; keep_prob return mask * X / keep_probnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))b1 = nd.zeros(num_hiddens1)W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))b2 = nd.zeros(num_hiddens2)W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))b3 = nd.zeros(num_outputs)params = [W1, b1, W2, b2, W3, b3]for param in params: param.attach_grad() drop_prob1, drop_prob2 = 0.2, 0.5 #通常的建议是把靠近输入层的丢弃概率设得小一点def net(X): X = X.reshape((-1, num_inputs)) H1 = (nd.dot(X, W1) + b1).relu() if autograd.is_training(): # 只在训练模型时使用丢弃法 H1 = dropout(H1, drop_prob1) # 在第一层全连接后添加丢弃层 H2 = (nd.dot(H1, W2) + b2).relu() if autograd.is_training(): H2 = dropout(H2, drop_prob2) # 在第二层全连接后添加丢弃层 return nd.dot(H2, W3) + b3 num_epochs, lr, batch_size = 5, 0.5, 256loss = gloss.SoftmaxCrossEntropyLoss()train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr) 12345epoch 1, loss 1.1687, train acc 0.553, test acc 0.739epoch 2, loss 0.6004, train acc 0.774, test acc 0.837epoch 3, loss 0.5014, train acc 0.817, test acc 0.852epoch 4, loss 0.4513, train acc 0.835, test acc 0.863epoch 5, loss 0.4185, train acc 0.847, test acc 0.862 注意,通过 is_training() 来判断运行模式为训练还是测试 123456print(autograd.is_training())with autograd.record(): print(autograd.is_training()) FalseTrue 关于dropout函数 简洁实现 1234567891011net = nn.Sequential()net.add(nn.Dense(256, activation=&quot;relu&quot;), nn.Dropout(drop_prob1), # 在第一个全连接层后添加丢弃层 nn.Dense(256, activation=&quot;relu&quot;), nn.Dropout(drop_prob2), # 在第二个全连接层后添加丢弃层 nn.Dense(10))net.initialize(init.Normal(sigma=0.01))trainer = gluon.Trainer(net.collect_params(), &apos;sgd&apos;, &#123;&apos;learning_rate&apos;: lr&#125;)d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer) 正向传播、反向传播 反向传播公式：（注意蓝色框里） 结合上面👆所示，可以看出正向传播与反向传播相互依赖【7】 因此，在模型参数初始化完成后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。既然我们在反向传播中使用了正向传播中计算得到的中间变量来避免重复计算，那么这个复用也导致正向传播结束后不能立即释放中间变量内存。这也是训练要比预测占用更多内存的一个重要原因。另外需要指出的是，这些中间变量的个数大体上与网络层数线性相关，每个变量的大小跟批量大小和输入个数也是线性相关的，它们是导致较深的神经网络使用较大批量训练时更容易超内存的主要原因。 CNN经典网络参考【8】、【9】、【11】 用Keras实现CNN 卷积神经网络(CNN)简介 文章目录 1 动机 2 数据集 3 卷积 3.1 这有用吗？ 3.2 填充 3.3 Conv图层 3.4 实现卷积 4 池化 4.1 实现池化 5 Softmax 5.1 用法 5.2 交叉熵损失 5.3 实现Softmax 6 结论 code：仅仅使用numpy构建一个CNN 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# -*- coding: utf-8 -*-import mnistfrom conv import Conv3x3import numpy as npclass MaxPool2: # A Max Pooling layer using a pool size of 2. def iterate_regions(self, image): &apos;&apos;&apos; Generates non-overlapping 2x2 image regions to pool over. - image is a 2d numpy array &apos;&apos;&apos; h, w, _ = image.shape new_h = h // 2 new_w = w // 2 for i in range(new_h): for j in range(new_w): im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)] yield im_region, i, j def forward(self, input): &apos;&apos;&apos; Performs a forward pass of the maxpool layer using the given input. Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters). - input is a 3d numpy array with dimensions (h, w, num_filters) &apos;&apos;&apos; h, w, num_filters = input.shape output = np.zeros((h // 2, w // 2, num_filters)) for im_region, i, j in self.iterate_regions(input): output[i, j] = np.amax(im_region, axis=(0, 1)) return outputclass Softmax: # A standard fully-connected layer with softmax activation. def __init__(self, input_len, nodes): # We divide by input_len to reduce the variance of our initial values self.weights = np.random.randn(input_len, nodes) / input_len self.biases = np.zeros(nodes) def forward(self, input): &apos;&apos;&apos; Performs a forward pass of the softmax layer using the given input. Returns a 1d numpy array containing the respective probability values. - input can be any array with any dimensions. &apos;&apos;&apos; input = input.flatten() input_len, nodes = self.weights.shape totals = np.dot(input, self.weights) + self.biases exp = np.exp(totals) return exp / np.sum(exp, axis=0)# The mnist package handles the MNIST dataset for us!# Learn more at https://github.com/datapythonista/mnisttest_images = mnist.test_images()[:1000]test_labels = mnist.test_labels()[:1000]conv = Conv3x3(8) # 28x28x1 -&gt; 26x26x8pool = MaxPool2() # 26x26x8 -&gt; 13x13x8softmax = Softmax(13 * 13 * 8, 10) # 13x13x8 -&gt; 10def forward(image, label): &apos;&apos;&apos; Completes a forward pass of the CNN and calculates the accuracy and cross-entropy loss. - image is a 2d numpy array - label is a digit &apos;&apos;&apos; # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier # to work with. This is standard practice. out = conv.forward((image / 255) - 0.5) out = pool.forward(out) out = softmax.forward(out) print(out,&apos;-----&apos;) # Calculate cross-entropy loss and accuracy. np.log() is the natural log. loss = -np.log(out[label]) acc = 1 if np.argmax(out) == label else 0 print(&apos;label:&apos;, label, &apos;loss: &apos;, loss) return out, loss, accprint(&apos;MNIST CNN initialized!&apos;)loss = 0num_correct = 0for i, (im, label) in enumerate(zip(test_images, test_labels)): # Do a forward pass. _, l, acc = forward(im, label) loss += l num_correct += acc # Print stats every 100 steps. if i % 100 == 99: print( &apos;[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%&apos; % (i + 1, loss / 100, num_correct) ) loss = 0 num_correct = 0 批量归一化本节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易 。 通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。 处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。 但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。 批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。BN主要是让收敛变快。 BN需要学习的参数有两个：拉伸（scale）参数 γ和偏移（shift）参数 β 分为两种情况 对全连接层BN 批量归一化层置于全连接层中的仿射变换x和激活函数φ之间， x→BN→φ BN实现如下： 对卷积层BN BN层置于卷积后、激活函数φ之前。 假设卷积后通道数n，对于每个通道，需要分别做BN，每个通道都有独立的γ和 β。总共需要学习的参数为2n 预测时的批量归一化 使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。 未来发展在2014年Hinton给出了一个名为“What is wrong with CNN”的演讲，提出了四点质疑【10】。 这些质疑都预示着，CNN的进一步发展不能再仅仅依靠简单的改改模型了。在前沿的研究中，其中很有前景的是这三个方向。分别是，基于球体做CNN，在流形上做CNN和在图上做CNN。这三者都可以看作是将传统CNN从欧几里得空间向外延展到非欧空间，分别到黎曼空间，流形空间和图上。 循环神经网络语言模型 注意，这里的P 指的是我们需要的决策概率。 P的计算如下：（有点像贝叶斯统计推断） n 元语法通过马尔科夫假设，简化语言模型的计算 这里的马尔可夫假设是指一个词的出现只与前面n个词相关，即n阶马尔可夫链（Markov chain of order n） RNN它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。 词嵌入（word2vec）参考：Word2Vec-知其然知其所以然 word2vec 中的数学原理详解 词嵌入，也就是用来对词进行编码，将文本词语映射到实数域向量中。 word2vec工具2012年由Google提出。包含了两个模型，即跳字模型（skip-gram） 和连续词袋模型（continuous bag of words，CBOW）。两个模型分别从两个角度来建立词的预测模型，CBOW是通过一个或多个单词的上下文来进行这个词语的预测，而Skip Gram模型是通过一个或多个单词来进行上下文的预测【13】。 skip-gram 训练结束后，对于词典中的任一索引为ii的词，我们均得到该词作为中心词和背景词的两组词向量vi和ui。在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。 CBOW 与跳字模型最大的不同在于，连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词。 同跳字模型不一样的一点在于，我们一般使用连续词袋模型的背景词向量作为词的表征向量。 Word2vec 的训练为了降低计算复杂度，介绍两种近似训练方法，即负采样（negative sampling）和层序softmax（hierarchical softmax）。 负采样通过考虑同时含有正类样本和负类样本的相互独立事件来构造损失函数。其训练中每一步的梯度计算开销与采样的噪声词的个数线性相关。 层序softmax使用了二叉树，并根据根结点到叶结点的路径来构造损失函数。其训练中每一步的梯度计算开销与词典大小的对数相关。 参考 1. 深度学习简介 知乎：什么是 word embedding? 3.2. 线性回归的从零开始实现 3.3. 线性回归的简洁实现 3.11. 模型选择、欠拟合和过拟合 3.12. 权重衰减 3.14. 正向传播、反向传播和计算图 CNN简史 CNN经典网络解读 Geoffrey Hinton talk “What is wrong with convolutional neural nets ?” 经典CNN结构简析：AlexNet、VGG、NIN、GoogLeNet、ResNet etc. GoogLeNet网络模型 白话word2vec","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}],"author":"Dufy"},{"title":"⭐️Python笔记","slug":"Python笔记","date":"2019-12-10T16:00:00.000Z","updated":"2021-04-25T15:59:09.059Z","comments":true,"path":"2019/12/11/Python笔记/","link":"","permalink":"http://yoursite.com/2019/12/11/Python笔记/","excerpt":"","text":"Python简介历史参考：Python简史 1989年圣诞节：Guido von Rossum开始写Python语言的编译器。 1991年2月：第一个Python编译器（同时也是解释器）诞生，它是用C语言实现的（后面），可以调用C语言的库函数。在最早的版本中，Python已经提供了对“类”，“函数”，“异常处理”等构造块的支持，还有对列表、字典等核心数据类型，同时支持以模块为基础来构造应用程序。 1994年1月：Python 1.0正式发布。 2000年10月16日：Python 2.0发布，增加了完整的垃圾回收，提供了对Unicode的支持。与此同时，Python的整个开发过程更加透明，社区对开发进度的影响逐渐扩大，生态圈开始慢慢形成。 2008年12月3日：Python 3.0发布，它并不完全兼容之前的Python代码，不过因为目前还有不少公司在项目和运维中使用Python 2.x版本，所以Python 3.x的很多新特性后来也被移植到Python 2.6/2.7版本中。 编译型 or 解释型二者的执行过程不同： 解释型优点： 一次编译，处处执行(Java的最大卖点) 可以确保你看到的代码就是你执行的代码(python) 弱类型，开发速度很快 python解释器 https://www.liaoxuefeng.com/wiki/897692888725344/966138843228672 Python的解释器很多，但使用最广泛的还是CPython。如果要和Java或.Net平台交互，最好的办法不是用Jython或IronPython，而是通过网络调用来交互，确保各程序之间的独立性。 在 说说Python程序的执行过程 里面提到： 在 from aa import pr 时候，会生产 .pyc临时文件 pyc的目的是重用 回想本文的第二段在解释编译型语言和解释型语言的优缺点时，我说编译型语言的优点在于，我们可以在程序运行时不用解释，而直接利用已经“翻译”过的文件。也就是说，我们之所以要把py文件编译成pyc文件，最大的优点在于我们在运行程序时，不需要重新对该模块进行重新的解释。 java 中的 JIT(just-in-time) JIT 就是说在解释型语言中，对于经常用到的或者说有较大性能提升的代码在解释的时候编译成机器码，其他一次性或者说没有太大性能提升的代码还是以字节码的方式执行。这样的话，就能在保证移植性的同时，又能让性能提升一大截 在 CPython 中是没有 JIT 这个功能 python 特点 要知道Python擅长什么 Python之父说：大部分觉得Python慢的应用都是没有正确地使用Python。对于CPU密集型的任务有多种方法来提升性能–使用Numpy来做计算，调用外部C代码，以及尽量避免GIL锁。 python 缺点 Python的缺点主要集中在以下几点。 执行效率稍低，对执行效率要求高的部分可以由其他语言（如：C、C++）编写。 代码无法加密，但是现在很多公司都不销售卖软件而是销售服务，这个问题会被弱化。 在开发时可以选择的框架太多（如Web框架就有100多个），有选择的地方就有错误。 Python从一开始就特别在意可拓展性(extensibility)。 Python可以在多个层次上拓展。从高层上，你可以引入.py文件。在底层，你可以引用C语言的库。 Python程序员可以快速的使用Python写.py文件作为拓展模块。但当性能是考虑的重要因素时，Python程序员可以深入底层，写C程序，编译为.so文件引入到Python中使用。 Python就好像是使用钢构建房一样，先规定好大的框架。而程序员可以在此框架下相当自由的拓展或更改。 到今天，Python的框架已经确立。Python语言以对象为核心组织代码(Everything is object)，支持多种编程范式(multi-paradigm)，采用动态类型(dynamic typing)，自动进行内存回收(garbage collection)。Python支持解释运行(interpret)，并能调用C库进行拓展。Python有强大的标准库 (battery included)。由于标准库的体系已经稳定，所以Python的生态系统开始拓展到第三方包。这些包，如Django, web.py, wxpython, numpy, matplotlib,PIL，将Python升级成了物种丰富的热带雨林。 参考书 《流畅的python》，这本书都是基础知识， 《Python编程快速上手 让繁琐工作自动化》 《面向对象分析与设计》 《python源码剖析》，这本书是python的源代码，c语言写的，精通python必读 Cpython 源码学习 Your Guide to the CPython Source Code 中文翻译：https://zhuanlan.zhihu.com/p/79656976 变量作用域应该尽量减少对全局变量的使用 Python查找一个变量时会按照“局部作用域”、“嵌套作用域”、“全局作用域”和“内置作用域”的顺序进行搜索，前三者我们在上面的代码中已经看到了，所谓的“内置作用域”就是Python内置的那些标识符，我们之前用过的input、print、int等都属于内置作用域。 再看看下面这段代码，我们希望通过函数调用修改全局变量a的值，但实际上下面的代码是做不到的。 12345678def foo(): a = 200 print(a) # 200if __name__ == '__main__': a = 100 foo() print(a) # 100 如果我们希望在foo函数中修改全局作用域中的a，代码如下所示。 123456789def foo(): global a a = 200 print(a) # 200if __name__ == '__main__': a = 100 foo() print(a) # 200 在实际开发中，我们应该尽量减少对全局变量的使用，因为全局变量的作用域和影响过于广泛，可能会发生意料之外的修改和使用，除此之外全局变量比局部变量拥有更长的生命周期，可能导致对象占用的内存长时间无法被垃圾回收。 事实上，减少对全局变量的使用，也是降低代码之间耦合度的一个重要举措，同时也是对迪米特法则的践行。减少全局变量的使用就意味着我们应该尽量让变量的作用域在函数的内部，但是如果我们希望将一个局部变量的生命周期延长，使其在定义它的函数调用结束后依然可以使用它的值，这时候就需要使用闭包。 说明： 很多人经常会将“闭包”和“匿名函数”混为一谈，但实际上它们并不是一回事，如果想了解这个概念，可以看看维基百科的解释或者知乎上对这个概念的讨论。 print 修改print 输出形式 输出默认传参：sep=’ ‘, end=’\\n’ 123a = [&apos;a&apos;, &apos;b&apos;, &apos;d&apos;]for i in a: print(i) abd 修改 end=’*’ 1234a = ['a', 'b', 'd']for i in a: print(i, end='*')a*b*d* 修改 sep 1234print('abc','sdf', 'oo', sep='**')print('abc','sdf', 'oo')abc**sdf**ooabc sdf oo callable() 判断是否可以被调用,也就是后面能否加() 1234def func(): passprint(callable(func))print(callable(123)) TrueFalse id()，查看变量内存地址123lst = [11,33]lst2 = [11,33]print(id(lst), id(lst2)) 4350061696 4350169328 哈希 hash()，获取哈希值 1234a = '哈哈'c = '哈哈'b = '呵呵'print(hash(a), hash(b), hash(c), sep='\\n') 9176276550143421218-3214937411119109839176276550143421218 MD5 123456import hashlibobj = hashlib.md5()obj.update('jiajia'.encode('utf-8'))resilt = obj.hexdigest()print(resilt) #cd611a31ea969b908932d44d126d195b zip()，拉链函数实现：https://docs.python.org/zh-cn/3/library/functions.html#zip 123456789101112def zip(*iterables): # zip('ABCD', 'xy') --&gt; Ax By sentinel = object() iterators = [iter(it) for it in iterables] while iterators: result = [] for it in iterators: elem = next(it, sentinel) if elem is sentinel: return result.append(elem) yield tuple(result) 1234567l1 = [1, 2, 3]l2 = ['s', 'w', 't']l3 = 'jkl'a = zip(l1, l2, l3) # &lt;class 'zip'&gt;print('__iter__' in dir(a)) # 可迭代的for i in a: print(i) True(1, ‘s’, ‘j’)(2, ‘w’, ‘k’)(3, ‘t’, ‘l’) 14. 最长公共前缀 12345678910class Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: # 解法2 res = '' for i in zip(*strs): if len(set(i)) == 1: res += i[0] else: break return res 1234strs = [&quot;flower&quot;,&quot;fla&quot;,&quot;flight11&quot;]min(len(i) for i in strs)for i in zip(*strs): print(i) 123(&apos;f&apos;, &apos;f&apos;, &apos;f&apos;)(&apos;l&apos;, &apos;l&apos;, &apos;l&apos;)(&apos;o&apos;, &apos;a&apos;, &apos;i&apos;) filter()123456lst = [1,54,3,1,6,8,100]# 找出能被3整除的数f = filter(lambda x: x%3==0, lst)print(f)print('__iter__' in dir(f))print(list(f)) &lt;filter object at 0x10c0ce250&gt;True[54, 3, 6] map()将列表中每一项加1 123lst = [1,54,3]m = map(lambda x: x+1, lst)print(list(m)) [2, 55, 4] int 不同进制 二进制（如0b100，换算成十进制是4） 八进制（如0o100，换算成十进制是64） 十进制（100） 十六进制（0x100，换算成十进制是256） 在python 中可以直接翻译成十进制 12&gt;&gt;&gt; 0o10064 进制转换 bin/oct/hex 1234a = 5print(bin(a)) # 转为二进制b = 0b101print(b) 0b1015 123a = 11print(oct(a)) #八进制print(hex(a)) # 十六进制 0o130xb 运算 以下假设变量a为10，变量b为21： 运算符 描述 实例 + 加 - 两个对象相加 a + b 输出结果 31 - 减 - 得到负数或是一个数减去另一个数 a - b 输出结果 -11 * 乘 - 两个数相乘或是返回一个被重复若干次的字符串 a * b 输出结果 210 / 除 - x 除以 y b / a 输出结果 2.1 % 取模 - 返回除法的余数 b % a 输出结果 1 // 取整除 - 向下取接近商的整数 &gt;&gt;&gt; 9//2 4 &gt;&gt;&gt; -9//2 -5 ** 幂 - 返回x的y次幂 a**b 为10的21次方 赋值 12345678910# 赋值a = b = c =0print(a,b,c)a, b, c = 1,4,7print(a,b,c)# 同步赋值,注意运算时，都利用原始值a, b = 2,5a, b = a+b, aprint(a,b) 1230 0 01 4 77 2 str 运算：+ 、* 互动，input() 12name = input(&apos;请输入名字：&apos;)print(name) 索引 123s = &apos;我喜欢你&apos;s1= s[2]print(s[2], s[-1]) 欢 你 切片 s[start：end：step]，结束位置取不到 capitalize()：首字母大写 12s = 'i am Saler, SD dsf'print(s.capitalize()) I am saler, sd dsf lower(), upper()：全部变为大小写 12print(s.lower())print(s.upper()) i am saler, sd dsfI AM SALER, SD DSF 注意，忽略大小写推荐优先使用upper() 如：转换为小写后，不一样，但是大写却一样 123456789101112131415print('μ')print('µ')s1 = 'μ'.upper()s2 = 'µ'.upper()print(s1.encode('utf8'))print(s2.encode('utf8'))b'\\xce\\x9c'b'\\xce\\x9c's1 = 'μ'.lower()s2 = 'µ'.lower()print(s1.encode('utf8'))print(s2.encode('utf8'))b'\\xce\\xbc'b'\\xc2\\xb5' center(), 将字符串补充至固定长度 123456s= 'wer's1= s.center(10)print(s1)print(s.center(10,'*')) wer ***wer**** strip(), 去掉字符串两端的空格（空白、\\t (tab键)、\\n） 12s = &apos; \\t \\n 哥 特让他 \\t&apos;print(s.strip()) 哥 特让他 e.g. input()时候，紧接着使用一个strip()，这样即使用户输入 ‘admin ‘，仍能通过 指定字符去除 12s = &apos;dfjkjhgfdd&apos;print(s.strip(&apos;df&apos;)) jkjhg replace(), 字符串替换 12s = &apos;今天的天气&apos;print(s.replace(&apos;天&apos;, &apos;*&apos;)) # 今*的*气 正则替换 1234import res = 'sdf(sas)90SFopQS-,ds??d's_clear = re.sub(r'[^A-Z0-9]', '', s) # 使用正则替换所有 非大写和数字 的字符print(s_clear) # 90SFQS split()， 字符串切割 12s = &apos;张无忌_赵敏_周芷若_令狐&apos;print(s.split(&apos;_&apos;)) #默认以空格为分隔符 [‘张无忌’, ‘赵敏’, ‘周芷若’, ‘令狐’] join(), 把列表合成字符串 123l = ['张无忌', '赵敏', '周芷若', '令狐']print('_爱_'.join(l))张无忌_爱_赵敏_爱_周芷若_爱_令狐 格式化输出 %s, d%, f% 123print(&apos;我叫%s, 年龄%s&apos; % (&apos;竹节&apos;,10))print(&apos;我叫%s, 年龄%d&apos; % (&apos;竹节&apos;,10))print(&apos;我叫%s, 年龄%0.2f&apos; % (&apos;竹节&apos;,10.2134)) 我叫竹节, 年龄10我叫竹节, 年龄10我叫竹节, 年龄10.21 f’..{}..’ 123name = '莉莉'r = 32.1987print(f'我叫&#123;name&#125;, 喜欢&#123;r:.2f&#125;') 我叫莉莉, 喜欢32.20 查找 startswith()， 判断是否以..开头 endswith(), 判断结尾 12345name = '张为'print(name.startswith('张'))print(name.startswith('里'))name1 = 'cyzxz'name1.startswith(('x1', 'yz', 'c')), name.startswith('yz') # (True, False)，满足任意一个即可 count()，计数 12s = &apos;asfdsd&apos;print(s.count(&apos;d&apos;)) # 2 find(), 查找，返回索引，找不到返回 -1 12s = &apos;我最喜欢PythonP&apos;print(s.find(&apos;P&apos;)) # 4 1print(s.find(&apos;最喜欢&apos;)) # 1 inex()找不到会报错 12s = &apos;我最喜欢PythonP&apos;print(s.index(&apos;A&apos;)) ValueError: substring not found index 1str.index(str, beg=0, end=len(string)) 参数： str – 指定检索的字符串 beg – 开始索引，默认为0。 end – 结束索引，默认为字符串的长度。 123a = 'qwe?tui?'a.index('?') # 3a.index('?',4) # 7 isdigit(),判断字符串是否全部由整数组成，不包括小数点 123print(&apos;126.7&apos;.isdigit())print(&apos;1267&apos;.isdigit())print(&apos;126ddd7&apos;.isdigit()) FalseTrueFalse isalpha()， 判断字符串是否全部由字母组成 12a = &apos;runoobDFSD菜鸟教程&apos;a.isalpha() # True 中文的汉字会被 isalpha 判定为 True 如果想区分中文和英文可以使用 unicode。 12a = 'runoobDFSD菜鸟教程'a.encode('UTF-8').isalpha() # False 12a = &apos;ruSμ&apos;a.encode(&apos;UTF-8&apos;).isalpha() # False eval() 可以把字符串当代码去执行，有返回值 12s = &apos;1+1&apos;print(eval(s), type(eval(s))) # 2 &lt;class &apos;int&apos;&gt; 再比如，我们需要将 “[‘哈哈’, ‘混合’, ‘呵呵’]”—&gt;&gt;[‘哈哈’, ‘混合’, ‘呵呵’] 123a = \"['哈哈', '混合', '呵呵']\"lst = eval(a)print(lst) #['哈哈', '混合', '呵呵'] exec(),没有返回值 123s = 'aaa=100'exec(s)print(aaa) # 100 repr() 一个字符串最应该表现的形式 123print(str(&apos;你好，我叫\\t增加了 &apos;))print(repr(&apos;你好，我叫\\t增加了 &apos;))print(repr(&quot;你好，&apos;我叫\\t增加了 &quot;)) 编码参考 【13】 ASCII、Unicode和UTF-8 Unicode 是「字符集」 UTF-8 、ASCII等则是「编码规则」 粗略地说就是，字符编码提供一种映射，使屏幕上显示的内容和内存、磁盘内存储的内容对应起来。有许多种不同的字符编码 在Python 3，所有的字符串都是使用Unicode编码的字符序列 ASCII 8位 –&gt;&gt; 0000 0000 0000 0000 ANSI(16位) –&gt;&gt;GBK(国标码的扩展码)–&gt;&gt;万国码Unicode（32位） 123字母A： 0100 0001 # ACSII字母A： 0000 0000 0100 0001 # 国标码字母A： 0000 0000 0100 0001 0000 0000 0100 0001 #Unicode unicode 目前的Unicode字符分为17组编排，0x0000至0x10FFFF，每组称为平面（Plane），而每平面拥有65536个码位，共1114112个。然而目前只用了少数平面。UTF-8、UTF-16、UTF-32都是将数字转换到程序数据的编码方案。 不同位置代表的字符可以查：https://home.unicode.org/， 十六进制表示 ord(), chr() ord()查 看编码对应的十进制数 chr() 函数返回代表指定 unicode 的字符。 12print(ord('c')) #99print(chr(99)) # 'c' \\后面还可以跟一个八进制或者十六进制数来表示字符，例如\\141和\\x61都代表小写字母a，前者是八进制的表示法，后者是十六进制的表示法。也可以在\\后面跟Unicode字符编码来表示字符 0x266a(十六进制) &lt;——&gt; 9834(十进制) 12'\\u266A', chr(9834), chr(0x266a)('♪', '♪', '♪') 1'\\u4e2d\\u6587\\u266A\\u1555' # ('中文♪ᕕ', 'ᕕ') 字符展示： 12for i in range(65536): print(chr(i), end=&apos; &apos;) 输出大写、小写字母和数字： 1234print([chr(i) for i in range(65, 91)]) # 所有大写字母print([chr(i) for i in range(97, 123)]) # 所有小写字母print([chr(i) for i in range(48, 58)]) # 所有数字print([chr(i) for i in range(19968, 40960)]) # 所有中文，汉字集 \\0x4E00-0x9FFF，十六进制，对应 19968-40959 1234[&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;D&apos;, &apos;E&apos;, &apos;F&apos;, &apos;G&apos;, &apos;H&apos;, &apos;I&apos;, &apos;J&apos;, &apos;K&apos;, &apos;L&apos;, &apos;M&apos;, &apos;N&apos;, &apos;O&apos;, &apos;P&apos;, &apos;Q&apos;, &apos;R&apos;, &apos;S&apos;, &apos;T&apos;, &apos;U&apos;, &apos;V&apos;, &apos;W&apos;, &apos;X&apos;, &apos;Y&apos;, &apos;Z&apos;][&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, &apos;g&apos;, &apos;h&apos;, &apos;i&apos;, &apos;j&apos;, &apos;k&apos;, &apos;l&apos;, &apos;m&apos;, &apos;n&apos;, &apos;o&apos;, &apos;p&apos;, &apos;q&apos;, &apos;r&apos;, &apos;s&apos;, &apos;t&apos;, &apos;u&apos;, &apos;v&apos;, &apos;w&apos;, &apos;x&apos;, &apos;y&apos;, &apos;z&apos;][&apos;0&apos;, &apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;, &apos;7&apos;, &apos;8&apos;, &apos;9&apos;][&apos;一&apos;, &apos;丁&apos;, &apos;丂&apos;, &apos;七&apos;, &apos;丄&apos;, &apos;丅&apos;, &apos;丆&apos;, &apos;万&apos;, &apos;丈&apos;, &apos;三&apos;, &apos;上&apos;, &apos;下&apos;, &apos;丌&apos;, &apos;不&apos;, &apos;与&apos;, &apos;丏&apos;, &apos;丐&apos;, &apos;丑&apos;, &apos;丒&apos;, &apos;专&apos;, &apos;且&apos;, &apos;丕&apos;, &apos;世&apos;, &apos;丗&apos;, &apos;丘&apos;, &apos;丙&apos;, &apos;业&apos;, &apos;丛&apos;, &apos;东&apos;, &apos;丝&apos;, &apos;丞&apos;, &apos;丟&apos;, &apos;丠&apos;, &apos;両&apos;, &apos;丢&apos;, &apos;丣&apos;, &apos;两&apos;, &apos;严&apos;, &apos;並&apos;, &apos;丧&apos;, &apos;丨&apos;, &apos;丩&apos;, &apos;个&apos;, &apos;丫&apos;, &apos;丬&apos;, &apos;中&apos;, &apos;丮&apos;, &apos;丯&apos;, &apos;丰&apos;, &apos;丱&apos;, &apos;串&apos;, &apos;丳&apos;, &apos;临&apos;, &apos;丵&apos;, &apos;丶&apos;, &apos;丷&apos;, &apos;丸&apos;, &apos;丹&apos;, &apos;为&apos;, &apos;主&apos;, &apos;丼&apos;, &apos;丽&apos;, &apos;举&apos;, &apos;丿&apos;, &apos;乀&apos;, &apos;乁&apos;, &apos;乂&apos;, &apos;乃&apos;, &apos;乄&apos;, &apos;久&apos;, &apos;乆&apos;, &apos;乇&apos;, &apos;么&apos;, &apos;义&apos;, &apos;乊&apos;, &apos;之&apos;, &apos;乌&apos;, &apos;乍&apos;, &apos;乎&apos;, &apos;乏&apos;, &apos;乐&apos;, &apos;乑&apos;, &apos;乒&apos;, &apos;乓..... 缺点 Unicode太浪费空间！！（内存中存储的字符串默认使用unicode，目的是Unicode定长，好处理） 于是提出UTF-8（8-bit Unicode Transformation Format）， 可变长度编码 utf8不同语言所占内存空间大小： 英文：1byte 欧洲：2byte 中文：3byte 总结： encode() 与decode()由于Python的字符串类型是str，在内存中以Unicode表示，一个字符对应若干个字节。如果要在网络上传输，或者保存到磁盘上，就需要把str变为以字节为单位的bytes。 以Unicode表示的str通过encode()方法可以编码为指定的bytes，例如： 1234&gt;&gt;&gt; 'ABC'.encode('ascii')b'ABC'&gt;&gt;&gt; '中文'.encode('utf-8')b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' bytes为&lt;class ‘bytes’&gt; b’\\xe4\\xb8\\xad\\xe5\\x9b\\xbd’, 每个\\x一个字节 反过来，如果我们从网络或磁盘上读取了字节流，那么读到的数据就是bytes。要把bytes变为str，就需要用decode()方法： decode()把bytes转为字符串，如果发生UnicodeDecodeError，需要切换解码方式 1234&gt;&gt;&gt; b'ABC'.decode('ascii')'ABC'&gt;&gt;&gt; b'\\xe4\\xb8\\xad\\xe6\\x96\\x87'.decode('utf-8')'中文' 比如，程序得到如下输出内容： 12345------------------------------SOMC1601180KGEJ ----&gt; id: \"49\"label_id: \"189\"prob: 3.0label_name: \"\\346\\216\\222\\351\\230\\273\" label_name 的解析： 1print(b'\\346\\216\\222\\351\\230\\273'.decode('utf8')) # 注意，这里是八进制 排阻 拓展 bytes,一堆字节，python 程序中最小的数据单位 图片，mp3，视频等都要用bytes来读取 应用 1) 计算机系统通用的字符编码工作方式 参考：字符串和编码 在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。 用记事本编辑的时候，从文件读取的UTF-8字符被转换为Unicode字符到内存里，编辑完成后，保存的时候再把Unicode转换为UTF-8保存到文件： 浏览网页的时候，服务器会把动态生成的Unicode内容转换为UTF-8再传输到浏览器： 所以你看到很多网页的源码上会有类似&lt;meta charset=&quot;UTF-8&quot; /&gt;的信息，表示该网页正是用的UTF-8编码。 2）检测字符串是否全为中文字符： 123456def is_all_chinese(check_str): for ch in check_str: if not u'\\u4e00' &lt;= ch &lt;= u'\\u9fff': return False return Trueis_all_chinese('水黼电费'), is_all_chinese('水黼电费aa'), \"\\u4e01\", chr(40000), ord('\\u9fff'), ord('\\u4e00') 1(True, False, &apos;丁&apos;, &apos;鱀&apos;, 40959, 19968) 3）去除标点 12345def remove_punctuation(line): rule = re.compile(r\"[^a-zA-Z0-9\\u4e00-\\u9fff]\") # 此处可以扩展 line = rule.sub('*',line) return lineremove_punctuation('sds按时发斯蒂 是多少芬,sd s!! 1/2w、`是') 1&apos;sds按时发斯蒂*是多少芬*sd*s***1*2w**是&apos; 文件操作 模式 注意，’w‘ 与 ‘a’ 区别： 主要是下次打开文件时候，会不会将之前的内容清空 e.g. finally块的代码不论程序正常还是异常都会执行到（甚至是调用了sys模块的exit函数退出Python环境，finally块都会被执行，因为exit函数实质上是引发了SystemExit异常），因此我们通常把finally块称为“总是执行代码块”， 12345678910111213141516171819202122232425262728293031323334353637from math import sqrtdef is_prime(n): \"\"\"判断素数的函数\"\"\" assert n &gt; 0 for factor in range(2, int(sqrt(n)) + 1): if n % factor == 0: return False return True if n != 1 else Falsedef main(): filenames = ('a.txt', 'b.txt', 'c.txt') fs_list = [] try: for filename in filenames: fs_list.append(open(filename, 'w', encoding='utf-8')) for number in range(1, 100): if is_prime(number): if number &lt; 10: fs_list[0].write(str(number) + '\\n') elif number &lt; 50: fs_list[1].write(str(number) + '\\n') else: fs_list[2].write(str(number) + '\\n') except IOError as ex: print(ex) print('写文件时发生错误!') finally: for fs in fs_list: fs.close() print('操作完成!')if __name__ == '__main__': main() 注意fs_list.append(open(filename, &#39;w&#39;, encoding=&#39;utf-8&#39;))与fs_list.append(open(filename, &#39;a&#39;, encoding=&#39;utf-8&#39;)) 区别 读取时的路径写法 b模式，一般处理非文本文件，不能指定encoding rb：读取字节 wb：写入字节 读取f1,复制到f2 1234f1 = open('a.png',mode='rb')f2 = open('b/a.png', mode='wb') for line in f1: f2.write(line) f.close() 不要忘 with 这里我们再简要介绍一下with关键字。with关键字用于python的 上下文管理器机制 。为了防止像open这一类文件打开方法在操作过程中出现异常或者错误，或者是操作结束后忘记关闭文件而导致文件非正常关闭出现文件泄露、破坏等后果，python提供了with这个上下文管理器机制，保证文件会正常关闭，不需要手动输入close语句。这里注意with语句后面加冒号，操作命令语句注意缩进 文件修改操作 涉及到文件的读写、替换，删除和重命名 123456789import oswith open(r'test.txt', mode='r', encoding='utf8') as f1, \\ open(r'test_副本.txt', mode='w', encoding='utf8') as f2: for line in f1: if '的' in line: line = line.replace('的', 'AA') f2.write(line)os.remove('test.txt') # 删除文件os.rename('test_副本.txt','test.txt') 结果如下： 序列化和反序列化我们已经知道如何将文本数据和二进制数据保存到文件中，序列化则是希望把一个列表或者一个字典中的数据保存到文件中 什么是序列化和反序列化？ 自由的百科全书维基百科上对这两个概念是这样解释的：“序列化（serialization）在计算机科学的数据处理中，是指将数据结构或对象状态转换为可以存储或传输的形式，这样在需要的时候能够恢复到原先的状态，而且通过序列化的数据重新获取字节时，可以利用这些字节来产生原始对象的副本（拷贝）。与这个过程相反的动作，即从一系列字节中提取数据结构的操作，就是反序列化（deserialization）”。 JSON 文件如果希望把一个列表或者一个字典中的数据保存到文件中，将数据以JSON格式进行保存 json是我们前后端交互的枢纽. 相当于编程界的普通话. 大家沟通都用json. 为什么这样呢? 因为json的语法格式可以完美的表示出一个对象. 那什么是json: json全称javascript object notation. 翻译过来叫js对象简谱. JSON的数据类型和Python的数据类型对应关系如下： Python JSON dict object list, tuple array str string int, float, int- &amp; float-derived Enums number True / False true / false None null 1234s = &#123;\"name\":\"ad\", \"code\":12, 's':True, 'hh':None&#125;print(json.dumps(s))-------------------------&#123;\"name\": \"ad\", \"code\": 12, \"s\": true, \"hh\": null&#125; json模块主要有四个比较重要的函数，分别是： 1）dump - 将Python对象按照JSON格式序列化到文件中 2）dumps - 将Python对象处理成JSON格式的字符串 3）load - 将文件中的JSON数据反序列化成对象 4）loads - 将字符串的内容反序列化成Python对象 dumps() dict 转json 1234import jsondic = &#123;'name':'ad', 'code':12&#125;s = json.dumps(dic)print(type(s), s) &lt;class ‘str’&gt; {“name”: “ad”, “code”: 12} 注意，涉及到中文会有坑,要加 ensure_ascii=False 1234dic = &#123;&apos;name&apos;:&apos;迭代似懂非懂ad&apos;, &apos;code&apos;:12&#125;s = json.dumps(dic)print(s)print(ascii(&apos;迭代似懂非懂ad&apos;)) {“name”: “\\u8fed\\u4ee3\\u4f3c\\u61c2\\u975e\\u61c2ad”, “code”: 12} ‘\\u8fed\\u4ee3\\u4f3c\\u61c2\\u975e\\u61c2ad’ 改为这样： 123dic = &#123;'name':'迭代似懂非懂ad', 'code':12&#125;s = json.dumps(dic, ensure_ascii=False)print(s) {“name”: “迭代似懂非懂ad”, “code”: 12} 列表一样可以进行类似转换： 123lst = ['孙悟空', '鲁达',True, '林冲']s = json.dumps(lst, ensure_ascii=False)print(s, type(s)) [“孙悟空”, “鲁达”, true, “林冲”] &lt;class ‘str’&gt; loads() 前端返回字符串，变为python 中字典 1234s = '&#123;\"name\":\"迭代似懂非懂ad\", \"code\":12&#125;'d =json.loads(s)print(d, type(d))print(d['name']) {‘name’: ‘迭代似懂非懂ad’, ‘code’: 12} &lt;class ‘dict’&gt;迭代似懂非懂ad 注意，s = ‘{“name”:”迭代似懂非懂ad”, “code”:12}’里面要用” “双引号 写入到文件, dump() 1234dic = &#123;\"a\": \"女王\", \"b\": \"萝莉\", \"c\": \"小清新\"&#125;f = open(\"test.json\", mode=\"w\", encoding=\"utf-8\")json.dump(dic, f, ensure_ascii=False) # 把对象打散成json写入到文件中f.close() 读取, load() 1234f = open(\"test.json\", mode=\"r\", encoding=\"utf-8\")dic = json.load(f)f.close()print(dic) {‘a’: ‘女王’, ‘b’: ‘萝莉’, ‘c’: ‘小清新’} pickle序列化后的数据只能被Python识别 1234import picklelst = ['孙悟空', '鲁达', '林冲']bs = pickle.dumps(lst)print(type(bs), bs) &lt;class ‘bytes’&gt; b’\\x80\\x03]q\\x00(X\\t\\x00\\x00\\x00\\xe5\\xad\\x99\\xe6\\x82\\x9f\\xe7\\xa9\\xbaq\\x01X\\x06\\x00\\x00\\x00\\xe9\\xb2\\x81\\xe8\\xbe\\xbeq\\x02X\\x06\\x00\\x00\\x00\\xe6\\x9e\\x97\\xe5\\x86\\xb2q\\x03e.’ 还原： 12bs = b'\\x80\\x03]q\\x00(X\\t\\x00\\x00\\x00\\xe5\\xad\\x99\\xe6\\x82\\x9f\\xe7\\xa9\\xbaq\\x01X\\x06\\x00\\x00\\x00\\xe9\\xb2\\x81\\xe8\\xbe\\xbeq\\x02X\\x06\\x 00\\x00\\x00\\xe6\\x9e\\x97\\xe5\\x86\\xb2q\\x03e.'print(pickle.loads(bs)) [‘孙悟空’, ‘鲁达’, ‘林冲’] load() , dump() 12lst = ['孙悟空', '鲁达', '林冲']pickle.dump(lst, open('l.data', mode='wb')) 123#读取序列化后的文件l = pickle.load(open('l.data', mode='rb'))print(type(l), l) &lt;class ‘list’&gt; [‘孙悟空’, ‘鲁达’, ‘林冲’] 列表要注意方法的复杂度【10】 Operation Average Case Amortized Worst Case Copy O(n) O(n) Append[1] O(1) O(1) Pop last O(1) O(1) Pop intermediate[2] O(n) O(n) Insert O(n) O(n) Get Item O(1) O(1) Set Item O(1) O(1) Delete Item O(n) O(n) Iteration O(n) O(n) Get Slice O(k) O(k) Del Slice O(n) O(n) Set Slice O(k+n) O(k+n) Extend[1] O(k) O(k) Sort O(n log n) O(n log n) Multiply O(nk) O(nk) x in s O(n) min(s), max(s) O(n) Get Length O(1) O(1) 操作 方法 实例 结果 追加(尾部) append() l = [‘a’, ‘b’]l.append(‘c’) print(l) [‘a’, ‘b’, ‘c’] 插入(索引) insert(index) l = [‘a’, ‘b’]l.insert(1,’c’)print(l) [‘a’, ‘c’, ‘b’] 合并新增 extend() l = [‘a’, ‘b’] l1 = [‘c’, ‘z’] l.extend(l1) print(l) [‘a’, ‘b’, ‘c’, ‘z’] 删除 remove() l = [‘a’, ‘b’, ‘c’, ‘z’, ‘b’]l.remove(‘b’)print(l) [‘a’, ‘c’, ‘z’, ‘b’] 弹出 pop() l = [‘a’, ‘b’, ‘c’, ‘z’, ‘b’]a = l.pop() # 返回最后一个 print(a, l) b [‘a’, ‘b’, ‘c’, ‘z’] pop(index) l = [‘a’, ‘b’, ‘c’, ‘z’, ‘b’]a = l.pop(1) # 指定索引删除 print(a, l) b [‘a’, ‘c’, ‘z’, ‘b’] 清空 clear() l = [‘a’, ‘b’, ‘c’, ‘z’, ‘b’] l.clear() print(l) [] 计数 count() l = [‘a’, ‘b’, ‘c’, ‘b’, ‘b’] print(l.count(‘b’)) 3 翻转 reverse() l = [‘a’, ‘b’, ‘c’, ‘a’] l.reverse() print(l) [‘a’, ‘c’, ‘b’, ‘a’] 排序 sort() l = [12, 34, 1, 354] l.sort() print(l) [1, 12, 34, 354] sort(reverse=True) l = [12, 34, 1, 354] l.sort(reverse=True) print(l) [354, 34, 12, 1] 注意 append() 效率比insert()高 切片 1234list1 = [1,3,4,5,7,8,9]print(list1[-2:]) #取出最后两个元素print(list1[0:5:2]) #间距取值print(list1[-1::-1]) #列表元素反向 [8, 9][1, 4, 7][9, 8, 7, 5, 4, 3, 1] 12list1[:0] = [-1,-2] # 列表开头插入2元素print(list1) [-1, -2, 1, 3, 4, 5, 7, 8, 9] 列表去重 利用set， 很巧妙的一种方法 123a = [1,3,6,7,5,6,9,6,3]print(list(set(a)))[1, 3, 5, 6, 7, 9] 反转之通杀，针对字符串、list均适用—–&gt;&gt;[::-1] 不改变原始变量 1234ml = [1,4,7,2,6]aa = 'dqwer'print(aa[::-1]) # rewqd print(ml[::-1]) # [6, 2, 7, 4, 1] 生成式 1234[i for i in range(10)] # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9][i for i in range(10) if i%2==0] #偶数,[0, 2, 4, 6, 8][i**2 for i in range(10) ] #平方,[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]mydict = &#123;str(i):i for i in range(4)&#125; #字典,&#123;'0': 0, '1': 1, '2': 2, '3': 3&#125; 123tup = ((1, 2, 3), (4, 5, 6), (7, 8, 9))fl2 = [x for x1 in tup for x in x1]print(fl2) [1, 2, 3, 4, 5, 6, 7, 8, 9] enumerate() 123list1 = ['离离','云上草', '业火']for index,value in enumerate(list1): print(index, value) any(), all()判断 any()，如果有一个为 True，则返回 True。 all —-&gt;and 123aa = ['', 23, 34]print(any(aa)) #Trueall(aa) #False sorted() 如下，二者效果是等价的 123456789101112131415#根据名字长度进行排序a = ['sa', 'wqe', 'ljkhgjh', 'c']#需要自己定义排序规则def func(item): return len(item)ret = sorted(a, key=func)print(ret)------------------------ret = sorted(a,key=lambda x:len(x))print(ret)['c', 'sa', 'wqe', 'ljkhgjh']['c', 'sa', 'wqe', 'ljkhgjh'] 按年龄排序： 123456lst = [ &#123;&apos;name&apos;:&apos;A&apos;, &apos;age&apos;:1&#125;, &#123;&apos;name&apos;:&apos;D&apos;, &apos;age&apos;:6&#125;, &#123;&apos;name&apos;:&apos;V&apos;, &apos;age&apos;:3&#125;,]print(sorted(lst, key=lambda x:x[&apos;age&apos;])) [{‘name’: ‘A’, ‘age’: 1}, {‘name’: ‘V’, ‘age’: 3}, {‘name’: ‘D’, ‘age’: 6}] 与sort()比较 sorted()有返回，sort()无返回；sorted不改变原始的列表，sort()则改变 tuple 意义 一个不变的对象要比可变的对象更加容易维护 如果不需要对元素进行添加、删除、修改的时候，可以考虑使用元组，当然如果一个方法要返回多个值，使用元组也是不错的选择。 元组在创建时间和占用的空间上面都优于列表 创建 12a = (&apos;s&apos;, &apos;1&apos;, &apos;s&apos;)print(a,type(a)) (‘s’, ‘1’, ‘s’) &lt;class ‘tuple’&gt; 注意，只有一个元素时，要这样写： 12a = (&apos;s&apos;,)print(a,type(a)) #(&apos;s&apos;,) &lt;class &apos;tuple&apos;&gt; 字典复杂度：【10】 Operation Average Case Amortized Worst Case k in d O(1) O(n) Copy[3] O(n) O(n) Get Item O(1) O(n) Set Item[1] O(1) O(n) Delete Item O(1) O(n) Iteration[3] O(n) O(n) 在dict中查找某个key是否存在操作的时间复杂度为o（1）；查找某个value是否存在操作的时间复杂度为o（n） 本质：dict作为一个hash链表，是对其key进行hash操作的，而非value。 验证： 1234567891011121314151617181920&gt; import time&gt; num = 10000&gt; dic = &#123;&#125;&gt; for i in range(num):&gt; dic[i] = i&gt; &gt; start = time.time()&gt; for i in range(num):&gt; if i in dic.keys(): # 时间快&gt; pass&gt; end = time.time()&gt; print (end - start)&gt; &gt; start = time.time()&gt; for i in range(num):&gt; if i in dic.values():&gt; pass&gt; end = time.time()&gt; print (end - start)&gt; 0.00188899040222167970.7385590076446533 操作 方法 实例 结果 添加 d[a] = b {a: b} 查询 d[a] 不存在会报错 d.get() d = {‘a’: 1, ‘b’: 2, ‘c’: 3} print(d.get(‘s’)) print(d.get(‘s’, 0)) # 查询不存在，则返回0 print(d.get(‘a’, 0)) print(d) # 原先的d 不变 None01{‘a’: 1, ‘b’: 2, ‘c’: 3} d.setdefault() 查询不到会新增，改变原先dict 基本格式是{key ：value} 字典创建 1）{a:b…..} 1d = &#123;12:&apos;混合&apos;, True:&apos;呵呵&apos;, &apos;name&apos;:1, (1,2):5&#125; 2）dict() 1234567# 字典创建,以下两种方法结果一样d = dict(a=1,b=2,c=3)print(d)def kw_dict(**kwargs): return kwargsprint(kw_dict(a=1,b=2,c=3)) {‘a’: 1, ‘b’: 2, ‘c’: 3}{‘a’: 1, ‘b’: 2, ‘c’: 3} 3）生成式 12items3 = &#123;num: num ** 2 for num in range(1, 10)&#125;&#123;1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81&#125; 4）zip()， 通过zip函数将两个序列压成字典 123&gt;&gt;&gt; dic = dict(zip('abc', [1, 2, 3]))&gt;&gt;&gt; dic&#123;'a': 1, 'c': 3, 'b': 2&#125; 1234l1 = [1, 2, 3]l2 = ['s', 'w', 't']print(dict(zip(l1, l2)))&#123;1: 's', 2: 'w', 3: 't'&#125; 与列表删除的比较 查询 Python 字典 setdefault() 方法和 get()方法 类似, 如果键不已经存在于字典中，将会添加键并将值设为默认值。 注意，get()不改变原dict, setdefault()会改变 e.g. 将列表中＞7的元素放在一起 123456789# 将大于7的放在一起ls = [1,4,8,0,33,5,9]result = &#123;&#125;for i in ls: if i&gt;7: result.setdefault('bigger', []).append(i) else: result.setdefault('smaller', []).append(i)print(result) {‘smaller’: [1, 4, 0, 5], ‘bigger’: [8, 33, 9]} 集合Python中的集合跟数学上的集合是一致的，不允许有重复元素，而且可以进行交集、并集、差集等运算。 复杂度【10】 Operation Average case Worst Case notes x in s O(1) O(n) Union s|t O(len(s)+len(t)) Intersection s&amp;t O(min(len(s), len(t)) O(len(s) * len(t)) replace “min” with “max” if t is not a set Multiple intersection s1&amp;s2&amp;..&amp;sn (n-1)*O(l) where l is max(len(s1),..,len(sn)) Difference s-t O(len(s)) s.difference_update(t) O(len(t)) Symmetric Difference s^t O(len(s)) O(len(s) * len(t)) s.symmetric_difference_update(t) O(len(t)) O(len(t) * len(s)) set和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。 判断值是否在set集合中的速度明显要比list快的多, 因为查找set用到了hash，时间在O(1)级别。 交并差 &amp; | - 12345s1 = &#123;'a', 's', 'q'&#125;s2 = &#123;'s', 'q', 'uu'&#125;print(s1 &amp; s2) #交集print(s1 | s2) #并集print(s1 - s2) #差集 {‘q’, ‘s’}{‘q’, ‘uu’, ‘s’, ‘a’}{‘a’} 增、删 add() 1234s = set()s.add(&apos;西新街&apos;)s.add(&apos;华西街&apos;)print(s) # &#123;&apos;华西街&apos;, &apos;西新街&apos;&#125; remove() 12s.remove(&apos;华西街&apos;)print(s) # &#123;&apos;西新街&apos;&#125; 深浅拷贝 先看赋值 赋值的时候，并未产生新的数据，而是对内存地址的复制 1234567891011l1 = ['克拉玛依', '苹果', '肉']l2 = l1print(l1)print(l2)l1.append('葡萄')print(l1)print(l2)print(id(l1)) # 查看变量内存地址print(id(l2)) [‘克拉玛依’, ‘苹果’, ‘肉’][‘克拉玛依’, ‘苹果’, ‘肉’][‘克拉玛依’, ‘苹果’, ‘肉’, ‘葡萄’][‘克拉玛依’, ‘苹果’, ‘肉’, ‘葡萄’]45111793124511179312 内存地址一样，说明此处‘l2 = l1’ 并没有产生新的对象，而是直接把内存地址复制了一份 换成 copy() 会产生新的列表, 或者使用l1[:]效果一样 1234567891011l1 = [&apos;克拉玛依&apos;, &apos;苹果&apos;, &apos;肉&apos;]l2 = l1.copy()print(l1)print(l2)l1.append(&apos;葡萄&apos;)print(l1)print(l2)print(id(l1)) # 查看变量内存地址print(id(l2)) [‘克拉玛依’, ‘苹果’, ‘肉’][‘克拉玛依’, ‘苹果’, ‘肉’][‘克拉玛依’, ‘苹果’, ‘肉’, ‘葡萄’][‘克拉玛依’, ‘苹果’, ‘肉’]44006036964400634928 但是，copy()是浅拷贝，有坑！！！ 浅拷贝，只拷贝第一层内容 e.g. 123456789101112l1 = ['克拉玛依', '苹果', ['a', 's']]l2 = l1.copy()# l2 = l1[:]print(l1)print(l2)l1[2].append('葡萄')print(l1)print(l2)print(id(l1[2])) # 查看变量内存地址print(id(l2[2])) [‘克拉玛依’, ‘苹果’, [‘a’, ‘s’]][‘克拉玛依’, ‘苹果’, [‘a’, ‘s’]][‘克拉玛依’, ‘苹果’, [‘a’, ‘s’, ‘葡萄’]][‘克拉玛依’, ‘苹果’, [‘a’, ‘s’, ‘葡萄’]]44145464004414546400 copy.deepcopy() 深拷贝 123456789101112import copyl1 = [&apos;克拉玛依&apos;, &apos;苹果&apos;, [&apos;a&apos;, &apos;s&apos;]]l2 = copy.deepcopy(l1)print(l1)print(l2)l1[2].append(&apos;葡萄&apos;)print(l1)print(l2)print(id(l1[2])) # 查看变量内存地址print(id(l2[2])) [‘克拉玛依’, ‘苹果’, [‘a’, ‘s’]][‘克拉玛依’, ‘苹果’, [‘a’, ‘s’]][‘克拉玛依’, ‘苹果’, [‘a’, ‘s’, ‘葡萄’]][‘克拉玛依’, ‘苹果’, [‘a’, ‘s’]]45678159844566749008 补充，列表删除不干净 12345l = [&apos;1a&apos;, &apos;1b&apos;, &apos;1c&apos;, &apos;1d&apos;]for item in l: if item.startswith(&apos;1&apos;): l.remove(item)print(l) [‘1b’, ‘1d’] 使用浅拷贝，修改如下： 1234for item in l.copy(): #浅拷贝，会产生新的列表 if item.startswith(&apos;1&apos;): l.remove(item)print(l) #[] 模块time sleep()，控制程序执行的频率 12import timetime.sleep(1) time()，计算时间差 datetime12from datetime import datetimefrom datetime import date 显示当前时间 12print(datetime.now())print(datetime(2018,1,2,12,12,19)) #创建时间 2020-09-06 14:16:51.0491762018-01-02 12:12:19 时间差 12345t1 = datetime(2018,1,2,12,12,19)t2 = datetime(2018,1,4,22,3,19)diff = t2-t1print(diff)print(diff.total_seconds()) 2 days, 9:51:00208260.0 格式化时间 123t = datetime.now()print(t)print(t.strftime('%Y年%m月%d日 %H时%M分%S秒')) 2020-09-06 14:26:27.7842632020年09月06日 14时26分27秒 strptime(), 字符串转化成时间 123456s1 = input('请输入第一个时间(格式 YYYY-mm-dd HH:MM:SS):')s2 = input('请输入第二个时间(格式 YYYY-mm-dd HH:MM:SS):')t1 = datetime.strptime(s1, '%Y-%m-%d %H:%M:%S') #p: parset2 = datetime.strptime(s2, '%Y-%m-%d %H:%M:%S') #p: parseprint(t2-t1) 请输入第一个时间(格式 YYYY-mm-dd HH:MM:SS):2020-2-23 1:1:12请输入第二个时间(格式 YYYY-mm-dd HH:MM:SS):2020-2-24 12:9:361 day, 11:08:24 date 12print(date.today())print(date(1988,1,13)) 2020-09-061988-01-13 random 随机取一个数 123print(random.random()) # (0,1)print(random.uniform(5,9)) # 随机小数print(random.randint(3,8)) #随机整数，可以取到边界 0.233645804058031485.62641034339873253 从列表中随机选择 choice() 12lst = [&apos;张三&apos;, &apos;李四&apos;, &apos;林冲&apos;, &apos;李逵&apos;]print(random.choice(lst)) 林冲 sample() 12lst = [&apos;孙悟空&apos;, &apos;鲁达&apos;, &apos;林冲&apos;, &apos;李逵&apos;, &apos;武松&apos;]print(random.sample(lst, 2)) [‘鲁达’, ‘李逵’] 练习 随机生成四位验证码 12345678def rand_num(): #产生随机整数 return str(random.randint(0,9))def rand_upper(): #大写字母 return chr(random.randint(65, 90))def rand_lower(): #小写字母范围 return chr(random.randint(97,122)) 123456789101112131415161718def rand_verify_code(n=4): lst = [] for i in range(n): which = random.randint(1,3) if which == 1: #随机数字 s = rand_num() elif which == 2: #随机大写字母 s = rand_upper() elif which == 3: s = rand_lower() lst.append(s) return ''.join(lst)print(rand_verify_code())---------------------------------Wh1JL82Ljt43 或者： 1234567891011121314def rand_verify_code(n=4): li = [rand_num, rand_upper, rand_lower] result = '' for _ in range(n): func = random.choice(li) s = func() result += s return resultprint(rand_verify_code())---------------------------------co5AR075bj4L hashlibmd5一定要加盐，否则容易撞库 MD5加密 MD5是一种不可逆的加密算法. 它是可靠的. 并且安全的 12345678910import hashlib#创建MD5对象obj = hashlib.md5()# 把要加密的信息传递给objobj.update('666666'.encode('utf8'))# 从obj拿到密文mi = obj.hexdigest()print(mi) f379eaf3c831b04de153469d1bec345e 正常加密容易撞库：https://www.cmd5.com/ 加盐–解决撞库 1234# 加盐--解决撞库obj = hashlib.md5(b&apos;ghjkytrerwe&apos;)obj.update(&apos;666666&apos;.encode(&apos;utf8&apos;))print(obj.hexdigest()) # bdac197d85f8671dfd2ebac2c4daa78a MD5应用–用户密码加密 1234567891011121314def func(salt, s): obj = hashlib.md5(salt) obj.update(s.encode('utf8')) return obj.hexdigest()# 用户注册username = input('请输入用户名：')password = input('请输入密码：')# 动态加盐，这里以用户名当盐mi_password = func(username.encode('utf8'), password)f= open('user.txt', mode='w', encoding='utf8')f.write(username)f.write('\\n')f.write(mi_password) #把密文写出去 登录 123456789101112# 登录username = input('请输入用户名：').strip()password = input('请输入密码：').strip()password = func(username.encode('utf8'), password)f = open('user.txt', mode= 'r', encoding='utf8')name = f.readline().strip()upwd = f.readline().strip()# 需要把用户输入的明文加密之后，再进行判断if username == name and password == upwd: print('登录成功')else: print('登录失败') 文件加密 12345678# 文件加密f = open('test.txt', mode='rb')obj = hashlib.md5(b'qwetytyu')for line in f: obj.update(line)# 得到加密结果print(obj.hexdigest()) 可以判断文件一致性 两个相同的文件的md5的值是一致的 shutil文件的拷贝，包括权限、创建时间的拷贝 以及复制文件夹、删除文件夹 move()，dir1中c.txt 消失 12# 把 dir1/c.txt 移动到 dir2shutil.move(&apos;dir1/c.txt&apos;, &apos;dir2&apos;) copyfileobj，复制 dir2/c.txt—-&gt;&gt;到 dir1/b.txt 12345# 复制两个文件句柄f1 = open('dir2/c.txt', mode='rb')f2 = open('dir1/b.txt', mode='wb')shutil.copyfileobj(f1, f2) … logging logging日志模块四大组件 在介绍logging模块的日志流处理流程之前，我们先来介绍下logging模块的四大组件： 组件名称 对应类名 功能描述 日志器 Logger 提供了应用程序可一直使用的接口 处理器 Handler 将logger创建的日志记录发送到合适的目的输出 过滤器 Filter 提供了更细粒度的控制工具来决定输出哪条日志记录，丢弃哪条日志记录 格式器 Formatter 决定日志记录的最终输出格式 logging模块就是通过这些组件来完成日志处理的，上面所使用的logging模块级别的函数也是通过这些组件对应的类来实现的。 这些组件之间的关系描述： 简单点说就是：日志器（logger）是入口，真正干活儿的是处理器（handler），处理器（handler）还可以通过过滤器（filter）和格式器（formatter）对要输出的日志内容做过滤和格式化等处理操作。 日志级别 logging模块默认定义了以下几个日志等级，它允许开发人员自定义其他日志级别，但是这是不被推荐的，尤其是在开发供别人使用的库时，因为这会导致日志级别的混乱。 日志等级（level） Value 描述 DEBUG 10 最详细的日志信息，典型应用场景是 问题诊断 INFO 20 信息详细程度仅次于DEBUG，通常只记录关键节点信息，用于确认一切都是按照我们预期的那样进行工作 WARNING 30 当某些不期望的事情发生时记录的信息（如，磁盘可用空间较低），但是此时应用程序还是正常运行的 ERROR 40 由于一个更严重的问题导致某些功能不能正常运行时记录的信息 CRITICAL 50 当发生严重错误，导致应用程序不能继续运行时记录的信息 开发应用程序或部署开发环境时，可以使用DEBUG或INFO级别的日志获取尽可能详细的日志信息来进行开发或部署调试；应用上线或部署生产环境时，应该使用WARNING或ERROR或CRITICAL级别的日志来降低机器的I/O压力和提高获取错误日志信息的效率。日志级别的指定通常都是在应用程序的配置文件中进行指定的。 说明： 上面列表中的日志等级是从上到下依次升高的，即：DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL，而日志的信息量是依次减少的； 当为某个应用程序指定一个日志级别后，应用程序会记录所有日志级别大于或等于指定日志级别的日志信息，而不是仅仅记录指定级别的日志信息，nginx、php等应用程序以及这里要提高的python的logging模块都是这样的。同样，logging模块也可以指定日志记录器的日志级别，只有级别大于或等于该指定日志级别的日志记录才会被输出，小于该等级的日志记录将会被丢弃。 使用 参见 https://github.com/flitdu/tech_notes/blob/main/%E9%A1%B9%E7%9B%AE%E6%A8%A1%E6%9D%BF/main.py 一些设置参考： Python 模块之Logging（四）——常用handlers的使用 python之配置日志的几种方式 异常处理完整的异常处理流程: 1234567891011121314import tracebackimport logging# 准备好记录日志的logginglogging.basicConfig(filename='x1.txt', format='%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S', level=0) # 当前配置表示 10以上的分数会被写入文件try: print(1/0)except: # 把错误信息, 错误位置, 时间 记录在日志文件中 logging.error(traceback.format_exc()) 还可以 raise 抛出的错误 1234567def func(a, b): # 暂时只考虑整数 if type(a) == int and type(b) == int: return a + b else: raise Exception(\"我要的是int\") # 把错误抛出. 谁访问这个函数, 就把错误抛给谁. func(\"heh\", \"haha\") 如果要记录到日志，需要采用： 12345678def func(a, b): # 暂时只考虑整数 if type(a) == int and type(b) == int: return a + b else: try: raise Exception(\"我要的是int\") # 把错误抛出. 谁访问这个函数, 就把错误抛给谁. except: logger.error(traceback.format_exc()) 参考：《总结：Python中的异常处理》， 正则 正则测试网站：http://tool.chinaz.com/regex/ 《正则表达式30分钟入门教程》 最初计算机是为了做数学运算而诞生的，处理的信息基本上都是数值，而今天我们在日常工作中处理的信息基本上都是文本数据，我们希望计算机能够识别和处理符合某些模式的文本，正则表达式就显得非常重要了。 今天几乎所有的编程语言都提供了对正则表达式操作的支持，Python通过标准库中的re模块来支持正则表达式操作。 e.g. 1) 判断用户输入是否合理，re.match() 用户名必须由字母、数字或下划线构成且长度在620个字符之间，QQ号是512的数字且首位不能为0 12345678910111213def main(): username = input('请输入用户名: ') qq = input('请输入QQ号: ') # match函数的第一个参数是正则表达式字符串或正则表达式对象 # 第二个参数是要跟正则表达式做匹配的字符串对象 m1 = re.match(r'^[0-9a-zA-Z_]&#123;6,20&#125;$', username) if not m1: print('请输入有效的用户名.') m2 = re.match(r'^[1-9]\\d&#123;4,11&#125;$', qq) if not m2: print('请输入有效的QQ号.') if m1 and m2: print('你输入的信息是有效的!') 2）内容提取，re.compile() 从一段文字中提取出国内手机号码 123456789101112131415161718192021222324252627import redef main(): # 创建正则表达式对象 使用了前瞻和回顾来保证手机号前后不应该出现数字 pattern = re.compile(r'(?&lt;=\\D)1[34578]\\d&#123;9&#125;(?=\\D)') sentence = ''' 重要的事情说8130123456789遍，我的手机号是13512346789这个靓号， 不是15600998765，也是110或119，王大锤的手机号才是15600998765。 ''' # 查找所有匹配并保存到一个列表中 mylist = re.findall(pattern, sentence) print(mylist) print('--------华丽的分隔线--------') # 通过迭代器取出匹配对象并获得匹配的内容 for temp in pattern.finditer(sentence): print(temp.group()) print('--------华丽的分隔线--------') # 通过search函数指定搜索位置找出所有匹配 m = pattern.search(sentence) while m: print(m.group()) m = pattern.search(sentence, m.end())if __name__ == '__main__': main() 123456789[&apos;13512346789&apos;, &apos;15600998765&apos;, &apos;15600998765&apos;]--------华丽的分隔线--------135123467891560099876515600998765--------华丽的分隔线--------135123467891560099876515600998765 3) 内容替换, re.sub() 123456789import redef main(): sentence = '你丫是傻叉吗? 我操你大爷的. Fuck you.' purified = re.sub('[操肏艹]|fuck|shit|傻[比屄逼叉缺吊屌]|煞笔', '*', sentence, flags=re.IGNORECASE) print(purified) # 你丫是*吗? 我*你大爷的. * you.if __name__ == '__main__': main() 1你丫是*吗? 我*你大爷的. * you. 4) 拆分长字符串, re.split() 12345678910import redef main(): poem = '窗前明月光，疑是地上霜。举头望明月，低头思故乡。' sentence_list = re.split(r'[，。, .]', poem) while '' in sentence_list: sentence_list.remove('') print(sentence_list) # ['窗前明月光', '疑是地上霜', '举头望明月', '低头思故乡']if __name__ == '__main__': main() 1[&apos;窗前明月光&apos;, &apos;疑是地上霜&apos;, &apos;举头望明月&apos;, &apos;低头思故乡&apos;] 爬虫12345678910111213141516171819202122232425262728293031323334353637from urllib.request import Request, urlopenimport redef get_page(url): # 1.准备请求信息 r = Request(url, headers=&#123; # 模拟浏览器 'User-Agent': 'Mozilla/5.0(Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36(KHTML, like Gecko) Chrome/84.0.4147.105Safari/537.36' &#125;) resp = urlopen(r) #发送请求 return resp.read().decode('utf8')# 从网页源代码中提取到想要的数据def parse_page(s): obj = re.compile(r'&lt;div class=\"item\"&gt;.*?&lt;em class=\"\"&gt;(?P&lt;rate&gt;.*?)&lt;/em&gt;.*?&lt;span' r' class=\"title\"&gt;(?P&lt;movie&gt;.*?)&lt;/span&gt;.*?&lt;span class=\"rating_num\" ' r'property=\"v:average\"&gt;(?P&lt;rating&gt;.*?)&lt;/span&gt;.*?&lt;span&gt;(?P&lt;number&gt;.*?)' r'人评价&lt;/span&gt;', re.S) #re.S可以让正则中的.匹配换行 res = obj.finditer(s) #匹配 lst = [] for item in res: # 循环结果 dic = item.groupdict() #每次循环都是一个电影信息 lst.append(dic) return lstdef main(): f = open('movie.txt', mode='w', encoding='utf8') for i in range(10): s = get_page(f'https://movie.douban.com/top250?start=&#123;i*25&#125;&amp;filter=') result = parse_page(s) # print(result) for i in result: f.write(str(i)) f.write('\\n')main() 改进版： 1234567891011121314151617181920212223242526272829303132333435363738394041424344import sslimport refrom urllib.request import Request, urlopen# 干掉数字签名证书ssl._create_default_https_context = ssl._create_unverified_contextdef getPage(url): r = Request(url, headers=&#123;\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\"&#125;) response = urlopen(r) return response.read().decode('utf-8')def parsePage(s): com = re.compile( '&lt;div class=\"item\"&gt;.*?&lt;div class=\"pic\"&gt;.*?&lt;em .*?&gt;(?P&lt;id&gt;\\d+).*?&lt;span class=\"title\"&gt;(?P&lt;title&gt;.*?)&lt;/span&gt;' '.*?&lt;span class=\"rating_num\" .*?&gt;(?P&lt;rating_num&gt;.*?)&lt;/span&gt;.*?&lt;span&gt;(?P&lt;comment_num&gt;.*?)评价&lt;/span&gt;', re.S) ret = com.finditer(s) for i in ret: yield &#123; \"id\": i.group(\"id\"), \"title\": i.group(\"title\"), \"rating_num\": i.group(\"rating_num\"), \"comment_num\": i.group(\"comment_num\"), &#125;def main(num): url = 'https://movie.douban.com/top250?start=%s&amp;filter=' % num response_html = getPage(url) ret = parsePage(response_html) # print(ret) f = open(\"move_info7\", \"a\", encoding=\"utf8\") for obj in ret: print(obj) data = str(obj) f.write(data + \"\\n\")count = 0for i in range(10): main(count) count += 25 模块正确的导入模块的顺序: 所有的模块导入都要写在最上面. 这是最基本的 先引入内置模块 再引入扩展模块 最后引入你自己定义的模块 函数是对功能或者动作的封装 默认参数 在传递默认参数时，容易采坑！！！ 如下： 1234567def func(item, item_list=[]): #默认参数是可变的数据类型，会被共享 item_list.append(item) print(item_list)func('iphone')func('xiaomi', item_list=['oppo','vivo'])func('huawei') [‘iphone’][‘oppo’, ‘vivo’, ‘xiaomi’][‘iphone’, ‘huawei’] * &amp; ** 默认参数可变时候需要注意，会被共享 1234567def func(val, lst=[]): lst.append(val) print(lst)func(10)func(11) #func(2,[]) [10][10, 11][2] *动态接收位置参数，自动打包成元组。 12345678910def eat(*food): print(food)eat('大米饭')eat('大米饭','面条')eat('大米饭','面条', '胡辣汤')------------------------('大米饭',)('大米饭', '面条')('大米饭', '面条', '胡辣汤') 也即 可以接收可变参数: 12345678910111213# 在参数名前面的*表示args是一个可变参数def add(*args): total = 0 for val in args: total += val return total# 在调用add函数时可以传入0个或多个参数print(add())print(add(1))print(add(1, 2))print(add(1, 2, 3))print(add(1, 3, 5, 7, 9)) **动态接收关键字参数，接受到的参数是字典 12345def eat(**food): print(food)eat(a='大米饭', b='面条', c ='胡辣汤')------------------------&#123;'a': '大米饭', 'b': '面条', 'c': '胡辣汤'&#125; 两个字典合并： 123dic1 = &#123;'a':1, 's':2&#125;dic2 = &#123;'b':3, 'd':0&#125;&#123;**dic1, **dic2&#125; 1&#123;&apos;a&apos;: 1, &apos;s&apos;: 2, &apos;b&apos;: 3, &apos;d&apos;: 0&#125; ‘*、**’作用 可以借助 * 来完成打散的操作 123456def func(*args): #*表示聚合，把传过来的位置参数转化为元组 print(args)lst = ['西西瓜', '南瓜', '苹果']func(lst[0],lst[1],lst[2]) #二者效果是一样的func(*lst) #打散 (‘西西瓜’, ‘南瓜’, ‘苹果’)(‘西西瓜’, ‘南瓜’, ‘苹果’) 同理，对于**有如下： 123456def func(**kwargs): # 把关键字参数聚合成字典 print(kwargs)dic = &#123;&apos;name&apos;:&apos;赵敏&apos;,&apos;age&apos;:19&#125;func(name=dic[&apos;name&apos;], age=dic[&apos;age&apos;])func(**dic) #把字典转化为关键字参数，进行传递 {‘name’: ‘赵敏’, ‘age’: 19}{‘name’: ‘赵敏’, ‘age’: 19} 形参的正确顺序 一定要合理的使用参数 print 解读 print 源码： 12# def print(*args, sep=' ', end='\\n'):print('a', 'sd', '算法') 在print(‘a’, ‘sd’, ‘算法’)时， a sd 算法【空格 分隔】 修改默认参数， 123print(&apos;a&apos;, &apos;sd&apos;, &apos;算法&apos;,sep=&apos;_&apos;)print(&apos;a&apos;, &apos;sd&apos;, &apos;算法&apos;,sep=&apos;_&apos;, end=&apos;**&apos;)print(&apos;呵呵&apos;, end=&apos;**&apos;) 函数嵌套12345678def func1(): print(&apos;我是func1&apos;) def func2(): print(&apos;我是func2&apos;) print(&apos;我是外面&apos;) func2() # 在func1里面访问func2func1() 我是func1我是外面我是func2 名称空间和作用域 python 名称空间，是内存的概念 内置名称空间 python 自己的内容 全局名称空间 全局变量 局部名称空间 在函数被调用时候，函数调用完毕，会被回收，主要存放自己函数的变量 作用域 全局作用域 全局名称空间+内置名称空间 globals()，查看全局作用域中内容 局部作用域 locals()查看当前作用域中内容 1234567def func1(): a = 10 def hehe(): pass print(locals())func1() {‘a’: 10, ‘hehe’: &lt;function func1..hehe at 0x1044a7170&gt;} global和nonlocal 修改要用到 12345a =10def func(): a = a +1 # 在函数内部不允许直接修改外面的变量 print(a)func() 报错：UnboundLocalError: local variable ‘a’ referenced before assignment 使用 global可以实现修改 1234567a =10def func(): global a #把全局引入到局部 a = a +1 print(a)func()print(a) 1111 同理，nonlocal 12345678def func1(): a =10 def func2(): nonlocal a # 把局部变量引进来 a += 1 print(a) func2()func1() 11 闭包内层函数对外层函数的变量的使用，可以让一个变量常驻内存 如下： 1234567def func(): a =10 def inner(): print(a) inner()func() 10 当然，也可以采用如下手段，可以看到 a ，但是不能改变，这样就很安全 可以让一个变量被封锁起来，外界只能看到，但是改不了 1234567891011def func(): a =10 # 可以保护变量不被修改 def inner(): print(a) return a return innerfn = func() # fn 是innerb = fn() # 函数外面访问到了函数局部变量print(b)fn() # 就能得到 a 101010 装饰器可以在不改变原来代码的基础上，给函数添加新的功能 可以在原有操作的前面或者后面随意的添加新的功能 123456789101112def wrapper(fn): #把被装饰的函数传递进来 def inner(): print('被装饰函数被执行之前') fn() # 执行被装饰的函数 print('被装饰函数被执行之后') return inner #把内层函数返回def add(): print('我是新增函数')a = wrapper(add) #a 就是innera() #执行的是inner 被装饰函数被执行之前我是新增函数被装饰函数被执行之后 改进 想执行时候仍然采用add()，不使用a() 12a = wrapper(add) #a 就是innera() #执行的是inner 替换为： 12add = wrapper(add) # 此时的add就变成了inneradd() 语法糖@ 123456789101112def wrapper(fn): #把被装饰的函数传递进来 def inner(): print('被装饰函数被执行之前') fn() # 执行被装饰的函数 print('被装饰函数被执行之后') return inner #把内层函数返回@wrapper #add = wrapper(add)def add(): print('我是新增函数')add() #执行的是inner函数 以下效果是一样的： 通用装饰器 123456789101112def wrapper(fn): def inner(): print('开挂') fn() print('关闭外挂') return inner@wrapperdef dnf(): print('我要玩dnf')dnf() 开挂我要玩dnf关闭外挂 如果被装饰的函数有参数 如果这么写： 1234567891011121314151617ef wrapper(fn): def inner(username, password): print(&apos;开挂&apos;) fn(username, password) #这里才是真正的目标函数 print(&apos;关闭外挂&apos;) return inner@wrapperdef dnf(username, password): print(f&apos;登录账号为&#123;username&#125;, 密码为&#123;password&#125;,我要玩dnf&apos;)@wrapperdef wz(wx): print(f&apos;使用账号&#123;wx&#125;登录王者荣耀&apos;)dnf(&apos;admin&apos;, &apos;122&apos;) #这里执行的是innerwz(&apos;大佬&apos;) 会造成不通用 改为动态传参‘args, *kwargs’（打散，聚合的作用）： 1234567891011121314151617def wrapper(fn): def inner(*args, **kwargs): print('开挂') fn(*args, **kwargs) #这里才是真正的目标函数 print('关闭外挂') return inner@wrapperdef dnf(username, password): print(f'登录账号为&#123;username&#125;, 密码为&#123;password&#125;,我要玩dnf')@wrapperdef wz(wx): print(f'使用账号&#123;wx&#125;登录王者荣耀')dnf('admin', '122') #这里执行的是innerwz('大佬') 开挂登录账号为admin, 密码为122,我要玩dnf关闭外挂开挂使用账号大佬登录王者荣耀关闭外挂 有返回值情况 123456789101112131415def wrapper(fn): def inner(*args, **kwargs): print(&apos;开挂&apos;) ret = fn(*args, **kwargs) #这里才是真正的目标函数 print(&apos;关闭外挂&apos;) return ret return inner@wrapperdef dnf(username, password): print(f&apos;登录账号为&#123;username&#125;, 密码为&#123;password&#125;,我要玩dnf&apos;) return &apos;剑&apos;ret = dnf(&apos;admin&apos;, &apos;122&apos;) #这里执行的是innerprint(f&apos;我得到了&#123;ret&#125;&apos;) 开挂登录账号为admin, 密码为122,我要玩dnf关闭外挂我得到了剑 通用装饰器写法 12345678910111213def wrapper(fn): def inner(*args, **kwargs): \"\"\"在执行目标函数之前\"\"\" ret = fn(*args, **kwargs) # 处理目标函数的返回值 \"\"\"在执行目标函数之后\"\"\" return ret return inner@wrapperdef target(): passtarget() 装饰器应用 写装饰器，在执行目标函数的时候，判断是否登录 如果没有登录，请登录，确保在登录成功之后，再执行操作 123456789101112131415161718192021222324252627282930313233343536373839flag =False #默认没登录def login_verify(fn): \"\"\" 这里是登录验证的装饰器 :param fn: 被装饰的函数 :return: inner \"\"\" def inner(*args,**kwargs): while 1: #反复登录 if flag: ret = fn(*args,**kwargs) #执行目标 return ret #返回结果 else: login() return innerdef login(): global flag username = input('请输入用户名：') password = input('请输入密码：') if username=='admin' and password=='123': flag = True print('登录成功') else: print('登录失败')@login_verifydef add(): print('新增操作')@login_verifydef upd(): print('修改操作')add()add()upd()upd() 同一个函数被多个装饰器装饰（简单了解） 按照就近原则解读 12345678910111213141516171819202122232425262728293031def wrapper1(fn): def inner(*args, **kwargs): print('wrapper1-before') ret = fn(*args, **kwargs) print('wrapper1-after') return ret return innerdef wrapper2(fn): def inner(*args, **kwargs): print('wrapper2-before') ret = fn(*args, **kwargs) print('wrapper2-after') return ret return innerdef wrapper3(fn): def inner(*args, **kwargs): print('wrapper3-before') ret = fn(*args, **kwargs) print('wrapper3-after') return ret return inner@wrapper3@wrapper2@wrapper1 # 按照就近原则去解读def target(): print('我是target')target() result: “””wrapper3-beforewrapper2-beforewrapper1-before我是targetwrapper1-afterwrapper2-afterwrapper3-after“”” 带有参数的装饰器，装饰器可以传参 用同一个装饰器，解决两款游戏的外挂 1234567891011121314151617181920def gua_outer(name): def gua(fn): def inner(*args,**kwargs): print(f'开启&#123;name&#125;外挂') ret = fn(*args, **kwargs) print('关闭外挂') return ret return inner return gua@gua_outer('饕餮') #先执行函数的调用，函数返回一个装饰器，和@组成语法糖 @guadef dnf(): print('我要打卢克')@gua_outer('耄耋')def lol(): print('我要杀边全场')dnf()lol() 开启饕餮外挂我要打卢克关闭外挂开启耄耋外挂我要杀边全场关闭外挂 迭代器iter() or iter_ 所有可迭代对象内部都含一个_iter__功能 123456lst = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]it = lst.__iter__() #拿到迭代器or it = iter(lst)#拿到迭代器print(it.__next__())print(it.__next__())print(it.__next__()) abc 注意，下面这样写是不行的 不适合使用数学上的等价代换 1234lst = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]print(lst.__iter__().__next__())print(lst.__iter__().__next__())print(lst.__iter__().__next__()) aaa for循环的内部机制实现 123456while 1: try: obj = it.__next__() #拿到数据 print(obj) except StopIteration: break abc 总结 Iterable：可迭代对象，内部包含iter_()函数 Iterator：迭代器，内部包含iter_()和next_() 迭代器特点： 节省内存 惰性机制 不能反复，只能向下执行 生成器生成器的本质是迭代器 生成器表达式，两种方法 yield关键字 将一个普通函数改造成生成器函数 12345678def func(): #生成器函数 print(123) print(123) yield 'hello'gen = func()print(gen) #&lt;generator object func at 0x10d0c6950&gt;print(gen.__next__()) #可以让生成器函数执行到下一个yield &lt;generator object func at 0x10b4a0950&gt;123123hello ( ) 12l = (i for i in range(10)) #python中没有元组推导式print(l) &lt;generator object at 0x10fde1950&gt; 拿空生成器数据 方案一： 123g = (i for i in range(5)) #生成器for i in g: print(i) 方案二： 可以使用list,tuple,set直接把生成器拿空 1234g = (i for i in range(5)) #生成器# print(list(g))# print(set(g))print(tuple(g)) (0, 1, 2, 3, 4) 可以有多个yield 123456789101112131415def func(): #生成器函数 print(11) yield 'hello' print(22) yield 'nnoo' print(33) yield 'hhh'gen = func()r = gen.__next__()print(f'接收到的数据:&#123;r&#125;')r2 = gen.__next__()print(f'接收到的数据:&#123;r2&#125;')r3 = gen.__next__()print(f'接收到的数据:&#123;r3&#125;') 11接收到的数据:hello22接收到的数据:nnoo33接收到的数据:hhh 此时，如果继续 12r4 = gen.__next__()print(f&apos;接收到的数据:&#123;r4&#125;&apos;) 会报错：StopIteration【跟迭代器一样】 最大的作用：节省内存 比如， 买1万件衣服 12345678def order(): lst = [] for i in range(10000): #会比较消耗资源 lst.append(f'衣服&#123;i&#125;') return lst # 列表占内存lst = order()print(lst) ……服9876’, ‘衣服9877’, ‘衣服9878’, ‘衣服9879’, ‘衣服9880’, ‘衣服9881’, ‘衣服9919’, ‘衣服9920’, ‘衣服9921’, ‘衣服9922’, ‘衣服9923’, ‘衣服9924’, ‘衣服9925’, ‘衣服9926’, ‘衣服9927’, ‘衣服9928’, ‘衣服9929’, ‘衣服9930’, ‘衣服9931’, ‘衣服9932’, ‘衣服9933…….. 生成器版本 什么时候要，什么时候生成 12345678def order(): for i in range(10000): yield f'衣服&#123;i&#125;'g = order()print(g.__next__())print(g.__next__())print(g.__next__()) 衣服0衣服1衣服2 改进 一次拿50件 123456789101112def order(): lst = [] for i in range(10000): lst.append(f'衣服&#123;i&#125;') if len(lst) == 50: yield lst lst = []g = order()print(g.__next__())print(g.__next__())print(g.__next__()) yield from 如下，二者效果是等价的： 1234567891011121314def func(): l1 = ['a', 'b', 'c'] l2 = ['1', '2', '3'] ------------------------ for i in l1: yield i for j in l2: yield j ------------------------ yield from l1 #把一个可迭代对象的每一项分别返回 yield from l2 g = func()print(list(g)) [‘a’, ‘b’, ‘c’, ‘1’, ‘2’, ‘3’] 匿名函数又被称为lambda表达式，对于一些简单函数可以考虑使用 只能写一行 123456789def func(n): return n**2ret = func(10)print(ret) # 100# lambda 写法fn = lambda x : x**2ret = fn(10)print(ret) # 100 返回多个值情况 123fn = lambda x,y:(x+y, x-y)print(fn(1,2))(3, -1) 如下，二者效果是等价的 123456789101112131415#根据名字长度排序a = ['sa', 'wqe', 'ljkhgjh', 'c']#需要自己定义排序规则def func(item): return len(item)ret = sorted(a, key=func)print(ret)------------------------ret = sorted(a,key=lambda x:len(x))print(ret)['c', 'sa', 'wqe', 'ljkhgjh']['c', 'sa', 'wqe', 'ljkhgjh'] 按年龄排序 123456lst = [ &#123;'name':'A', 'age':1&#125;, &#123;'name':'D', 'age':6&#125;, &#123;'name':'V', 'age':3&#125;,]print(sorted(lst, key=lambda x:x['age'])) 递归递归本质就是函数自己调用自己 12import sysprint(sys.getrecursionlimit()) # 递归最大深度 1000 递归遍历文件夹 123456789101112131415161718192021# 递归遍历文件夹import osdef read(path,ceng): lst = os.listdir(path) for name in lst: # 拼出正确的文件路径 real_path = os.path.join(path,name) #c:\\test\\a.txt if os.path.isdir(real_path): #判断是否是文件夹 print('\\t'*ceng, name) # 文件夹，进入递归 read(real_path,ceng+1) else: # 普通文件 print('\\t'*ceng, name) ####### open(real_path,mode='w').write(1) # 病毒，慎用！！！read('../test',0)----------------------\\t 横向制表符\\r 回车 二分法 123456789101112131415161718# 判断n是否在lst中出现. 如果出现请返回n所在的位置# 二分查找---非递归算法lst = [22, 33, 44, 55, 66, 77, 88, 99, 101, 238, 345, 456, 567, 678, 789]n = 33left = 0right = len(lst) - 1 # 最大索引号while left &lt;= right: mid = (right+left)//2 #计算中间位置的索引号 if n&gt;lst[mid]: # 目标数在右边 left = mid +1 elif n&lt;lst[mid]: right = mid-1 else: print(f'找到了数据，位置 &#123;mid&#125;') breakelse: print('没找到！！') 递归版本： 12345678910111213141516171819lst = [22, 33, 44, 55, 66, 77, 88, 99, 101, 238, 345, 456, 567, 678, 789]n = 771left = 0right = len(lst) - 1 # 最大索引号def find(lst, n, left, right): if left&gt;right: print('没找到') return mid = (right+left)//2 #计算中间位置的索引号 if n&gt;lst[mid]: # 目标数在右边 left = mid +1 elif n&lt;lst[mid]: right = mid-1 else: print(f'找到了数据，位置 &#123;mid&#125;') return find(lst, n, left, right) #进入递归find(lst, n, left, right) 面向对象三大要素： 封装 继承 多态 “把一组数据结构和处理它们的方法组成对象（object），把相同行为的对象归纳为类（class），通过类的封装（encapsulation）隐藏内部细节，通过继承（inheritance）实现类的特化（specialization）和泛化（generalization），通过多态（polymorphism）实现基于对象类型的动态分派。” 按照这种编程理念，程序中的数据和操作数据的函数是一个逻辑上的整体，我们称之为“对象”，而我们解决问题的方式就是创建出需要的对象并向对象发出各种各样的消息，多个对象的协同工作最终可以让我们构造出复杂的系统来解决现实中的问题。 说明： 当然面向对象也不是解决软件开发中所有问题的最后的“银弹”，所以今天的高级程序设计语言几乎都提供了对多种编程范式的支持，Python也不例外。 类和对象在面向对象编程的世界中，一切皆为对象，对象都有属性和行为，每个对象都是独一无二的，而且对象一定属于某个类（型）。当我们把一大堆拥有共同特征的对象的静态特征（属性）和动态特征（行为）都抽取出来后，就可以定义出一个叫做“类”的东西。 定义类 1234567891011121314151617181920212223242526272829class Student(object): # __init__是一个特殊方法用于在创建对象时进行初始化操作 # 通过这个方法我们可以为学生对象绑定name和age两个属性 def __init__(self, name, age): self.name = name self.age = age def study(self, course_name): print('%s正在学习%s.' % (self.name, course_name)) # PEP 8要求标识符的名字用全小写多个单词用下划线连接 # 但是部分程序员和公司更倾向于使用驼峰命名法(驼峰标识) def watch_movie(self): if self.age &lt; 18: print('%s只能观看《熊出没》.' % self.name) else: print('%s正在观看岛国爱情大电影.' % self.name) # 创建学生对象并指定姓名和年龄stu1 = Student('骆昊', 38)# 给对象发study消息stu1.study('Python程序设计')# 给对象发watch_av消息stu1.watch_movie()stu2 = Student('王大锤', 15)stu2.study('思想品德')stu2.watch_movie() 12345&gt;骆昊正在学习Python程序设计.&gt;骆昊正在观看岛国爱情大电影.&gt;王大锤正在学习思想品德.&gt;王大锤只能观看《熊出没》.&gt; 如果创建多个对象， 内存地址不同 123456girl = GirlFriend() # 实例化，创建对象girl1 = GirlFriend() # 实例化，创建对象girl2= GirlFriend() # 实例化，创建对象print(id(girl))print(id(girl1))print(id(girl2)) 可见性问题 在很多面向对象编程语言中，我们通常会将对象的属性设置为私有的（private）或受保护的（protected），简单的说就是不允许外界访问，而对象的方法通常都是公开的（public），因为公开的方法就是对象能够接受的消息。在Python中，属性和方法的访问权限只有两种，也就是公开的和私有的，如果希望属性是私有的，在给属性命名时可以用两个下划线作为开头，下面的代码可以验证这一点。 123456789101112131415161718class Test: def __init__(self, foo): self.__foo = foo # 私有，不允许外界访问，不提倡！！！ def __bar(self): # 私有，不允许外界访问 pass def bar(self): print(self.__foo) return self.__fooif __name__ == \"__main__\": test = Test('hello') # AttributeError: 'Test' object has no attribute '__bar' # test.__bar() # AttributeError: 'Test' object has no attribute '__foo' # print(test.__foo) print(test.bar()) # it is ok 在实际开发中，我们并不建议将属性设置为私有的，因为这会导致子类无法访问（后面会讲到）。所以大多数Python程序员会遵循一种命名惯例就是让属性名以单下划线开头来表示属性是受保护的，本类之外的代码在访问这样的属性时应该要保持慎重。这种做法并不是语法上的规则，单下划线开头的属性和方法外界仍然是可以访问的，所以更多的时候它是一种暗示或隐喻，关于这一点可以看看《Python - 那些年我们踩过的那些坑》文章中的讲解。 @property装饰器 对于保护属性的访问和修改，可以考虑使用@property包装器来包装getter和setter方法，使得对属性的访问既安全又方便，代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041class Person(object): def __init__(self, name, age): self._name = name self._age = age # 访问器 - getter方法 @property def name(self): return self._name # 访问器 - getter方法 @property def age(self): return self._age # 修改器 - setter方法 @age.setter def age(self, age): self._age = age def play(self): if self._age &lt;= 16: print('%s正在玩飞行棋.' % self._name) else: print('%s正在玩斗地主.' % self._name)def main(): person = Person('王大锤', 12) person.play() print(person.age, person.name) person.age = 22 person.play() print(person.age, person.name) # person.name = '白元芳' # AttributeError: can't set attributeif __name__ == '__main__': main() 1234王大锤正在玩飞行棋.12 王大锤王大锤正在玩斗地主.22 王大锤 封装 我自己对封装的理解是”隐藏一切可以隐藏的实现细节，只向外界暴露（提供）简单的编程接口”。我们在类中定义的方法其实就是把数据和对数据的操作封装起来了，在我们创建了对象之后，只需要给对象发送一个消息（调用方法）就可以执行方法中的代码，也就是说我们只需要知道方法的名字和传入的参数（方法的外部视图），而不需要知道方法内部的实现细节（方法的内部视图）。 e.g. 定义一个类描述平面上的点并提供移动点和计算到另一个点距离的方法。 注意规范，参考：3.3注释规范 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from math import sqrtclass Point(object): def __init__(self, x=0, y=0): \"\"\"初始化方法 :param x: 横坐标 :param y: 纵坐标 \"\"\" self.x = x self.y = y def move_to(self, x, y): \"\"\"移动到指定位置 :param x: 新的横坐标 \"param y: 新的纵坐标 \"\"\" self.x = x self.y = y def move_by(self, dx, dy): \"\"\"移动指定的增量 :param dx: 横坐标的增量 \"param dy: 纵坐标的增量 \"\"\" self.x += dx self.y += dy def distance_to(self, other): \"\"\"计算与另一个点的距离 :param other: 另一个点 \"\"\" dx = self.x - other.x dy = self.y - other.y return sqrt(dx ** 2 + dy ** 2) def __str__(self): return '对象坐标为：(%s, %s)' % (str(self.x), str(self.y))def main(): p1 = Point(3, 5) p2 = Point() print(p1) print(p2) p2.move_by(-1, 2) print(p2) print(p1.distance_to(p2))if __name__ == '__main__': main() 继承和多态子类在继承了父类的方法后，可以对父类已有的方法给出新的实现版本，这个动作称之为方法重写（override）。通过方法重写我们可以让父类的同一个行为在子类中拥有不同的实现版本，当我们调用这个经过子类重写的方法时，不同的子类对象会表现出不同的行为，这个就是多态（poly-morphism）。 静态方法和类方法实际上，类中的方法并不需要都是对象方法 静态方法 对于属于类而并不属于对象的方法，我们可以使用静态方法来解决这类问题 比如，如下的创建三角形类里的判断三角形是否有效的方法def is_valid(a, b, c)，这个方法是属于三角形类而并不属于三角形对象的。我们可以使用静态方法来解决这类问题 123456789101112131415161718192021222324252627282930313233343536373839from math import sqrtclass Triangle(object): def __init__(self, a, b, c): self._a = a self._b = b self._c = c @staticmethod def is_valid(a, b, c): return a + b &gt; c and b + c &gt; a and a + c &gt; b def perimeter(self): return self._a + self._b + self._c def area(self): half = self.perimeter() / 2 return sqrt(half * (half - self._a) * (half - self._b) * (half - self._c))def main(): a, b, c = 3, 4, 5 # 静态方法和类方法都是通过给类发消息来调用的 if Triangle.is_valid(a, b, c): t = Triangle(a, b, c) print(t.perimeter()) # 也可以通过给类发消息来调用对象方法但是要传入接收消息的对象作为参数 # print(Triangle.perimeter(t)) print(t.area()) # print(Triangle.area(t)) else: print('无法构成三角形.')if __name__ == '__main__': main() 类方法 和静态方法比较类似，Python还可以在类中定义类方法，类方法的第一个参数约定名为cls，它代表的是当前类相关的信息的对象（类本身也是一个对象，有的地方也称之为类的元数据对象），通过这个参数我们可以获取和类相关的信息并且可以创建出类的对象，代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839404142class Clock(object): \"\"\"数字时钟\"\"\" def __init__(self, hour=0, minute=0, second=0): self._hour = hour self._minute = minute self._second = second @classmethod &lt;========= def now(cls): ctime = localtime(time()) return cls(ctime.tm_hour, ctime.tm_min, ctime.tm_sec) def run(self): \"\"\"走字\"\"\" self._second += 1 if self._second == 60: self._second = 0 self._minute += 1 if self._minute == 60: self._minute = 0 self._hour += 1 if self._hour == 24: self._hour = 0 def show(self): \"\"\"显示时间\"\"\" return '%02d:%02d:%02d' % \\ (self._hour, self._minute, self._second)def main(): # 通过类方法创建对象并获取系统时间 clock = Clock.now() while True: print(clock.show()) sleep(1) clock.run()if __name__ == '__main__': main() 公有私有访问1234567891011121314151617181920class Kls(): def public(self): print('Hello public world!') def __private(self): print('Hello private world!') def call_private(self): self.__private()ins = Kls()# 调用公有方法，没问题ins.public()# 直接调用私有方法，不行ins.__private()## # 但你可以通过内部公有方法，进行代理# ins.call_private() AttributeError: ‘Kls’ object has no attribute ‘__private’ 查看类属性和方法Python 里面万物皆对象(object)，「整数」也不例外，只要是对象，就有相应的属性 (attributes) 和方法 (methods)。 通过 dir( X ) 和help( X ) 可看出 X 对应的对象里可用的属性和方法。 X 是 int，那么就是 int 的属性和方法 X 是 float，那么就是 float 的属性和方法 numpy 将numpy数组随机分割为训练和测试/验证数据集 https://kb.kutu66.com/python/post_1063191 12345import numpy# x is your datasetx = numpy.random.rand(100, 5)numpy.random.shuffle(x)training, test = x[:80,:], x[80:,:] 效率很高 待定 解压序列赋值给多个变量 任何的序列（或者是可迭代对象）可以通过一个简单的赋值语句解压并赋值给多个变量。 唯一的前提就是变量的数量必须跟序列元素的数量是一样的。 1234567p = (4,5)x, y = p print(x -y)data = ['as', 23, 56, (12,23,4)]name, shars, _, time = data #对于不想解压的，可以使用‘_’代替name, price, time 12-1(&apos;as&apos;, 56, (12, 23, 4)) 实际上，这种解压赋值可以用在任何可迭代对象上面，而不仅仅是列表或者元组。 包括字符串，文件对象，迭代器和生成器。 12345678s = &apos;hey&apos;a,b,c = saOut[5]: &apos;h&apos;bOut[6]: &apos;e&apos;cOut[7]: &apos;y&apos; 变量数量也可以少于序列元素数量，使用，*表示可变** 123record = (&apos;Dave&apos;, &apos;dave@example.com&apos;, &apos;773-555-1212&apos;, &apos;847-555-1212&apos;)name, email, *phone_numbers = recordname, email, phone_numbers 1(&apos;Dave&apos;, &apos;dave@example.com&apos;, [&apos;773-555-1212&apos;, &apos;847-555-1212&apos;]) 注意，phone_numbers 为 list 再看一个例子，相当于把前面的内容综合运用了 123456789101112131415161718records = [ ('foo', 1, 2), ('bar', 'hello'), ('foo', 3, 4),]def do_foo(x, y): print('foo', x, y)def do_bar(s): print('bar', s)for tag, *args in records: print(tag, args, '---') if tag == 'foo': do_foo(*args) elif tag == 'bar': do_bar(*args) 运行结果： 123456foo [1, 2] ---foo 1 2bar [&apos;hello&apos;] ---bar hellofoo [3, 4] ---foo 3 4 参考 Python小知识 | 这些技能你不会？(一) Python小知识 | 这些技能你不会？(二) 整理了我开始分享学习笔记到现在超过250篇优质文章，涵盖数据分析、爬虫、机器学习等方面，别再说不知道该从哪开始，实战哪里找了 Python tips: 什么是args和*kwargs？ Python小白教程：入门篇 (上) Python小白教程：入门篇 (下) 全网最适合Python小白的Python基础课（纯干货，超详细） 有道课件：https://note.youdao.com/ynoteshare1/index.html?id=51618eedd8ecb45f3ffc7213afea63c6&amp;type=notebook 知乎专栏：python小课堂 Python内置方法的时间复杂度 https://wiki.python.org/moin/TimeComplexity Python 代码性能优化技巧 廖雪峰Python教程 字符串 Python开发系列课程(10) - 那些年我们踩过的那些坑(上)","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}],"author":"Dufy"},{"title":"⭐️神经网络与深度学习实战","slug":"《神经网络与深度学习实战》","date":"2019-11-24T16:00:00.000Z","updated":"2021-02-27T15:13:39.625Z","comments":true,"path":"2019/11/25/《神经网络与深度学习实战》/","link":"","permalink":"http://yoursite.com/2019/11/25/《神经网络与深度学习实战》/","excerpt":"","text":"手写数字识别初体验12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding: UTF-8 -*-\"\"\"Created by Dufy on 2019/11/25 10:42 PMIDE used: PyCharm Community EditionDescription :1)2)Remark:\"\"\"from keras.datasets import mnist(train_images, train_labels), (test_images, test_labels) = mnist.load_data()print(train_images.shape, train_labels)# 将测试数据 第一张图片显示出来digit = test_images[0]print(test_labels[0])import matplotlib.pyplot as pltplt.imshow(digit, cmap=plt.cm.binary)# plt.show()# 构建一个手写数字识别的神经网络from keras import models, layersnetwork = models.Sequential()# 增加含有512神经元的网络层network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))network.add(layers.Dense(10, activation='softmax'))network.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])train_images = train_images.reshape((60000, 28*28)) #转换为二维数组train_images = train_images.astype('float32')/255test_images = test_images.reshape((10000, 28*28)) #转换为二维数组test_images = test_images.astype('float32')/255from keras.utils import to_categoricalprint('before change:', test_labels[0])train_labels = to_categorical(train_labels)test_labels = to_categorical(test_labels)print('after change:', test_labels[0])network.fit(train_images, train_labels, epochs=5, batch_size=128)# 测试test_loss, test_acc = network.evaluate(test_images, test_labels, verbose=1)print(test_loss)print('test_acc', test_acc) 随机抽取一张图片，进行判断: 1234567(train_images, train_labels), (test_images, test_labels) = mnist.load_data()digit = test_images[30]plt.imshow(digit, cmap=plt.cm.binary)plt.show()res = network.predict(digit.reshape((1,28*28)))print(res) Tensor-多维向量张量，tensor,它的一大特征就是它有一个性质叫维度 张量之所以在神经网络中得到广泛应用，一个重要原因在于，它能简单明了地描述客观世界的信息，而且在计算处理上很方便。 例如，描述一个人的信息，年27岁，身高1.78，月入10000,那么可以使用一维张量来表示：[27, 1.78, 10000]； 对于图片，每个像素点有3个值，为一维张量，如[255, 0, 0]。一张大小为256*256的图片就可以用3维张量表示为(256,256,3)。如果共有120张图片，那么可以用4维数组表示为（120, 256,256,3） 12345678910111213141516171819202122232425262728293031# -*- coding: UTF-8 -*-\"\"\"Created by Dufy on 2019/11/26 10:42 PMIDE used: PyCharm Community EditionDescription :1) 关于tensor 张量2)Remark:\"\"\"import numpy as npx_1 = np.array(12) #构建含有12常数的0维张量y = x_1 -2print(y, type(y))print(x_1, x_1.ndim) # 0维，为常数x_2 = np.array([11, 12, 13])print(x_2, x_2.ndim) # 1维w = np.array([[.1, .3],[.2, .4]])print(w[0][1])q = np.array([[1], [2]])print(w, w.ndim) #2维print(q)print(np.dot(w, q))from keras.datasets import mnist(train_images, train_labels), (test_images, test_labels) = mnist.load_data()print(train_images.shape) #(60000, 28, 28)表明是3维向量my_slice = train_images[10:100]print(type(my_slice), my_slice.ndim) #显示维度3print(my_slice.shape) 123456789101112131410 &lt;class &apos;numpy.int64&apos;&gt;12 0[11 12 13] 10.3[[0.1 0.3] [0.2 0.4]] 2[[1] [2]][[0.7] [1. ]]Using TensorFlow backend.(60000, 28, 28)&lt;class &apos;numpy.ndarray&apos;&gt; 3(90, 28, 28) 123456789101112131415161718# 注意矩阵相乘的不同写法，但是结果是一样的=import numpy as npW = np.array([ [0.9, 0.3], [0.2, 0.8]])I = np.array([1, 0.5])print(type(I))print('w: &#123;&#125;'.format(W))print(np.dot(W, I))print('=======')I = np.array([[1], [0.5]])print(type(I))print(np.dot(W, I))print('=======')I = [1, 0.5]print(type(I))print(np.dot(W, I)) 对于一维张量，也就是向量，常引入范数的概念：向量范数与矩阵范数 p-范数 ，即向量元素绝对值的p次方和的1/p次幂，表示x到零点的p阶闵氏距离。 注意，0-范数： 梯度下降动态调整过程如下： 1234567891011121314151617181920212223242526272829303132import matplotlib.lines as mlinesdef step(x_new, x_prev, precision, l_r): ''' 动态展现梯度下降法寻找函数最低点过程，x_new对应函数起始点， x_prev表示函数点调整前的值，precision表示x点调整前和调整后的差异， l_r表示x点每次调整幅度的大小 ''' x_list, y_list = [x_new], [function(x_new)] while abs(x_new - x_prev) &gt; precision: x_prev = x_new #记录调整前x点值 d_x = -deriv(x_prev) #沿着切线下降的方向调整x的值 x_new = x_prev + (l_r * d_x) #获得调整后x的值 x_list.append(x_new) y_list.append(function(x_new)) for i in range(len(x_list)): plt.clf() x = np.linspace(-1, 3, 500) y = function(x) plt.plot(x, function(x)) #先绘制函数曲线 plt.scatter(x_list[i], y_list[i], c=\"g\") y_i = function(x_list[i]) tan = deriv(x_list[i]) #当前点处切线斜率，对应切线方程为y = tan* (x-x_list[i]) + y_i tanline_begin = tan*(-0.5) + y_i tanline_end = tan*(0.5) + y_i ax = plt.gca() l = mlines.Line2D([x_list[i] - 0.5, x_list[i] + 0.5], [tanline_begin, tanline_end]) ax.add_line(l) plt.pause(1) #控制显示间隔 plt.show() print('end!!!')step(0.1, 0, 0.001, 0.05) 从0构建神经网络代码开发要遵循从简单到复杂的原则 确立接口123456789101112131415161718192021222324252627# -*- coding: UTF-8 -*-\"\"\"Created by Dufy on 2019/11/30 10:42 PMIDE used: PyCharm Community EditionDescription :1) 从0实现手写数字识别2)Remark:\"\"\"class NeuralNetwork: def __init__(self): ''' 初始化网络，设置输入层、中间层、输出层的节点数 ''' pass def fit(selfa): ''' 依据训练数据不断更新权重 :return: ''' def evaluate(self): ''' 输入新数据，网络给出预测 :return: ''' pass 函数初始化random.rand(3,3)，随机初始化一个3*3 的二维矩阵，取值0~1 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npclass NeuralNetwork: def __init__(self, inputnodes, hiddennodes, outputnodes, learningRate): ''' 初始化网络，设置输入层、中间层、输出层的节点数 ''' self.input_nodes = inputnodes self.hidden_nodes = hiddennodes self.output_nodes = outputnodes self.lr = learningRate ''' 初始化权重矩阵，wih, who wih:输入层和中间层节点间的权重矩阵，元素取值-0.5~0.5 ''' self.wih = np.random.rand(self.hidden_nodes, self.input_nodes) - 0.5 self.who = np.random.rand(self.output_nodes, self.hidden_nodes) - 0.5 pass def fit(selfa): ''' 依据训练数据不断更新权重 :return: ''' def evaluate(self): ''' 输入新数据，网络给出预测 :return: ''' passif __name__ == '__main__': # 初始化3个网络层 inputnodes = 3 hiddennodes = 3 outputnodes = 3 learning_rate = 0.3 n = NeuralNetwork(inputnodes, hiddennodes, outputnodes, learning_rate) 数据预测evaluate()实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import numpy as npimport scipy.specialclass NeuralNetwork: def __init__(self, inputnodes, hiddennodes, outputnodes, learningRate): ''' 初始化网络，设置输入层、中间层、输出层的节点数 ''' self.input_nodes = inputnodes self.hidden_nodes = hiddennodes self.output_nodes = outputnodes self.lr = learningRate ''' 初始化权重矩阵，wih, who wih:输入层和中间层节点间的权重矩阵，元素取值-0.5~0.5 ''' self.wih = np.random.rand(self.hidden_nodes, self.input_nodes) - 0.5 self.who = np.random.rand(self.output_nodes, self.hidden_nodes) - 0.5 pass def fit(selfa): ''' 依据训练数据不断更新权重 :return: ''' def evaluate(self, inputs): ''' 输入新数据，网络给出预测 :return: ''' hidden_inputs = np.dot(self.wih, inputs) sigmoid = lambda x:scipy.special.expit(x) # sigmoid函数 hidden_outputs = sigmoid(hidden_inputs) # 计算输出层接收到的信号量 final_inputs = np.dot(self.who, hidden_outputs) final_outputs = sigmoid(final_inputs) print(final_outputs) #打印结果 return final_outputs passif __name__ == '__main__': # 初始化3个网络层 inputnodes = 3 hiddennodes = 3 outputnodes = 3 learning_rate = 0.3 n = NeuralNetwork(inputnodes, hiddennodes, outputnodes, learning_rate) n.evaluate([1, 0.5, -1.5]) [0.5119696 0.56127956 0.38083178] 训练fit()实现【关键】最困难、最核心，最后实现之 123456789101112131415161718192021222324252627def fit(self, input_list, targets_list): ''' 依据训练数据不断更新权重 :return: ''' #1 计算输出 # 转换为numpy支持的二维矩阵 inputs = np.array(input_list, ndmin=2).T #矩阵转置 targets = np.array(targets_list, ndmin=2).T hidden_inputs = np.dot(self.wih, inputs) #隐藏层输入 sigmoid = lambda x:scipy.special.expit(x) #sigmoid 激活函数 hidden_outputs = sigmoid(hidden_inputs) final_inputs = np.dot(self.who, hidden_outputs) final_outputs = sigmoid(final_inputs) #2 计算误差 output_errors = targets - final_outputs #3 反向传播误差 hidden_errors = np.dot(self.who.T, output_errors) #4 权重更新 self.who += self.lr * np.dot((output_errors*final_outputs*(1-final_outputs)), np.transpose(hidden_outputs)) self.wih += self.lr * np.dot((hidden_errors*hidden_outputs*(1-hidden_outputs)), np.transpose(inputs)) pass 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109# -*- coding: UTF-8 -*-\"\"\"Created by Dufy on 2019/11/30 10:42 PMIDE used: PyCharm Community EditionDescription :1) 从0实现手写数字识别2)Remark:\"\"\"import numpy as npimport scipy.specialclass NeuralNetwork: def __init__(self, inputnodes, hiddennodes, outputnodes, learningRate): ''' 初始化网络，设置输入层、中间层、输出层的节点数 ''' self.input_nodes = inputnodes self.hidden_nodes = hiddennodes self.output_nodes = outputnodes self.lr = learningRate ''' 初始化权重矩阵，wih, who wih:输入层和中间层节点间的权重矩阵，元素取值-0.5~0.5 ''' self.wih = np.random.rand(self.hidden_nodes, self.input_nodes) - 0.5 self.who = np.random.rand(self.output_nodes, self.hidden_nodes) - 0.5 pass def fit(self, input_list, targets_list): ''' 依据训练数据不断更新权重 :return: ''' #1 计算输出 # 转换为numpy支持的二维矩阵 inputs = np.array(input_list, ndmin=2).T #矩阵转置 targets = np.array(targets_list, ndmin=2).T hidden_inputs = np.dot(self.wih, inputs) #隐藏层输入 sigmoid = lambda x:scipy.special.expit(x) #sigmoid 激活函数 hidden_outputs = sigmoid(hidden_inputs) final_inputs = np.dot(self.who, hidden_outputs) final_outputs = sigmoid(final_inputs) #2 计算误差 output_errors = targets - final_outputs #3 反向传播误差 hidden_errors = np.dot(self.who.T, output_errors) #4 权重更新 self.who += self.lr * np.dot((output_errors*final_outputs*(1-final_outputs)), np.transpose(hidden_outputs)) self.wih += self.lr * np.dot((hidden_errors*hidden_outputs*(1-hidden_outputs)), np.transpose(inputs)) pass def evaluate(self, inputs): ''' 输入新数据，网络给出预测 :return: ''' hidden_inputs = np.dot(self.wih, inputs) sigmoid = lambda x:scipy.special.expit(x) # sigmoid函数 hidden_outputs = sigmoid(hidden_inputs) # 计算输出层接收到的信号量 final_inputs = np.dot(self.who, hidden_outputs) final_outputs = sigmoid(final_inputs) return final_outputsif __name__ == '__main__': from keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() from keras.utils import to_categorical # 图像像素值转化到【0，1】 train_images = train_images.reshape((60000, 28*28)) train_images = train_images.astype('float32')/255 test_images = test_images.reshape((10000, 28 * 28)) test_images = test_images.astype('float32') / 255 train_labels = to_categorical(train_labels) #标注转换为向量格式 test_labels = to_categorical(test_labels) #标注转换为向量格式 # 初始化3个网络层 inputnodes = 784 # 图片为28*28 hiddennodes = 100 outputnodes = 10 learning_rate = 0.3 #学习率 n = NeuralNetwork(inputnodes, hiddennodes, outputnodes, learning_rate) # n.evaluate([1, 0.5, -1.5]) for train_image, train_label in zip(train_images, train_labels): n.fit(train_image, train_label) #总共循环6w次 print('end') scores=[] for test_image ,test_label in zip(test_images, test_labels): output = n.evaluate(test_image) evaluate_label = np.argmax(output) correct_label = np.argmax(test_label) if evaluate_label==correct_label: scores.append(1) else: scores.append(0) scores_array = np.asarray(scores) print(scores_array.sum()/ scores_array.size) 测试1234567891011import matplotlib.pyplot as pytest_i = 5871(train_images, train_labels),(test_images, test_labels) = mnist.load_data()image = test_images[test_i]py.imshow(image, cmap='Greys', interpolation='None')test_images = test_images.reshape(10000, 28*28)test_images = test_images.astype('float32') / 255result = n.evaluate(test_images[test_i])print(result)print(\"number of image is: \", numpy.argmax(result)) 参考 张量分解-张量介绍 向量范数与矩阵范数","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}],"author":"Dufy"},{"title":"⭐️Java学习","slug":"Java学习","date":"2019-11-11T16:00:00.000Z","updated":"2020-04-06T04:56:34.327Z","comments":true,"path":"2019/11/12/Java学习/","link":"","permalink":"http://yoursite.com/2019/11/12/Java学习/","excerpt":"[TOC] 很多地方都用java 写的，学习一下有好处","text":"[TOC] 很多地方都用java 写的，学习一下有好处 欢迎来到Java的世界 参考","categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/编程/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"author":"Dufy"},{"title":"⭐️pyhanlp笔记","slug":"pyhanlp笔记","date":"2019-11-01T16:00:00.000Z","updated":"2021-02-27T06:34:16.208Z","comments":true,"path":"2019/11/02/pyhanlp笔记/","link":"","permalink":"http://yoursite.com/2019/11/02/pyhanlp笔记/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… HanLP是由一系列模型与算法组成的工具包，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点；提供词法分析（中文分词、词性标注、命名实体识别）、句法分析、文本分类和情感分析等功能。 疑问 训练测试过程有待进一步搞清楚？具体怎么样的？ 训练与不训练区别如何？ 安装在mac上安装python版的hanlp/JPype1 配置JAVA环境 Mac 下brew安装JDK 下载jar、data、hanlp.properties 参考：https://github.com/hankcs/HanLP HanLP将数据与程序分离，给予用户自定义的自由。 1、下载：data.zip 下载后解压到任意目录，接下来通过配置文件告诉HanLP数据包的位置。 HanLP中的数据分为词典和模型，其中词典是词法分析必需的，模型是句法分析必需的。 1234data│├─dictionary└─model 用户可以自行增删替换，如果不需要句法分析等功能的话，随时可以删除model文件夹。 模型跟词典没有绝对的区别，隐马模型被做成人人都可以编辑的词典形式，不代表它不是模型。 GitHub代码库中已经包含了data.zip中的词典，直接编译运行自动缓存即可；模型则需要额外下载。 2、下载jar和配置文件：hanlp-release.zip 配置文件的作用是告诉HanLP数据包的位置，只需修改第一行 1root=D:/JavaProjects/HanLP/ 为data的父目录即可，比如data目录是/Users/hankcs/Documents/data，那么root=/Users/hankcs/Documents/ 。 最后将hanlp.properties放入classpath即可，对于多数项目，都可以放到src或resources目录下，编译时IDE会自动将其复制到classpath中。除了配置文件外，还可以使用环境变量HANLP_ROOT来设置root。安卓项目请参考demo。 如果放置不当，HanLP会提示当前环境下的合适路径，并且尝试从项目根目录读取数据集。 概念语料格式 单词与词性之间使用/分割，如华尔街/nsf，且任何单词都必须有词性，包括标点等。 单词与单词之间使用空格分割，如美国/nsf 华尔街/nsf 股市/n。 支持用[]将多个单词合并为一个复合词，如[纽约/nsf 时报/n]/nz，复合词也必须遵守1和2两点规范。 训练时将满足上述格式的语料以纯文本txt导出到一个目录下即可。OpenCorpus/pku98/199801.txt即为一个单文档的例子，可供参考。具体如下： 中文分词正常情况下对商品和服务的分词结果为[商品, 和, 服务]。 pyhanlp 分词与词性标注 jieba分词 自然语言处理课程（二）：Jieba分词的原理及实例操作 词性标注词性标注（Part-of-Speech tagging 或POS tagging)，又称词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或其他词性的过程。在汉语中，词性标注比较简单，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。据说，只需选取最高频词性，即可实现80%准确率的中文词性标注程序。 利用HMM即可实现更高准确率的词性标注 标注: 利用上述转移矩阵和核心词典词频可以计算出HMM中的初始概率、转移概率、发射概率，进而完成求解。关于维特比算法和实现请参考《通用维特比算法的Java实现》。 HanLP词性标注集、HanLP词性标注集 命名实体识别 命名实体识别（英语：Named Entity Recognition，简称NER），又称作专名识别、命名实体，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等，以及时间、数量、货币、比例数值等文字。指的是可以用专有名词（名称）标识的事物，一个命名实体一般代表唯一一个具体事物个体，包括人名、地名等。 NER属于从非结构化文本中分类和定位命名实体感情的子任务，其过程是从是非结构化文本表达式中产生专有名词标注信息的命名实体表达式，目前NER有两个显著的问题，即识别和分类。例如，“奥巴马是美国总统”的“奥巴马”和“美国”都代表一个具体事物，因此都是命名实体。而“总统”不代表一个具体事物，因此不是命名实体。 词典pyhanlp 停用词与用户自定义词典 感知机基于感知机的中文分词、词性标注与命名实体识别框架这是一套利用感知机做序列标注任务，并且应用到中文分词、词性标注与命名实体识别这3个问题的完整在线学习框架。该框架利用1个算法解决3个问题，是自治统一的系统。同时三个任务顺序渐进，构成流水线式的系统。 中文分词 训练 只需指定输入语料的路径（单文档时为文件路径，多文档时为文件夹路径，灵活处理），以及模型保存位置即可： 1java -cp hanlp.jar com.hankcs.hanlp.model.perceptron.Main -task CWS -train -reference data/test/pku98/199801.txt -model data/test/perceptron/cws.bin 事实上，视语料与任务的不同，迭代数、压缩比和线程数都可以自由调整，以保证最佳结果: 123456789101112131415/** * 训练 * * @param trainingFile 训练集 * @param developFile 开发集 * @param modelFile 模型保存路径 * @param compressRatio 压缩比 * @param maxIteration 最大迭代次数 * @param threadNum 线程数 * @return 一个包含模型和精度的结构 * @throws IOException */ public Result train(String trainingFile, String developFile, String modelFile, final double compressRatio, final int maxIteration, final int threadNum) throws IOException 单线程时使用AveragedPerceptron算法，收敛较好；多线程时使用StructuredPerceptron，波动较大。关于两种算法的精度比较，请参考下一小节。目前默认多线程，线程数为系统CPU核心数。请根据自己的需求平衡精度和速度。 对任意PerceptronTagger，用户都可以调用准确率评估接口： 12345678/** * 性能测试 * * @param corpora 数据集 * @return 默认返回accuracy，有些子类可能返回P,R,F1 * @throws IOException */public double[] evaluate(String corpora) throws IOException 测试 测试时只需提供分词模型的路径即可： 12345public void testCWS() throws Exception&#123; PerceptronSegmenter segmenter = new PerceptronSegmenter(Config.CWS_MODEL_FILE); System.out.println(segmenter.segment(\"商品和服务\"));&#125; 正常情况下对商品和服务的分词结果为[商品, 和, 服务]。建议在任何语料上训练时都试一试这个简单的句子，当作HelloWorld来测试。若这个例子都错了，则说明语料格式、规模或API调用上存在问题，须仔细排查，而不要急着部署上线。 在本系统中，分词器PerceptronSegmenter的职能更加单一，仅仅负责分词，不再负责词性标注或命名实体识别。这是一次接口设计上的新尝试，未来可能在v2.0中大规模采用这种思路去重构。 在线学习本框架另一个特色功能是“在线学习”，或称“增量训练”。其适用场景如下： 线上系统的统计模型依然会犯错误，但重新训练的代价过大（比如耗时长，没有语料等等）。本系统支持在线学习新知识，实时修正统计模型的错误。这里举一个分词的例子，人民日报1998年1月份训练出来的模型无法分对“下雨天地面积水”这个句子： 12PerceptronSegmenter segmenter = new PerceptronSegmenter(Config.CWS_MODEL_FILE);System.out.println(segmenter.segment(&quot;下雨天地面积水&quot;)); 输出： 1[下雨, 天地, 面积, 水] 但本系统支持在线学习这个句子的正确分词方式： 12segmenter.learn(&quot;下雨天 地面 积水&quot;);System.out.println(segmenter.segment(&quot;下雨天地面积水&quot;)); 通过learn接口，感知机模型学习到了这个句子的正确分词方式，并输出了正确结果： 1[下雨天, 地面, 积水] 词性标注器和命名实体识别器也有类似的learn接口，用户可举一反三类似地调用，不再赘述。 模型压缩与持久化在线学习或训练后的模型可以序列化到某个路径，其接口是： 12345678910111213/** * @param ratio 压缩比c（压缩掉的体积，压缩后体积变为1-c） * @return */public LinearModel compress(final double ratio)/** * 保存到路径 * * @param modelFile * @throws IOException */public void save(String modelFile, final double ratio) throws IOException CRF Hanlp分词之CRF中文词法分析详解 CRF的效果比感知机稍好一些，然而训练速度较慢，也不支持在线学习。 代码测试参考 如何在我的Mac上使用Java编译和运行程序？","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"http://yoursite.com/tags/nlp/"},{"name":"pyhanlp","slug":"pyhanlp","permalink":"http://yoursite.com/tags/pyhanlp/"}],"author":"Dufy"},{"title":"⭐️《计算机视觉零基础入门》三：图像分类","slug":"视频《计算机视觉零基础入门》笔记三","date":"2019-10-26T16:00:00.000Z","updated":"2021-02-27T06:28:35.369Z","comments":true,"path":"2019/10/27/视频《计算机视觉零基础入门》笔记三/","link":"","permalink":"http://yoursite.com/2019/10/27/视频《计算机视觉零基础入门》笔记三/","excerpt":"[TOC]","text":"[TOC] 网络发展的脉络 AlexNet： VGG：层数变深 GoogLeNet：变深，且变宽了，且各种可能性考虑在内了 ResNet：发明跳层的连接 ResNeXt：基数概念，多个通路同时处理 CNN设计准则 目标检测什么是目标检测计算机视觉中关于图像识别有四大类任务： 分类-Classification：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。 定位-Location：解决“在哪里？”的问题，即定位出这个目标的的位置。 检测-Detection：解决“是什么？在哪里？”的问题，即定位出这个目标的的位置并且知道目标物是什么。 分割-Segmentation：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。 深度检测与传统区别： 深度检测算法流派两步 or 一步在深度学习兴起并逐渐成为计算机视觉的核心方法之后，基于深度学习算法的一系列目标检测算法大致可以分为两大流派： 两步走（two-stage）算法：先产生候选区域然后再进行CNN分类(RCNN系列) 一步走（one-stage）算法：直接对输入图像应用算法并输出类别和相应的定位(yolo系列) 无论是两步走还是一步走算法，它们都是在识别的快和准两点上寻找一个平衡点和或者极端点。要么准要么快，但随着深度学习和计算机视觉的向前发展，既快有准的算法也在逐渐的实现当中。 两步之R-CNN R-CNN 作为将深度学习引入目标检测算法的开山之作，在目标检测算法发展历史上具有重大意义。R-CNN 算法是两步走方法的代表，即先生成候选区域（region proposal），然后再利用 CNN 进行识别分类。由于候选框对于算法的成败起着关键作用，所以该方法就以 Region 开头首字母 R 加 CNN 进行命名。 这个网络也是目标检测的鼻祖了。其原理非常简单，主要通过提取多个Region Proposal(候选区域)来判断位置，作者认为以往的对每个滑动窗口进行检测算法是一种浪费资源的方式。在RCNN中，不再对所有的滑动窗口跑算法，而是只选择一些窗口，在少数窗口上运行CNN。 总体而言，R-CNN 方法分为四个步骤： 输入图像 利用selective search对图像生成1K~2K的候选区域（region proposal），这个量比传统的算法要少得多。具体一点，选出region proposal的方法是运行图像分割算法，对于分割算法跑出来的块，把它作为可能的region proposal输出。 提取特征：将region proposal resize为统一大小，送进去掉了softmax的CNN，对每个候region proposal提取特征 对区域进行分类：对从CNN output出来的特征向量送进每一类的SVM分类, 如果我有十个类别，那么每个region proposal要跑10个SVM，得到类别。这里为什么要用SVM而不是softmax，有一种说法是为了解决样本不均衡的问题，另外是早期神经网络还不如现在这样发达，当时SVM还是比较领先的分类器。 修正：对CNN output的特征向量（这个特征向量和第4步中拿去喂给SVM的是一个向量）做回归（左上角右下角的四个坐标），修正region proposal的位置。 视频参考：【深度学习大讲堂】首期第五讲：基于深度学习的目标检测技术 虽然 R-CNN 在 2013年的当时可谓横空出世，但也存在许多缺陷：selective search 方法生成训练网络的正负样本候选区域在速度上非常慢，影响了算法的整体速度；CNN 需要分别对每一个生成的候选区域进行一次特征提取，存在着大量的重复运算，制约了算法性能。 ​ 一步之yolo v1纵然两步走的目标检测算法在不断进化，检测准确率也越来越高，但两步走始终存在的速度的瓶颈。在一些实时的目标检测需求的场景中，R-CNN 系列算法终归是有所欠缺。因而一步走（one-stage）算法便应运而生了，其中以 yolo 算法系列为代表，演绎了一种端到端的深度学习系统的实时目标检测效果。yolo 算法系列的主要思想就是直接从输入图像得到目标物体的类别和具体位置，不再像 R-CNN 系列那样产生候选区域。这样做的直接效果便是快。 R-CNN SPP-Net ​ Fast-R-CNN Faster-RCNN 机器之心：像玩乐高一样拆解Faster R-CNN：详解目标检测的实现过程 图片分割 图像描述 图片生成 图片检索 参考 机器之心:深度 | 用于图像分割的卷积神经网络：从R-CNN到Mark R-CNN RCNN, Fast R-CNN 与 Faster RCNN理解及改进方法 图解YOLO RCNN, Fast R-CNN 与 Faster RCNN理解及改进方法 RCNN- 将CNN引入目标检测的开山之作 第八章 目标检测","categories":[{"name":"CV","slug":"CV","permalink":"http://yoursite.com/categories/CV/"}],"tags":[{"name":"视觉","slug":"视觉","permalink":"http://yoursite.com/tags/视觉/"}],"author":"Dufy"},{"title":"⭐️《计算机视觉零基础入门》二：深度学习基础","slug":"视频《计算机视觉零基础入门》笔记二","date":"2019-10-25T16:00:00.000Z","updated":"2021-02-27T06:28:07.061Z","comments":true,"path":"2019/10/26/视频《计算机视觉零基础入门》笔记二/","link":"","permalink":"http://yoursite.com/2019/10/26/视频《计算机视觉零基础入门》笔记二/","excerpt":"[TOC] 深度学习的理论基础部分","text":"[TOC] 深度学习的理论基础部分 神经网络神经元 前馈神经网络 梯度下降 误差BP 深度学习与传统神经网络区别 目标函数 关于交叉熵参见：简单的交叉熵损失函数，你真的懂了吗？ 梯度消失 改进的梯度下降 Batch Normalization由来 避免过适应 CNN基本组件 关于感受野，如何计算感受野(Receptive Field)——原理 在卷积神经网络中，感受野的定义是 卷积神经网络每一层输出的特征图（feature map）上的像素点在原始图像上映射的区域大小。 ——博客园 误差bp ## 参考","categories":[{"name":"CV","slug":"CV","permalink":"http://yoursite.com/categories/CV/"}],"tags":[{"name":"视觉","slug":"视觉","permalink":"http://yoursite.com/tags/视觉/"}],"author":"Dufy"},{"title":"⭐️《计算机视觉零基础入门》一","slug":"视频《计算机视觉零基础入门》笔记","date":"2019-10-20T16:00:00.000Z","updated":"2021-02-27T06:27:54.175Z","comments":true,"path":"2019/10/21/视频《计算机视觉零基础入门》笔记/","link":"","permalink":"http://yoursite.com/2019/10/21/视频《计算机视觉零基础入门》笔记/","excerpt":"[TOC] 计算机视觉的基础部分","text":"[TOC] 计算机视觉的基础部分 CV研究内容 开源库介绍OpenCV 12345678910111213import cv2jpg = cv2.imread('lena.jpg')print(jpg.shape) # 显示照片尺寸# print(jpg[:, :, 0])matrix = jpg[:, :, 0]print(matrix)cv2.imshow('Image', jpg)cv2.waitKey(0)# 释放窗口cv2.destroyAllWindows() Caffe TF 一段小代码： 1234567891011import tensorflow as tf#创建矩阵相乘matrix1 = tf.constant([[3, 3]]) #1*2matrix2 = tf.constant([[2], [2]]) #2*1product = tf.matmul(matrix1,matrix2)# 上面的操作是定义图，然后用Session去计算with tf.Session() as sess: result2 = sess.run(product) print(result2) 1[[12]] Pytorch Keras 图像预处理 颜色空间 图片存储原理 图像增强的目标 点运算 形态学运算 邻域运算 ==================================== 右上方这个框是怎么得到的呢？参考：高斯滤波器原理及其实现、高斯滤波 对于5*5，x-y坐标如下： 代入高斯分布函数可得到如下矩阵： 代码： 12% matlab中可以使用fspecial函数实现高斯滤波模板fspecial('gaussian', [ 5 5], 1) 123456789101112131415161718192021222324% ------------------------------------%关于高斯滤波生成%% date 2019/10/24% by Dufy%% ------------------------------------clcclearclose allformat compactsigma = 1.5;f=@(x,y)(1/2/pi/sigma^2 * exp(-(x.^2+y.^2)/2/sigma^2) );% gauss_filter = zeros(3,3)location_xy= [-2,2;-1,2;0,2;1,2;2,2]location_xy= [-1,1;0,1;1,1]filter(1,:) = [f(location_xy(:,1),location_xy(:,2)) ]'location_xy= location_xy + [0,-1]filter(2,:) =[f(location_xy(:,1),location_xy(:,2)) ]'location_xy= location_xy + [0,-1]filter(3,:) =[f(location_xy(:,1),location_xy(:,2)) ]'sum(filter(:))filter = filter/sum(filter(:)) 高斯滤波效果 ====================================** 频率域 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# coding=utf-8'''-------------------------------------------------Created by Dufy on 2019/10/25IDE used: PyCharm Community EditionDescription :1)openCV滤波2)比较高斯滤波，中值滤波-------------------------------------------------Change Activity:-------------------------------------------------'''import cv2import numpy as npimport matplotlib.pyplot as plt# jpg = cv2.imread('lenna-full.jpg') #彩色图像# print(jpg.shape) # 显示照片尺寸# # print(jpg[:, :, 0])# matrix = jpg[:, :, 0]#读取灰度图像img = cv2.imread('lenna.png', 0)print(img.shape) # 显示照片尺寸# print(jpg[:, :, 0])print(img)for i in range(2000): #添加点噪声 temp_x = np.random.randint(0, img.shape[0]) temp_y = np.random.randint(0, img.shape[1]) img[temp_x][temp_y] = 255# cv2.imshow('Image', img)# cv2.waitKey(0)blur_1 = cv2.GaussianBlur(img,(5,5),0)blur_2 = cv2.medianBlur(img, 5)plt.subplot(131)plt.imshow(img,'gray')plt.title('original image with noise')plt.subplot(132)plt.imshow(blur_1,'gray')plt.title('gauss blur')plt.subplot(133)plt.imshow(blur_2,'gray')plt.title('medium filter')plt.show() 关于cv2.GaussianBlur(img,(5,5),0)，这里的0 的解释，参见：OpenCV高斯滤波GaussianBlur 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# coding=utf-8'''-------------------------------------------------Created by Dufy on 2019/10/25IDE used: PyCharm Community EditionDescription :1)openCV滤波2)比较拉普拉斯，sobel滤波-------------------------------------------------Change Activity:-------------------------------------------------'''import cv2import numpy as npimport matplotlib.pyplot as plt# jpg = cv2.imread('lenna-full.jpg') #彩色图像# print(jpg.shape) # 显示照片尺寸# # print(jpg[:, :, 0])# matrix = jpg[:, :, 0]#读取灰度图像img = cv2.imread('chess.jpg', 0)print(img.shape) # 显示照片尺寸# print(jpg[:, :, 0])print(img)# for i in range(2000): #添加点噪声# temp_x = np.random.randint(0, img.shape[0])# temp_y = np.random.randint(0, img.shape[1])# img[temp_x][temp_y] = 255# cv2.imshow('Image', img)# cv2.waitKey(0)laplas = cv2.Laplacian(img, cv2.CV_64F) #拉普拉斯算子sobelx = cv2.Sobel(img, cv2.CV_64F,1,0, ksize=5)sobely = cv2.Sobel(img, cv2.CV_64F,0,1, ksize=5)plt.subplot(221)plt.imshow(img,'gray')plt.title('original image')plt.subplot(222)plt.imshow(laplas,'gray')plt.title('Laplacian')plt.subplot(223)plt.imshow(sobelx,'gray')plt.title('Sobel X')plt.subplot(224)plt.imshow(sobely,'gray')plt.title('Sobel Y')plt.show() 参考python 图像的离散傅立叶变换 1234567891011121314151617181920212223242526272829303132333435# coding=utf-8'''-------------------------------------------------Created by Dufy on 2019/10/25IDE used: PyCharm Community EditionDescription :1)openCV滤波2)FFT-------------------------------------------------Change Activity:-------------------------------------------------'''import numpy as npimport matplotlib.pyplot as pltimg = plt.imread('lenna.png')#根据公式转成灰度图img = 0.2126 * img[:,:,0] + 0.7152 * img[:,:,1] + 0.0722 * img[:,:,2]#显示原图plt.subplot(121),plt.imshow(img,'gray'),plt.title('original')#进行傅立叶变换，并显示结果f = np.fft.fft2(img)fshift = np.fft.fftshift(f)magnitude_spectrum = 20 * np.log(np.abs(fshift))plt.subplot(122),plt.imshow(magnitude_spectrum,'gray'),plt.title('magnitude_spectrum')plt.show() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# coding=utf-8&apos;&apos;&apos;-------------------------------------------------Created by Dufy on 2019/10/25IDE used: PyCharm Community EditionDescription :1)openCV滤波2)比较自适应直方图均衡与CLAHE效果-------------------------------------------------Change Activity:-------------------------------------------------&apos;&apos;&apos;import cv2import numpy as npimport matplotlib.pyplot as plt# jpg = cv2.imread(&apos;lenna-full.jpg&apos;) #彩色图像# print(jpg.shape) # 显示照片尺寸# # print(jpg[:, :, 0])# matrix = jpg[:, :, 0]#读取灰度图像img = cv2.imread(&apos;test.jpg&apos;, 0) #直接读为灰度图像print(img.shape) # 显示照片尺寸# print(jpg[:, :, 0])print(img)res = cv2.equalizeHist(img)clahe = cv2.createCLAHE(clipLimit=2,tileGridSize=(10,10))cla = clahe.apply(img)plt.subplot(131)plt.imshow(img,&apos;gray&apos;)plt.title(&apos;original image&apos;)plt.subplot(132)plt.imshow(res,&apos;gray&apos;)plt.title(&apos;Laplacian&apos;)plt.subplot(133)plt.imshow(cla,&apos;gray&apos;)plt.title(&apos;clahe&apos;)plt.show() 图像特征及描述 颜色特征 几何特征 特征点 ==================================== Harris角点 ==================================== ==================================== FAST角点 ==================================== ==================================== 斑点 ==================================== ==================================== SIFT ==================================== ==================================== SURF ==================================== ==================================== ORB ==================================== ==================================== LBP ==================================== ==================================== Gabor ==================================== 代码 ORB特征匹配,参考https://blog.csdn.net/a19990412/article/details/81208468 1234567891011121314151617181920212223242526272829303132333435363738394041# coding=utf-8'''-------------------------------------------------Created by Dufy on 2019/10/25IDE used: PyCharm Community EditionDescription :1)openCV滤波2)ORB 特征点匹配-------------------------------------------------Change Activity:-------------------------------------------------'''import cv2import numpy as npimport matplotlib.pyplot as plt# jpg = cv2.imread('lenna-full.jpg') #彩色图像# print(jpg.shape) # 显示照片尺寸# # print(jpg[:, :, 0])# matrix = jpg[:, :, 0]#读取灰度图像img2 = cv2.imread('test_orb1.jpg', 0) #直接读为灰度图像img1 = cv2.imread('test_orb2.jpg', 0) #直接读为灰度图像orb = cv2.ORB_create()kp1, des1 = orb.detectAndCompute(img1, None)kp2, des2 = orb.detectAndCompute(img2, None)bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)matches = bf.match(des1, des2)matches = sorted(matches, key=lambda x: x.distance)img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:80], img2, flags=2)plt.imshow(img3), plt.show() 深度学习之前的方法 图像分割 人脸检测 行人检测 参考 https://www.bilibili.com/video/av57591442/ 参考书 傅立叶变换如何理解？美颜和变声都是什么原理？李永乐老师告诉你","categories":[{"name":"CV","slug":"CV","permalink":"http://yoursite.com/categories/CV/"}],"tags":[{"name":"视觉","slug":"视觉","permalink":"http://yoursite.com/tags/视觉/"}],"author":"Dufy"},{"title":"⭐️学习资料","slug":"学习资料","date":"2019-10-16T16:00:00.000Z","updated":"2021-05-09T15:48:53.465Z","comments":true,"path":"2019/10/17/学习资料/","link":"","permalink":"http://yoursite.com/2019/10/17/学习资料/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… BOOK花书https://github.com/MingchaoZhu/DeepLearning Python for《Deep Learning》，该书为《深度学习》(花书) 数学推导、原理剖析与源码级别代码实现 李航《统计学习方法》《统计学习方法》这本书，附件里并没有代码实现，于是许多研究者复现了里面算法的代码，并放在github里分享，这里介绍几个比较热门的《统计学习方法》代码实现的项目： 《统计学习方法》的代码实现 1）https://github.com/fengdu78/lihang-code （标星：10.2k+） 这个仓库由黄海广博士整理，第一版的监督学习方法已经整理完毕(更新完十二章)，仓库的主要内容以Jupyter Notebook格式展现，同时介绍书上的主要算法及公式推导。 2) https://github.com/WenDesi/lihang_book_algorithm （标星：4.2k+） 这个仓库不介绍任何机器学习算法的原理，只是将《统计学习方法》中每一章的算法用我自己的方式实现一遍。除了李航书上的算法外，还实现了一些其他机器学习的算法，这个仓库用Python代码实现。(更新完十二章) 3）https://github.com/Dod-o/Statistical-Learning-Method_Code （标星：3.7k+） 这个仓库力求每行代码都有注释，重要部分注明公式来源。具体会追求下方这样的代码，学习者可以照着公式看程序，让代码有据可查。(更新完十章) 代码截图，注释完整且规范 4）https://github.com/SmirkCao/Lihang （标星：3.1k+） 这个仓库用markdown编写，前十二章更新完毕，后面部分也更新了大部分，没有代码，但是，公式推导相当全。 《代数、拓扑学、微分学、最优化理论–用于计算机科学和机器学习》要搞机器学习离不开数学，本文分享一本来自宾夕法尼亚大学计算机系教授Jean Gallier主编的面向机器学习的“数学全书”，内容涵盖线性代数、概率统计、拓扑学、微积分、最优化理论等面向ML的数学知识，共计1900余页！ 介绍链接：https://mp.weixin.qq.com/s/C_dcGAdsWuUjnvhRS5t-Xw 邱锡鹏深度学习https://www.zhihu.com/question/26754848 https://nndl.github.io/ ： 🍉书 笔记https://github.com/Vay-keen/Machine-learning-learning-notes 《机器学习》（西瓜书）公式推导解析 https://github.com/datawhalechina/pumpkin-book 在线阅读地址：https://datawhalechina.github.io/pumpkin-book 周志华《机器学习》手推笔记https://github.com/Sophia-11/Machine-Learning-Notes 动手深度学习英文：https://d2l.ai/index.html【内含pytorch版本】 中文：https://zh.d2l.ai/index.html pytorch版本 https://github.com/dsgiitr/d2l-pytorch 视频 哔哩哔哩： https://space.bilibili.com/209599371/channel/detail?cid=23541 Foundations of Machine Learning链接： https://cs.nyu.edu/~mohri/mlbook/ 知乎介绍：https://www.zhihu.com/question/22221180 综合项目查找网站Learn X by doing Y这个网站收集以学习为目的各种实战教程（比如自己动手写数据库、编译器等等），按照计算机语言进行分类，帮助读者快速查找可以自己动手做的简单项目。 Youtube 计算机科学课程资源This is The Entire Computer Science Curriculum in 1000 YouTube Videos Halfrost-Field 冰霜之地关于 阅读开源框架源代码的心得 ML/NLP/DLML-NLP 此项目是机器学习(Machine Learning)、深度学习(Deep Learning)、NLP面试中常考到的知识点和代码实现，也是作为一个算法工程师必会的理论基础知识。 NLP-Resources NLP_ability NLP算法工程师的日常以及核心竞争力 https://mp.weixin.qq.com/s/4fNF3LiaaQNxJqu55JLY8Q 针对不同的场景，选择合适的方法，获得最好的效果： 需求定性 调研–数据、模型 模型训练优化 部署上线 机器学习、NLP面试中常考到的知识点和代码实现也是作为一个算法工程师必会的理论基础知识。 https://github.com/NLP-LOVE/ML-NLP 注意，里面有关于推荐系统的章节 干货 | 如何从零学习人工智能？最好的资源都在这里了https://www.leiphone.com/news/201610/Bo67kHXGUcXbDFAL.html 莫烦https://morvanzhou.github.io/ 机器学习numpy 实现https://github.com/ddbourgin/numpy-ml 深度学习500问https://github.com/scutan90/DeepLearning-500-questions 谷歌机器学习速成课程https://github.com/yuanxiaosc/Google-Machine-learning-crash-course 【完结】深度学习CV算法工程师从入门到初级面试有多远，大概是25篇文章的距离https://mp.weixin.qq.com/s/HZ3Cd2jHuikyFN9ydvcMTw 注意是25篇文章集合 DeepLearning_Notes_CV学习记录 整理了我开始分享学习笔记到现在超过250篇优质文章，涵盖数据分析、爬虫、机器学习等方面，别再说不知道该从哪开始，实战哪里找了 AiLearning: 机器学习 、深度学习 、自然语言处理 NLPhttps://github.com/apachecn/AiLearning GitHub上最励志的计算机自学教程：8个月，从中年Web前端到亚马逊百万年薪软件工程师 | 中文版https://github.com/jwasham/coding-interview-university NLP干货https://mp.weixin.qq.com/s/3In1Z3kcrIeFb99I_p6JEw 视频 学堂在线https://next.xuetangx.com/ CS224n 斯坦福深度自然语言处理课https://www.bilibili.com/video/av41393758/ 吴恩达深度学习 《深度学习工程师微专业》链接点此 吴恩达深度学习编程作业视频 吴恩达《深度学习》学习笔记(xmind)、作业代码、代码视频讲解 https://github.com/greebear/deeplearning.ai-notes 数据挖掘：理论与算法（自主模式） 课程描述 最有趣的理论+最有用的算法=不得不学的数据科学 网易–深度学习基础课程本课程适用于对深度学习及其在计算机视觉领域的应用等内容感兴趣的高校学生或者工程技术人员。 LinuxLinux C&amp;C++网络编程实践-陈硕 Python官方文档 https://docs.python.org/3/ 可以查看函数实现方法 比如， Python-100-DaysTheAlgorithms/PythonAll Algorithms implemented in Python C++侯捷C++系列视频已经保存在网盘 TensorFlowzhufz/nlp_research NLP research：基于tensorflow的nlp深度学习项目，支持文本分类/句子匹配/序列标注/文本生成 四大任务 PyTorchPyTorch的tutorialhttps://pytorch.org/tutorials/ 爬虫 《自己动手写网络爬虫》 素材图片Pexels 天堂图片网https://www.ivsky.com/ 蜂鸟网 参考","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}],"author":"Dufy"},{"title":"⭐️《白话大数据与机器学习》","slug":"《白话大数据与机器学习》","date":"2019-10-14T16:00:00.000Z","updated":"2021-02-27T06:23:25.776Z","comments":true,"path":"2019/10/15/《白话大数据与机器学习》/","link":"","permalink":"http://yoursite.com/2019/10/15/《白话大数据与机器学习》/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 数据生命周期大数据产业生产流程从数据的生命周期的传导和演变上可以分为这样几个部分： 大数据技术方向大数据人才可以分为3个方向： 大数据架构 大数据分析 大数据开发 步入数据之门承载了信息的东西才是数据 何为信息 信息是用来消除随机不确定性的东西—–香农《通信的数学理论》 何为算法 商业智能 通过应用基于事实的支持系统来辅助商业决策的制定。商业智能技术提供使企业迅速分析数据的技术和方法，包括收集、管理和分析数据，将这些数据转化为有用的 信息。 排列组合 指标所有这类用单一的数据定义来概括性描述一些抽象或复杂数据的方式方法都叫做“指标”。 没有指标的体检报告： 在指标建立的过程中，我们实际做的是一个建模的过程，围绕建模要做很多辅助工作。 指标的选择重点考虑如下原则： 数字化 易衡量，获取成本要低 意义清晰 周期适当 尽量客观 信息论信息熵 用户画像用户画像，是指 画像的方法：标签 标签从哪来？ 大数据框架大数据解决的是单机无法处理的数据 框架的作用 参考","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/大数据/"}],"author":"Dufy"},{"title":"⭐️关于雷达一些基本知识","slug":"关于雷达一些基本知识","date":"2019-10-12T16:00:00.000Z","updated":"2021-02-27T06:24:46.662Z","comments":true,"path":"2019/10/13/关于雷达一些基本知识/","link":"","permalink":"http://yoursite.com/2019/10/13/关于雷达一些基本知识/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 雷达，是英文Radar的音译，源于radio detection and ranging的缩写，意思为”无线电探测和测距”，即用无线电的方法发现目标并测定它们的空间位置。 因此，雷达也被称为“无线电定位”。 雷达是利用电磁波探测目标的电子设备。 雷达发展史里程碑事件链接：http://www.radartutorial.eu/04.history/hi04.en.html 雷达系统组成 动图参见：http://www.radartutorial.eu/01.basics/Radar%20Principle.en.html 具体组成部件如下： 测距和定位 动画见：http://www.radartutorial.eu/01.basics/Distance-determination.en.html slant range(倾斜距离)通过对脉冲再次出现时间的测量，斜距R计算如下： azimuth angle（方位角）β测量 elevation angle（高低角/仰角） ε 测量 关于角方向 的测量，参考： 我们知道，在雷达正对目标时回波强度最大 参考 国外雷达学习网站：http://www.radartutorial.eu/","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[],"author":"Dufy"},{"title":"⭐️自动驾驶解读","slug":"自动驾驶解读","date":"2019-09-28T16:00:00.000Z","updated":"2021-02-27T06:32:51.491Z","comments":true,"path":"2019/09/29/自动驾驶解读/","link":"","permalink":"http://yoursite.com/2019/09/29/自动驾驶解读/","excerpt":"[TOC] 这是摘要……","text":"[TOC] 这是摘要…… 这是全文 这是一个测试，2019.。。。 1. 自动驾驶分级国际汽车工程师协会将自动驾驶分为5级，如下： 可见，从L2后才出现“自动驾驶”的称呼。如果一辆车能同时做到自适应巡航和车道保持辅助，那这款车就跨入了L2 的门槛。并且，从L3开始，对环境的观察由系统来替代。 2. 人工智能算法在自动驾驶中的应用场景 2.1 路障监测关于路障监测，用到如下两个重要的工具 占据栅格 不确定性锥 用来预测汽车附近物体位置和移动速度的工具 2.2 路线规划当路障监测完成，“轨迹规划器”模块开始工作，计算出最佳行进路线，并保证遵守交通规则，减少行程时间和碰撞风险 参考 《百面机器学习》","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[]},{"title":"⭐️杂项","slug":"杂项","date":"2000-01-28T16:00:00.000Z","updated":"2021-04-18T15:03:51.012Z","comments":true,"path":"2000/01/29/杂项/","link":"","permalink":"http://yoursite.com/2000/01/29/杂项/","excerpt":"","text":"github 加速访问国内加速访问Github的办法 主要是刷新dns 缓存【注：ip可能根据时间而变化，所以，发现慢的时候建议重新再获取一次ip进行修改】 1 打开http://IPAddress.com网站，搜目标网址对应的IP地址 2 修改本地电脑系统hosts文件 sudo vi /etc/hosts 3 刷新系统dns缓存 修改后会直接生效，如未生效重起或用命令刷新 Windows 中 用WIN+R打开命令行。输入ipconfig /flushdns Linux中 systemctl restart nscd Mac中 sudo dscacheutil -flushcache或sudo killall -HUP mDNSResponder jupyter网页打开： 1python3 -m IPython notebook 在线打开.ipynb文件： https://nbviewer.jupyter.org/ 重要参考： Jupyter Notebook新手入门手册 1234567891011121314代码：Python 代码Shell 指令Magic 指令+ 各种骚操作标记语言：字体（font）链接（link）公式（formula）图片（image）视频（video）网页（HTML）+ 各种骚操作 markdownhttps://www.zybuluo.com/mdeditor#fnref:latex hexo 发布 1hexo g &amp;&amp; hexo d Notionhttps://www.notion.so/ LaTeX在线公式编辑转图片 画图https://app.diagrams.net/ ![image-20210418225707396](../../../Library/Application Support/typora-user-images/image-20210418225707396.png) 参考 读者：厨子，你用啥写的文章啊","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}],"tags":[{"name":"编辑","slug":"编辑","permalink":"http://yoursite.com/tags/编辑/"}],"author":"Dufy"}]}