<!DOCTYPE html><html lang="zh-CN//语言"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="1EB8XoOl0C"><meta name="google-site-verification" content="K7thEgdLm0UfRWJ5MGdF7sCcjClSzAlxFLPv2Oz5CGM"><title> ⭐️动手深度学习 · Note</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="⭐️动手深度学习 - Dufy"><meta name="keywords"><meta name="author" content="Dufy"><link rel="short icon" href="/images/favicon.ico"><link rel="stylesheet" href="/css/bubuzou.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Note"><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script></head><body><header><div class="header row"> <a href="/" class="logo-link"><img src="/images/logo.png"></a><ul id="nav_list" class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" data-hover="博文" class="nav-list-link">博文</a></li><li class="nav-list-item"><a href="/archives/" target="_self" data-hover="归档" class="nav-list-link">归档</a></li></ul><div class="search"><a id="search_btn" href="#search"></a></div><div id="nav_btn" class="nav-btn"><span></span><span></span><span></span></div></div></header><div class="row scroll-con"><section class="container"><!-- for archive page--><div id="postAr" class="post"><article class="post-block"><h1 class="post-title">⭐️动手深度学习</h1><div class="post-info">2019-12-26</div><div class="post-content"><p>[TOC]</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaahoqu64cj30uw0lmac0.jpg" alt="image-20191226222332335"></p>
<a id="more"></a>


<h2 id="总论"><a href="#总论" class="headerlink" title="总论"></a>总论</h2><p>具体来说，应用深度学习需要同时理解：</p>
<ol>
<li>问题的动机和特点；</li>
<li>将大量不同类型神经网络层通过特定方式组合在一起的模型背后的数学原理；</li>
<li>在原始数据上拟合极复杂的深层模型的优化算法；</li>
<li>有效训练模型、避免数值计算陷阱以及充分利用硬件性能所需的工程技能；</li>
<li>为解决方案挑选合适的变量（超参数）组合的经验。</li>
</ol>
<p>我们将在同一个地方阐述剖析问题所需的批判性思维、解决问题所需的数学知识，以及实现解决方案所需的工程技能。</p>
<h2 id="深度学习简介"><a href="#深度学习简介" class="headerlink" title="深度学习简介"></a>深度学习简介</h2><p>事实上，这也是目前的机器学习和深度学习应用共同的核心思想：我们可以称其为“用数据编程”。</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaahudlvrfj31cq08i78g.jpg" alt="image-20191226222857691"></p>
<p>通俗来说，机器学习是一门讨论各式各样的适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。深度学习是指机器学习中的一类函数，它们的形式通常为多层神经网络。近年来，仰仗着大数据集和强大的硬件，深度学习已逐渐成为处理图像、文本语料和声音信号等复杂高维度数据的主要方法。</p>
<h3 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h3><p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaai5jvjxtj31d204ejte.jpg" alt="image-20191226223942025"></p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaai8zpwhnj31c205mq6h.jpg" alt="image-20191226224259603"></p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaaicmwi0xj31bc0620v4.jpg" alt="image-20191226224630522"></p>
<h3 id="机器学习和深度学习的关系"><a href="#机器学习和深度学习的关系" class="headerlink" title="机器学习和深度学习的关系"></a>机器学习和深度学习的关系</h3><p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaaj83s557j31340pitau.jpg" alt="image-20191226231644260"></p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaajdj0simj31ck0kwwoe.jpg" alt="image-20191226232156484"></p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaajt445xnj31dk0b877i.jpg" alt="image-20191226233656157"></p>
<blockquote>
<p>“经验”通常以“数据”形式存在</p>
</blockquote>
<h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><h3 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h3><p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf705tk39j31dw0qiwik.jpg" alt="image-20191231000143183"></p>
<blockquote>
<p>从打印<code>D</code>时显示的属性<code>&lt;NDArray 2*3 @cpu(0)&gt;</code>可以看出，它是二维数组，且被创建在CPU使用的内存上。其中“@cpu(0)”里的0没有特别的意义，并不代表特定的核。</p>
</blockquote>
<h3 id="自动求梯度"><a href="#自动求梯度" class="headerlink" title="自动求梯度"></a>自动求梯度</h3><p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gah2c9i10oj30h201qwej.jpg" alt="image-20200101145128346"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = nd.arange(<span class="number">4</span>).reshape((<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">print(<span class="string">'x:&#123;&#125;'</span>.format(x))</span><br><span class="line">print(<span class="string">'以下为求关于x的梯度，注意以下为一个求梯度的整体表达'</span>)</span><br><span class="line">x.attach_grad()</span><br><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">    y = <span class="number">2</span>*nd.dot(x.T, x)</span><br><span class="line">y.backward()</span><br><span class="line"><span class="keyword">assert</span> (x.grad - <span class="number">4</span>*x).norm().asscalar() == <span class="number">0</span>  </span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x:</span><br><span class="line">[[0.]</span><br><span class="line"> [1.]</span><br><span class="line"> [2.]</span><br><span class="line"> [3.]]</span><br><span class="line">&lt;NDArray 4x1 @cpu(0)&gt;</span><br><span class="line">以下为求关于x的梯度</span><br><span class="line"></span><br><span class="line">[[ 0.]</span><br><span class="line"> [ 4.]</span><br><span class="line"> [ 8.]</span><br><span class="line"> [12.]]</span><br><span class="line">&lt;NDArray 4x1 @cpu(0)&gt;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Python3 assert（断言）断言可以在条件不满足程序运行的情况下直接返回错误，而不必等待程序运行后出现崩溃的情况</p>
</blockquote>
<p>注意：以下两种写法结果是一样的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with autograd.record():</span><br><span class="line">    y = 2*nd.dot(x.T, x)</span><br><span class="line">    or</span><br><span class="line">    y = 2*x**2</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = nd.arange(4).reshape((4, 1))</span><br><span class="line">print(&apos;x:&#123;&#125;&apos;.format(x))</span><br><span class="line">print(&apos;以下为求关于x的梯度，注意以下为一个求梯度的整体表达&apos;)</span><br><span class="line">x.attach_grad()</span><br><span class="line">with autograd.record():</span><br><span class="line">    y = 2*x**2</span><br><span class="line">y.backward()</span><br><span class="line">assert (x.grad - 4*x).norm().asscalar() == 0  </span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>

<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gb3gb4ae6uj30sw0fetly.jpg" alt="image-20200120233713951"></p>
<h2 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h2><h3 id="神经网路图表示"><a href="#神经网路图表示" class="headerlink" title="神经网路图表示"></a>神经网路图表示</h3><p>线性回归和softmax回归都可看成单层神经网络。</p>
<blockquote>
<p>这种看法很新颖</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf63rpep8j30gw08kmxn.jpg" alt="image-20191230233033578"></p>
<h3 id="矢量计算表示"><a href="#矢量计算表示" class="headerlink" title="矢量计算表示"></a>矢量计算表示</h3><blockquote>
<p> 注意红框中的“分母”</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf67lzt7cj31d20qojwq.jpg" alt="image-20191230233416468"></p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf6c4y817j31cc08sq4w.jpg" alt="image-20191230233837633"></p>
<h3 id="从0实现"><a href="#从0实现" class="headerlink" title="从0实现"></a>从0实现</h3><p>参考【3】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = nd.random.normal(scale=<span class="number">1</span>, shape=(num_examples, num_inputs))  <span class="comment"># 标注差为1</span></span><br><span class="line"></span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += nd.random.normal(scale=<span class="number">0.01</span>, shape=labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(batch_size, features, labels)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(indices)  <span class="comment"># 样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = nd.array(indices[i: min(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features.take(j), labels.take(j)  <span class="comment"># take函数根据索引返回对应元素</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    print(<span class="string">'1个batch_size:'</span>, X, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gb3f5zzjkrj310o0gcqlo.jpg" alt="image-20200120225738295"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">w = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_inputs, <span class="number">1</span>))</span><br><span class="line">b = nd.zeros(shape=(<span class="number">1</span>,))</span><br><span class="line">print(w,b)</span><br><span class="line">w.attach_grad()  <span class="comment">#申请存储梯度所需要的内存</span></span><br><span class="line">b.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span><span class="params">(X, w, b)</span>:</span>  <span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line">    <span class="keyword">return</span> nd.dot(X, w) + b</span><br><span class="line"><span class="comment"># 定义损失函数, 两种写法【参见自动求梯度一节】</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span><span class="params">(y_hat, y)</span>:</span>  <span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line"><span class="comment">#     print((y_hat - y.reshape(y_hat.shape)) ** 2 / 2,'-------------')</span></span><br><span class="line"><span class="comment">#     return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2   #&lt;NDArray 10x1 @cpu(0)&gt; -------------</span></span><br><span class="line"></span><br><span class="line">    tep = y_hat - y.reshape(y_hat.shape)</span><br><span class="line">    print(nd.dot(tep.T, tep),<span class="string">'============'</span>)</span><br><span class="line">    <span class="keyword">return</span> nd.dot(tep.T, tep)</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 定义优化算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span>  <span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line"><span class="comment">#         print(param.grad)</span></span><br><span class="line">        param[:] = param - lr * param.grad / batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line">    <span class="comment"># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X</span></span><br><span class="line">    <span class="comment"># 和y分别是小批量样本的特征和标签</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):  <span class="comment">#每次取1个batch,得到X,y</span></span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X, w, b), y)  <span class="comment"># l是有关小批量X和y的损失,l:10*1</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用小批量随机梯度下降迭代模型参数</span></span><br><span class="line">    train_l = loss(net(features, w, b), labels)  <span class="comment">#计算所有训练样本的损失，train_l:1000*1</span></span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().asnumpy()))</span><br><span class="line">    </span><br><span class="line">print(true_w, w)</span><br><span class="line">print(true_b, b)</span><br><span class="line">--------------------------------------------------</span><br><span class="line">[[<span class="number">0.00077456</span>]</span><br><span class="line"> [<span class="number">0.01180387</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x1 @cpu(<span class="number">0</span>)&gt; </span><br><span class="line">[<span class="number">0.</span>]</span><br><span class="line">&lt;NDArray <span class="number">1</span> @cpu(<span class="number">0</span>)&gt;</span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.028056</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.000091</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.000050</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.000050</span></span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.000050</span></span><br><span class="line">[<span class="number">2</span>, <span class="number">-3.4</span>] </span><br><span class="line">[[ <span class="number">1.9994183</span>]</span><br><span class="line"> [<span class="number">-3.3996756</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x1 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line"><span class="number">4.2</span> </span><br><span class="line">[<span class="number">4.2005925</span>]</span><br><span class="line">&lt;NDArray <span class="number">1</span> @cpu(<span class="number">0</span>)&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>batch_size取值笔记</li>
</ul>
<p>通过修改不同的batch_size，可以看出，小的batch_size可以在更少用更少的epoch收敛</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gah3psi8ewj30r00lk0xl.jpg" alt="image-20200101153906858"></p>
<ul>
<li>关于data_iter(）解读</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1galkuqlb3tj31ka0jegyf.jpg" alt="image-20200105123416338"></p>
<h3 id="利用Gluon接口更简洁实现"><a href="#利用Gluon接口更简洁实现" class="headerlink" title="利用Gluon接口更简洁实现"></a>利用Gluon接口更简洁实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#=====================生成数据集</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = nd.random.normal(scale=<span class="number">1</span>, shape=(num_examples, num_inputs))</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += nd.random.normal(scale=<span class="number">0.01</span>, shape=labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#=====================读取数据</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> data <span class="keyword">as</span> gdata</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># 将训练数据的特征和标签组合</span></span><br><span class="line">dataset = gdata.ArrayDataset(features, labels)</span><br><span class="line"><span class="comment"># 随机读取小批量</span></span><br><span class="line">data_iter = gdata.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">    print(X, y)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="comment">#=====================定义模型</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))  <span class="comment"># 定义输出层个数为1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line">loss = gloss.L2Loss()  <span class="comment"># 平方损失又称L2范数损失</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化算法，小批量随机梯度下降（sgd）</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.03</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================训练模型</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss: %f'</span> % (epoch, l.mean().asnumpy()))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[[-0.9445115   1.9230403 ]</span><br><span class="line"> [ 0.28609878 -0.5688993 ]</span><br><span class="line"> [ 2.0842285  -1.3100709 ]</span><br><span class="line"> [ 1.233935   -1.609952  ]</span><br><span class="line"> [ 0.24503022  0.40995437]</span><br><span class="line"> [-1.3501216  -0.04495556]</span><br><span class="line"> [-1.4607683  -3.0632296 ]</span><br><span class="line"> [-0.03292305  0.57150906]</span><br><span class="line"> [ 0.29747176 -1.009977  ]</span><br><span class="line"> [ 1.4822187   0.07581457]]</span><br><span class="line">&lt;NDArray 10x2 @cpu(0)&gt; </span><br><span class="line">[-4.2168045  6.709853  12.82297   12.135502   3.3031228  1.6529553</span><br><span class="line"> 11.694708   2.186649   8.227948   6.9211607]</span><br><span class="line">&lt;NDArray 10 @cpu(0)&gt;</span><br><span class="line">epoch 1, loss: 0.041319</span><br><span class="line">epoch 2, loss: 0.000152</span><br><span class="line">epoch 3, loss: 0.000046</span><br></pre></td></tr></table></figure>

<p>模型参数读取：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># =====================获取模型参数</span><br><span class="line">dense = net[0]</span><br><span class="line">true_w, dense.weight.data()</span><br><span class="line">([2, -3.4], </span><br><span class="line"> [[ 1.999795 -3.399354]]</span><br><span class="line"> &lt;NDArray 1x2 @cpu(0)&gt;)</span><br><span class="line">true_b, dense.bias.data()</span><br><span class="line">(4.2, </span><br><span class="line"> [4.1996756]</span><br><span class="line"> &lt;NDArray 1 @cpu(0)&gt;)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>值得一提的是，在Gluon中我们无须指定每一层输入的形状，例如线性回归的输入个数。当模型得到数据时，例如后面执行<code>net(X)</code>时，模型将自动推断出每一层的输入个数。我们将在之后“深度学习计算”一章详细介绍这种机制。Gluon的这一设计为模型开发带来便利。【4】</p>
</blockquote>
<h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1galsbe7uerj30k209075h.jpg" alt="image-20200105165229591"></p>
<p>对于分类问题，模型输出为（0，1）间的概率值。</p>
<ul>
<li>损失函数形式</li>
</ul>
<p>采用平方损失不再合适。<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbq015t3r9j31bs06iacy.jpg" alt="image-20200209114203013"></p>
<p>损失函数采用交叉熵：</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gals6e0sulj30f203uglr.jpg" alt="image-20200105164740391"></p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gals7t2mszj31cc04mjtk.jpg" alt="image-20200105164902255"></p>
<ul>
<li>实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_inputs, num_outputs))</span><br><span class="line">b = nd.zeros(num_outputs)</span><br><span class="line"></span><br><span class="line">W.attach_grad()</span><br><span class="line">b.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">    X_exp = X.exp()</span><br><span class="line">    partition = X_exp.sum(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> softmax(nd.dot(X.reshape((<span class="number">-1</span>, num_inputs)), W) + b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -nd.pick(y_hat, y).log()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(axis=<span class="number">1</span>) == y.astype(<span class="string">'float32'</span>)).mean().asscalar()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数已保存在d2lzh包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中</span></span><br><span class="line"><span class="comment"># 描述</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net)</span>:</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        y = y.astype(<span class="string">'float32'</span>)</span><br><span class="line">        acc_sum += (net(X).argmax(axis=<span class="number">1</span>) == y).sum().asscalar()</span><br><span class="line">        n += y.size</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br><span class="line"></span><br><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch3</span><span class="params">(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">              params=None, lr=None, trainer=None)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                y_hat = net(X)</span><br><span class="line">                l = loss(y_hat, y).sum()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> trainer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                d2l.sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                trainer.step(batch_size)  <span class="comment"># “softmax回归的简洁实现”一节将用到</span></span><br><span class="line">            y = y.astype(<span class="string">'float32'</span>)</span><br><span class="line">            train_l_sum += l.asscalar()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(axis=<span class="number">1</span>) == y).sum().asscalar()</span><br><span class="line">            n += y.size</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size,</span><br><span class="line">          [W, b], lr)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> epoch 1, loss 0.7875, train acc 0.748, test acc 0.804</span><br><span class="line">epoch 2, loss 0.5729, train acc 0.812, test acc 0.821</span><br><span class="line">epoch 3, loss 0.5287, train acc 0.825, test acc 0.831</span><br><span class="line">epoch 4, loss 0.5045, train acc 0.831, test acc 0.833</span><br><span class="line">epoch 5, loss 0.4898, train acc 0.834, test acc 0.839</span><br></pre></td></tr></table></figure>

<ul>
<li>简洁实现</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import d2lzh as d2l</span><br><span class="line">from mxnet import gluon, init</span><br><span class="line">from mxnet.gluon import loss as gloss, nn</span><br><span class="line"></span><br><span class="line">batch_size = 256</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(10))</span><br><span class="line">net.initialize(init.Normal(sigma=0.01))</span><br><span class="line"></span><br><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), &apos;sgd&apos;, &#123;&apos;learning_rate&apos;: 0.1&#125;)</span><br><span class="line"></span><br><span class="line">num_epochs = 5</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,</span><br><span class="line">              None, trainer)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 0.7888, train acc 0.749, test acc 0.805</span><br><span class="line">epoch 2, loss 0.5746, train acc 0.810, test acc 0.825</span><br><span class="line">epoch 3, loss 0.5292, train acc 0.825, test acc 0.826</span><br><span class="line">epoch 4, loss 0.5054, train acc 0.831, test acc 0.837</span><br><span class="line">epoch 5, loss 0.4898, train acc 0.833, test acc 0.842</span><br></pre></td></tr></table></figure>

<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaphrn5rc3j30uk0dqtbi.jpg" alt="image-20200108214945500"></p>
<p>在图3.3所示的多层感知机中，输入和输出个数分别为4和3，中间的隐藏层中包含了5个隐藏单元（hidden unit）。由于输入层不涉及计算，图3.3中的多层感知<strong>机的层数为2</strong>。</p>
<ul>
<li>激活函数的引入</li>
</ul>
<p>为了 避免多层等效为单层NN，需要引入非线性变换，也就是激活函数</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbqaf5lmmpj317w0deqcp.jpg" alt="image-20200209174128364"></p>
<ul>
<li>常用激活函数及其导数</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbqachdnqej31bm0pu0w3.jpg" alt="image-20200209173854754"></p>
<ul>
<li>从0实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon, init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_inputs, num_hiddens))</span><br><span class="line">b1 = nd.zeros(num_hiddens)</span><br><span class="line">W2 = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_hiddens, num_outputs))</span><br><span class="line">b2 = nd.zeros(num_outputs)</span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 定义激活函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.maximum(X, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    X = X.reshape((<span class="number">-1</span>, num_inputs))</span><br><span class="line">    H = relu(nd.dot(X, W1) + b1)</span><br><span class="line">    <span class="keyword">return</span> nd.dot(H, W2) + b2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================训练模型</span></span><br><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,</span><br><span class="line">              params, lr)</span><br><span class="line">              </span><br><span class="line">-----------------------------</span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.8098</span>, train acc <span class="number">0.697</span>, test acc <span class="number">0.743</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.4928</span>, train acc <span class="number">0.817</span>, test acc <span class="number">0.851</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.4290</span>, train acc <span class="number">0.843</span>, test acc <span class="number">0.851</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.3962</span>, train acc <span class="number">0.853</span>, test acc <span class="number">0.869</span></span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.3736</span>, train acc <span class="number">0.862</span>, test acc <span class="number">0.869</span></span><br></pre></td></tr></table></figure>

<ul>
<li>简洁实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon, init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),  <span class="comment">#隐藏层单元个数为256</span></span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================训练模型</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.5</span>&#125;)</span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>,</span><br><span class="line">              <span class="literal">None</span>, trainer)</span><br><span class="line">              </span><br><span class="line">-----------------------</span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.7866</span>, train acc <span class="number">0.704</span>, test acc <span class="number">0.823</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.4899</span>, train acc <span class="number">0.820</span>, test acc <span class="number">0.825</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.4288</span>, train acc <span class="number">0.842</span>, test acc <span class="number">0.860</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.3978</span>, train acc <span class="number">0.853</span>, test acc <span class="number">0.868</span></span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.3681</span>, train acc <span class="number">0.864</span>, test acc <span class="number">0.871</span></span><br></pre></td></tr></table></figure>

<h2 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h2><ul>
<li>训练误差和泛化误差</li>
</ul>
<p>通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。</p>
<p>在机器学习里，我们通常假设训练数据集（训练题）和测试数据集（测试题）里的每一个样本都是从同一个概率分布中相互独立地生成的。</p>
<p>由于数据分布的差异，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，<strong>一味地降低训练误差并不意味着泛化误差一定会降低。</strong></p>
<p><strong>机器学习模型应关注降低泛化误差。</strong></p>
<ul>
<li>模型复杂度的影响</li>
</ul>
<blockquote>
<p>给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。应对欠拟合和过拟合的一个办法是针对数据集选择合适复杂度的模型。</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gapjvudr3hj30km0gujsj.jpg" alt="image-20200108230301991"></p>
<ul>
<li>具体反映</li>
</ul>
<p>多项式函数拟合实验【5】，注意其 表现形式</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbqcna8xznj316m0tajxo.jpg" alt="image-20200209185829678"></p>
<h3 id="避免过拟合"><a href="#避免过拟合" class="headerlink" title="避免过拟合"></a>避免过拟合</h3><ul>
<li>权重衰减【6】</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gasjke29scj31c40ten5k.jpg" alt="image-20200111130842927"></p>
<blockquote>
<p>可见，L2范数正则化令权重w1和w2先自乘小于1的数，再减去不含惩罚项的梯度。因此，L2范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。</p>
</blockquote>
<ul>
<li>dropout</li>
</ul>
<p>丢弃法不改变其输入的期望值，注意是对隐藏层的输出进行概率丢弃</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbqju8fzmuj31b60i8gpu.jpg" alt="image-20200209230721691"></p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gasjya1q7qj30nk0dm0un.jpg" alt="image-20200111132204528"></p>
<p>丢弃概率是丢弃法的超参数。</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gask345k59j31di06wn0g.jpg" alt="image-20200111132642902"></p>
<ul>
<li>实现</li>
</ul>
<p>主要是dropout 的添加实现，其它与之前的一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, gluon, init, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_prob)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= drop_prob &lt;= <span class="number">1</span></span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    <span class="comment"># 这种情况下把全部元素都丢弃</span></span><br><span class="line">    <span class="keyword">if</span> keep_prob == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X.zeros_like()</span><br><span class="line">    mask = nd.random.uniform(<span class="number">0</span>, <span class="number">1</span>, X.shape) &lt; keep_prob</span><br><span class="line">    <span class="keyword">return</span> mask * X / keep_prob</span><br><span class="line"></span><br><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_inputs, num_hiddens1))</span><br><span class="line">b1 = nd.zeros(num_hiddens1)</span><br><span class="line">W2 = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_hiddens1, num_hiddens2))</span><br><span class="line">b2 = nd.zeros(num_hiddens2)</span><br><span class="line">W3 = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_hiddens2, num_outputs))</span><br><span class="line">b3 = nd.zeros(num_outputs)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2, W3, b3]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br><span class="line">    </span><br><span class="line">drop_prob1, drop_prob2 = <span class="number">0.2</span>, <span class="number">0.5</span>  <span class="comment">#通常的建议是把靠近输入层的丢弃概率设得小一点</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    X = X.reshape((<span class="number">-1</span>, num_inputs))</span><br><span class="line">    H1 = (nd.dot(X, W1) + b1).relu()</span><br><span class="line">    <span class="keyword">if</span> autograd.is_training():  <span class="comment"># 只在训练模型时使用丢弃法</span></span><br><span class="line">        H1 = dropout(H1, drop_prob1)  <span class="comment"># 在第一层全连接后添加丢弃层</span></span><br><span class="line">    H2 = (nd.dot(H1, W2) + b2).relu()</span><br><span class="line">    <span class="keyword">if</span> autograd.is_training():</span><br><span class="line">        H2 = dropout(H2, drop_prob2)  <span class="comment"># 在第二层全连接后添加丢弃层</span></span><br><span class="line">    <span class="keyword">return</span> nd.dot(H2, W3) + b3</span><br><span class="line">              </span><br><span class="line">num_epochs, lr, batch_size = <span class="number">5</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">train_iter, test_iter = d2l.load_da</span><br><span class="line">ta_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,</span><br><span class="line">              params, lr)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 1.1687, train acc 0.553, test acc 0.739</span><br><span class="line">epoch 2, loss 0.6004, train acc 0.774, test acc 0.837</span><br><span class="line">epoch 3, loss 0.5014, train acc 0.817, test acc 0.852</span><br><span class="line">epoch 4, loss 0.4513, train acc 0.835, test acc 0.863</span><br><span class="line">epoch 5, loss 0.4185, train acc 0.847, test acc 0.862</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意,通过 is_training() 来判断运行模式为训练还是测试</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(autograd.is_training())</span><br><span class="line">with autograd.record():</span><br><span class="line">    print(autograd.is_training())</span><br><span class="line">    </span><br><span class="line">False</span><br><span class="line">True</span><br></pre></td></tr></table></figure>

<blockquote>
<p>关于dropout函数    <img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbst9grwoaj318w0n4adl.jpg" alt="image-20200211220427356"></p>
</blockquote>
<ul>
<li>简洁实现</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(256, activation=&quot;relu&quot;),</span><br><span class="line">        nn.Dropout(drop_prob1),  # 在第一个全连接层后添加丢弃层</span><br><span class="line">        nn.Dense(256, activation=&quot;relu&quot;),</span><br><span class="line">        nn.Dropout(drop_prob2),  # 在第二个全连接层后添加丢弃层</span><br><span class="line">        nn.Dense(10))</span><br><span class="line">net.initialize(init.Normal(sigma=0.01))</span><br><span class="line"></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), &apos;sgd&apos;, &#123;&apos;learning_rate&apos;: lr&#125;)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,</span><br><span class="line">              None, trainer)</span><br></pre></td></tr></table></figure>

<h2 id="正向传播、反向传播"><a href="#正向传播、反向传播" class="headerlink" title="正向传播、反向传播"></a>正向传播、反向传播</h2><p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbsv4luipuj31760iydoz.jpg" alt="image-20200211230902827"></p>
<p>反向传播公式：（注意蓝色框里）</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbsv5sdpasj317y0hmtie.jpg" alt="image-20200211231010920"></p>
<p>结合上面👆所示，可以看出正向传播与反向传播相互依赖【7】</p>
<blockquote>
<p>因此，在模型参数初始化完成后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。既然我们在反向传播中使用了正向传播中计算得到的中间变量来避免重复计算，那么这个复用也导致正向传播结束后不能立即释放中间变量内存。这也是训练要比预测占用更多内存的一个重要原因。另外需要指出的是，这些中间变量的个数大体上与网络层数线性相关，每个变量的大小跟批量大小和输入个数也是线性相关的，它们是导致较深的神经网络使用较大批量训练时更容易超内存的主要原因。</p>
</blockquote>
<h2 id="CNN经典网络"><a href="#CNN经典网络" class="headerlink" title="CNN经典网络"></a>CNN经典网络</h2><p>参考【8】、【9】、【11】</p>
<p><a href="https://geek-docs.com/deep-learning/cnn/keras-implement-cnn.html" target="_blank" rel="noopener">用Keras实现CNN</a></p>
<blockquote>
<p><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html" target="_blank" rel="noopener">卷积神经网络(CNN)简介</a></p>
<p>文章目录</p>
<ul>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#i" target="_blank" rel="noopener">1 动机</a></li>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#i-2" target="_blank" rel="noopener">2 数据集</a></li>
<li>3 卷积<ul>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#i-4" target="_blank" rel="noopener">3.1 这有用吗？</a></li>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#i-5" target="_blank" rel="noopener">3.2 填充</a></li>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#Conv" target="_blank" rel="noopener">3.3 Conv图层</a></li>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#i-6" target="_blank" rel="noopener">3.4 实现卷积</a></li>
</ul>
</li>
<li>4 池化<ul>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#i-8" target="_blank" rel="noopener">4.1 实现池化</a></li>
</ul>
</li>
<li>5 Softmax<ul>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#i-9" target="_blank" rel="noopener">5.1 用法</a></li>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#i-10" target="_blank" rel="noopener">5.2 交叉熵损失</a></li>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#Softmax-2" target="_blank" rel="noopener">5.3 实现Softmax</a></li>
</ul>
</li>
<li><a href="https://geek-docs.com/deep-learning/cnn/cnn-introduce.html#i-11" target="_blank" rel="noopener">6 结论</a></li>
</ul>
</blockquote>
<p>code：仅仅使用numpy构建一个CNN</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">import mnist</span><br><span class="line">from conv import Conv3x3</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">class MaxPool2:</span><br><span class="line">  # A Max Pooling layer using a pool size of 2.</span><br><span class="line"></span><br><span class="line">  def iterate_regions(self, image):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    Generates non-overlapping 2x2 image regions to pool over.</span><br><span class="line">    - image is a 2d numpy array</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    h, w, _ = image.shape</span><br><span class="line">    new_h = h // 2</span><br><span class="line">    new_w = w // 2</span><br><span class="line"></span><br><span class="line">    for i in range(new_h):</span><br><span class="line">      for j in range(new_w):</span><br><span class="line">        im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]</span><br><span class="line">        yield im_region, i, j</span><br><span class="line"></span><br><span class="line">  def forward(self, input):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    Performs a forward pass of the maxpool layer using the given input.</span><br><span class="line">    Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).</span><br><span class="line">    - input is a 3d numpy array with dimensions (h, w, num_filters)</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    h, w, num_filters = input.shape</span><br><span class="line">    output = np.zeros((h // 2, w // 2, num_filters))</span><br><span class="line"></span><br><span class="line">    for im_region, i, j in self.iterate_regions(input):</span><br><span class="line">      output[i, j] = np.amax(im_region, axis=(0, 1))</span><br><span class="line"></span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line">class Softmax:</span><br><span class="line">  # A standard fully-connected layer with softmax activation.</span><br><span class="line"></span><br><span class="line">  def __init__(self, input_len, nodes):</span><br><span class="line">    # We divide by input_len to reduce the variance of our initial values</span><br><span class="line">    self.weights = np.random.randn(input_len, nodes) / input_len</span><br><span class="line">    self.biases = np.zeros(nodes)</span><br><span class="line"></span><br><span class="line">  def forward(self, input):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    Performs a forward pass of the softmax layer using the given input.</span><br><span class="line">    Returns a 1d numpy array containing the respective probability values.</span><br><span class="line">    - input can be any array with any dimensions.</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    input = input.flatten()</span><br><span class="line"></span><br><span class="line">    input_len, nodes = self.weights.shape</span><br><span class="line"></span><br><span class="line">    totals = np.dot(input, self.weights) + self.biases</span><br><span class="line">    exp = np.exp(totals)</span><br><span class="line">    return exp / np.sum(exp, axis=0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># The mnist package handles the MNIST dataset for us!</span><br><span class="line"># Learn more at https://github.com/datapythonista/mnist</span><br><span class="line">test_images = mnist.test_images()[:1000]</span><br><span class="line">test_labels = mnist.test_labels()[:1000]</span><br><span class="line"></span><br><span class="line">conv = Conv3x3(8)                  # 28x28x1 -&gt; 26x26x8</span><br><span class="line">pool = MaxPool2()                  # 26x26x8 -&gt; 13x13x8</span><br><span class="line">softmax = Softmax(13 * 13 * 8, 10) # 13x13x8 -&gt; 10</span><br><span class="line"></span><br><span class="line">def forward(image, label):</span><br><span class="line">  &apos;&apos;&apos;</span><br><span class="line">  Completes a forward pass of the CNN and calculates the accuracy and</span><br><span class="line">  cross-entropy loss.</span><br><span class="line">  - image is a 2d numpy array</span><br><span class="line">  - label is a digit</span><br><span class="line">  &apos;&apos;&apos;</span><br><span class="line">  # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier</span><br><span class="line">  # to work with. This is standard practice.</span><br><span class="line">  out = conv.forward((image / 255) - 0.5)</span><br><span class="line">  out = pool.forward(out)</span><br><span class="line">  out = softmax.forward(out)</span><br><span class="line">  print(out,&apos;-----&apos;)</span><br><span class="line">  # Calculate cross-entropy loss and accuracy. np.log() is the natural log.</span><br><span class="line">  loss = -np.log(out[label])</span><br><span class="line">  acc = 1 if np.argmax(out) == label else 0</span><br><span class="line">  print(&apos;label:&apos;, label, &apos;loss: &apos;, loss)</span><br><span class="line">  return out, loss, acc</span><br><span class="line"></span><br><span class="line">print(&apos;MNIST CNN initialized!&apos;)</span><br><span class="line"></span><br><span class="line">loss = 0</span><br><span class="line">num_correct = 0</span><br><span class="line">for i, (im, label) in enumerate(zip(test_images, test_labels)):</span><br><span class="line">  # Do a forward pass.</span><br><span class="line">  _, l, acc = forward(im, label)</span><br><span class="line">  loss += l</span><br><span class="line">  num_correct += acc</span><br><span class="line"></span><br><span class="line">  # Print stats every 100 steps.</span><br><span class="line">  if i % 100 == 99:</span><br><span class="line">    print(</span><br><span class="line">      &apos;[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%&apos; %</span><br><span class="line">      (i + 1, loss / 100, num_correct)</span><br><span class="line">    )</span><br><span class="line">    loss = 0</span><br><span class="line">    num_correct = 0</span><br></pre></td></tr></table></figure>

<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdalfrggljj314k0dqanw.jpg" alt="image-20200329103418681"></p>
<hr>
<h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><p>本节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易 。</p>
<p>通常来说，<strong>数据标准化预处理对于浅层模型就足够有效了</strong>。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。</p>
<blockquote>
<p>处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。</p>
</blockquote>
<p>但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。</p>
<p><strong>批量归一化的提出正是为了应对深度模型训练的挑战。</strong>在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。BN主要是让收敛变快。</p>
<p>BN需要学习的参数有两个：拉伸（scale）参数 <strong>γ</strong>和偏移（shift）参数 <strong>β</strong></p>
<p>分为两种情况</p>
<ol>
<li><p>对全连接层BN                                                     </p>
<p>批量归一化层置于全连接层中的仿射变换x和激活函数φ之间， x→BN→φ</p>
<blockquote>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbxhohm0o9j31b80agdi5.jpg" alt="image-20200215231136436"></p>
<p>BN实现如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbxhrxlfe6j316y0u0dma.jpg" alt="image-20200215231455398"></p>
</blockquote>
</li>
<li><p>对卷积层BN</p>
</li>
</ol>
<p>BN层置于卷积后、激活函数φ之前。</p>
<p>假设卷积后通道数n，对于每个通道，需要分别做BN，每个通道都有独立的<strong>γ</strong>和 <strong>β</strong>。总共需要学习的参数为2n</p>
<ul>
<li>预测时的批量归一化</li>
</ul>
<p>使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，<strong>和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。</strong></p>
<h3 id="未来发展"><a href="#未来发展" class="headerlink" title="未来发展"></a>未来发展</h3><p>在2014年Hinton给出了一个名为“What is wrong with CNN”的演讲，提出了四点质疑【10】。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbvsnkwsstj316b0u0hbc.jpg" alt="image-20200214120012459"></p>
<p>这些质疑都预示着，CNN的进一步发展不能再仅仅依靠简单的改改模型了。在前沿的研究中，其中很有前景的是这三个方向。<br>分别是，基于球体做CNN，在流形上做CNN和在图上做CNN。这三者都可以看作是将传统CNN从欧几里得空间向外延展到非欧空间，分别到黎曼空间，流形空间和图上。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbvsi7op3gj310w0bgdp3.jpg" alt="image-20200214115502695"></p>
<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbym87xfprj31bq06imyt.jpg" alt="image-20200216223430090"></p>
<p>注意，这里的P 指的是我们需要的决策概率。</p>
<p>P的计算如下：（有点像贝叶斯统计推断）</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbymaace8hj31880aawg6.jpg" alt="image-20200216223629504"></p>
<h3 id="n-元语法"><a href="#n-元语法" class="headerlink" title="n 元语法"></a>n 元语法</h3><p>通过马尔科夫假设，简化语言模型的计算</p>
<blockquote>
<p>这里的马尔可夫假设是指一个词的出现只与前面n个词相关，即n阶马尔可夫链（Markov chain of order n）</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbymeosl55j31as0cyjv9.jpg" alt="image-20200216224042949"></p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbz3crnb9xj30t40fadh1.jpg" alt="image-20200217082702765"></p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbz3d8ziwdj318i0d8tab.jpg" alt="image-20200217082732173"></p>
<h3 id="词嵌入（word2vec）"><a href="#词嵌入（word2vec）" class="headerlink" title="词嵌入（word2vec）"></a>词嵌入（word2vec）</h3><p>参考：<a href="https://www.zybuluo.com/Dounm/note/591752" target="_blank" rel="noopener">Word2Vec-知其然知其所以然</a></p>
<p><a href="https://www.cnblogs.com/peghoty/p/3857839.html" target="_blank" rel="noopener">word2vec 中的数学原理详解</a></p>
<p>词嵌入，也就是用来对词进行编码，将文本词语映射到实数域向量中。</p>
<p>word2vec工具2012年由Google提出。包含了两个模型，即跳字模型（skip-gram） 和连续词袋模型（continuous bag of words，CBOW）。两个模型分别从两个角度来建立词的预测模型，CBOW是通过一个或多个单词的上下文来进行这个词语的预测，而Skip Gram模型是通过一个或多个单词来进行上下文的预测【13】。</p>
<ul>
<li>skip-gram</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbz9ht8bfrj30ng0aymy1.jpg" alt="image-20200217115931938"></p>
<p>训练结束后，对于词典中的任一索引为ii的词，我们均得到该词作为中心词和背景词的两组词向量<strong>v</strong>i和<strong>u</strong>i。在自然语言处理应用中，<strong>一般使用跳字模型的中心词向量作为词的表征向量。</strong></p>
<ul>
<li>CBOW</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbz9jgl7oaj30nw0asgmj.jpg" alt="image-20200217120107092"></p>
<p>与跳字模型最大的不同在于，连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词。</p>
<p>同跳字模型不一样的一点在于，我们一般使<strong>用连续词袋模型的背景词向量作为词的表征向量。</strong></p>
<h3 id="Word2vec-的训练"><a href="#Word2vec-的训练" class="headerlink" title="Word2vec 的训练"></a>Word2vec 的训练</h3><p>为了降低计算复杂度，介绍两种近似训练方法，即负采样（negative sampling）和层序softmax（hierarchical softmax）。</p>
<ul>
<li>负采样通过考虑同时含有正类样本和负类样本的相互独立事件来构造损失函数。其训练中每一步的梯度计算开销与采样的噪声词的个数线性相关。</li>
<li>层序softmax使用了二叉树，并根据根结点到叶结点的路径来构造损失函数。其训练中每一步的梯度计算开销与词典大小的对数相关。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://zh.d2l.ai/chapter_introduction/deep-learning-intro.html#" target="_blank" rel="noopener">1. 深度学习简介</a></li>
<li><a href="https://www.zhihu.com/question/32275069" target="_blank" rel="noopener">知乎：什么是 word embedding?</a><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaakaciu4nj31220rqdjh.jpg" alt="image-20191226235328939"></li>
<li><a href="https://zh.d2l.ai/chapter_deep-learning-basics/linear-regression-scratch.html" target="_blank" rel="noopener">3.2. 线性回归的从零开始实现</a></li>
<li><a href="https://zh.d2l.ai/chapter_deep-learning-basics/linear-regression-gluon.html" target="_blank" rel="noopener">3.3. 线性回归的简洁实现</a></li>
<li><a href="https://zh.d2l.ai/chapter_deep-learning-basics/underfit-overfit.html" target="_blank" rel="noopener">3.11. 模型选择、欠拟合和过拟合</a></li>
<li><a href="https://zh.d2l.ai/chapter_deep-learning-basics/weight-decay.html" target="_blank" rel="noopener">3.12. 权重衰减</a></li>
<li><a href="https://zh.d2l.ai/chapter_deep-learning-basics/backprop.html" target="_blank" rel="noopener">3.14. 正向传播、反向传播和计算图</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/39068853" target="_blank" rel="noopener">CNN简史</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/67938210" target="_blank" rel="noopener">CNN经典网络解读</a></li>
<li><a href="https://www.bilibili.com/video/av67372652/" target="_blank" rel="noopener">Geoffrey Hinton talk “What is wrong with convolutional neural nets ?”</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/47391705" target="_blank" rel="noopener">经典CNN结构简析：AlexNet、VGG、NIN、GoogLeNet、ResNet etc.</a></li>
<li><a href="https://blog.csdn.net/Chenyukuai6625/article/details/77930438" target="_blank" rel="noopener">GoogLeNet网络模型</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36312907" target="_blank" rel="noopener">白话word2vec</a></li>
<li></li>
</ol>
</div></article></div><div class="right-container"><div class="widget"><div id="arAnchorBar"></div></div></div></section></div><div class="right-menu"></div><div class="modal search-modal"><div class="input-field"><input type="text" id="search_input"><label for="search-input">搜索</label></div><div id="search_result" class="search-result"></div></div><div class="blog-overlay"></div><footer class="row"><div class="footer-con"><div class="paginator"><a href="/2020/01/05/书单/" title="⭐️书单" class="prev">上一篇</a><a href="/2019/12/11/Python笔记/" title="⭐️Python笔记" class="next">下一篇</a></div><div class="copyright"><p>© 2019 - 2021 <a target="_blank">Dufy</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <br> and <a href="https://github.com/Bulandent/hexo-theme-bubuzou" target="_blank">hexo-theme-bubuzou</a></p><p> <span style="padding-right: 6px;">闽ICP备16007301号-2</span></p></div><div class="totop"><i></i></div></div></footer><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="/scripts/jquery-1.8.2.min.js"></script><script src="/scripts/ar-anchor.js"></script><script src="/scripts/main.js"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>